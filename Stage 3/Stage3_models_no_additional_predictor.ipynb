{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Models with no additional predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models analysis for top 10 brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import ast\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None) # display all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/notebooks/Data')\n",
    "df_standby = pd.read_csv('aggregated_reviews_meta.csv')\n",
    "df_standby.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df = df_standby.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape is:  (7097, 17)\n",
      "Top 10 brand df's shape is:  (736, 17)\n"
     ]
    }
   ],
   "source": [
    "top10_brand_df = df[df['brand'].isin(['Sony','PWR+','Apple','Boss Audio','Polk Audio','Sangean', 'Tripp Lite', \n",
    "                    'Yamaha Audio','Belkin','Garmin'])]\n",
    "print(\"df shape is: \", df.shape)\n",
    "print(\"Top 10 brand df's shape is: \", top10_brand_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter_BERT_Sentiment data\n",
    "os.chdir('/notebooks/Data')\n",
    "df_BERT = pd.read_csv('Brand_BERT_sentiment.csv')\n",
    "#df_standby.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_bert = df_BERT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand\n",
      "Apple          0.606678\n",
      "Belkin         0.622052\n",
      "BossAudio      0.718182\n",
      "Garmin         0.595745\n",
      "PolkAudio      0.702373\n",
      "Pwr+           0.458546\n",
      "Sangean        0.540475\n",
      "Sony           0.609016\n",
      "TrippLite      0.412678\n",
      "YamahaAudio    0.695243\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Aggregate sentiment column of df_bert by brands create a new column for meta data\n",
    "bert_sentiment = pd.pivot_table(df_bert, index='Brand', values=\"Sentiment\", aggfunc=np.mean)\n",
    "print(bert_sentiment.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (top10_brand_df['brand'] == 'Apple'),\n",
    "    (top10_brand_df['brand'] == 'Belkin'),\n",
    "    (top10_brand_df['brand'] == 'Boss Audio'),\n",
    "    (top10_brand_df['brand'] == 'Garmin'),\n",
    "    (top10_brand_df['brand'] == 'Polk Audio'),\n",
    "    (top10_brand_df['brand'] == 'PWR+'),\n",
    "    (top10_brand_df['brand'] == 'Sangean'),\n",
    "    (top10_brand_df['brand'] == 'Sony'),\n",
    "    (top10_brand_df['brand'] == 'Tripp Lite'),\n",
    "    (top10_brand_df['brand'] == 'Yamaha Audio')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [0.606678, 0.622052, 0.718182, 0.595745, 0.702373, 0.458546, 0.540475, 0.609016, 0.412678, 0.695243]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "top10_brand_df['brand_sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load News_Sentiment data\n",
    "os.chdir('/notebooks/Data')\n",
    "df_news = pd.read_csv('Brand_News_Sentiment.csv')\n",
    "df_news.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (top10_brand_df['brand'] == 'Apple'),\n",
    "    (top10_brand_df['brand'] == 'Belkin'),\n",
    "    (top10_brand_df['brand'] == 'Boss Audio'),\n",
    "    (top10_brand_df['brand'] == 'Garmin'),\n",
    "    (top10_brand_df['brand'] == 'Polk Audio'),\n",
    "    (top10_brand_df['brand'] == 'PWR+'),\n",
    "    (top10_brand_df['brand'] == 'Sangean'),\n",
    "    (top10_brand_df['brand'] == 'Sony'),\n",
    "    (top10_brand_df['brand'] == 'Tripp Lite'),\n",
    "    (top10_brand_df['brand'] == 'Yamaha Audio')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [0.130569, 0.206817, 0.307327, 0.219252, 0.166181, 0.569831, 0.319515, 0.082589, 0.076493, 0.139865]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "top10_brand_df['brand_news_sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.12.2)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.19.5)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.5.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# do encoding for categorical variable\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "new_df = top10_brand_df\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "new_df['brand_encode'] = encoder.fit_transform(new_df['brand'], new_df['rankElectronics'])\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "new_df['main_cat_encode'] = encoder.fit_transform(new_df['main_cat'], new_df['rankElectronics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "We will be using four models for prediction of sales rank. The following are as shown:<br>\n",
    "1. XGBoost Regressor \n",
    "2. Neural Network\n",
    "3. Ridge / Lasso Regression\n",
    "4. Random Forest\n",
    "5. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df['normRankElectronics'] = np.array(sales_df.rankElectronics.rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split\n",
    "X = sales_df.drop(['rankElectronics', 'asin', 'title', 'brand', 'main_cat', 'normRankElectronics', 'brand_sentiment', 'brand_news_sentiment'], axis=1)\n",
    "y = sales_df.normRankElectronics\n",
    "\n",
    "X['vote'] = X['vote'].fillna(0)# to replace NaN with 0\n",
    "\n",
    "# Standard Scaler or Min Max Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4222)\n",
    "\n",
    "X_train[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\"]] = scaler.fit_transform(X_train[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\"]])\n",
    "X_test[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\"]] = scaler.transform(X_test[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "#Defining MAPE function\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "#models is a list of the fitted models, model_names is list of model names\n",
    "\n",
    "def model_performance(models, model_names):\n",
    "    #create empty df with col names\n",
    "    df = pd.DataFrame(columns = ['Model', 'Train: Rsquare', 'Test: Rsquare', 'Train: MAE', 'Test: MAE', 'Train: RMSE', 'Test: RMSE', 'Train: MAPE', 'Test: MAPE'])\n",
    "    \n",
    "    for n, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        #prepare values for model\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        name = model_names[n] \n",
    "        rsquare_train = r2_score(y_train, y_train_pred)\n",
    "        mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "        rmse_train = math.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        \n",
    "        rsquare_test = r2_score(y_test, y_pred)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred)\n",
    "        rmse_test = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        mape_train = MAPE(y_train, y_train_pred)\n",
    "        mape_test = MAPE(y_test, y_pred)\n",
    "\n",
    "        #append row to df\n",
    "        df = df.append({'Model' \n",
    "                        : name, 'Train: Rsquare' : rsquare_train, 'Test: Rsquare' : rsquare_test, 'Train: MAE': mae_train, 'Test: MAE' : mae_test, 'Train: RMSE': rmse_train,\n",
    "                         'Test: RMSE' : rmse_test, 'Train: MAPE': mape_train, 'Test: MAPE': mape_test}, \n",
    "                    ignore_index = True)\n",
    "            \n",
    "    return df.set_index('Model').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared on train dataset=0.19469439175619685\n",
      "R-Squared on test dataset=0.5218370630022122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>dtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.445184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.194694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>128.453769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>157.805276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>157.531492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>191.021574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>174.806516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>114.039335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  dtr\n",
       "Train: Rsquare    0.445184\n",
       "Test: Rsquare     0.194694\n",
       "Train: MAE      128.453769\n",
       "Test: MAE       157.805276\n",
       "Train: RMSE     157.531492\n",
       "Test: RMSE      191.021574\n",
       "Train: MAPE     174.806516\n",
       "Test: MAPE      114.039335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Without additional predictors\n",
    "dtr = DecisionTreeRegressor(max_depth=4,\n",
    "                           min_samples_split=5,\n",
    "                           max_leaf_nodes=10)\n",
    "\n",
    "dtr.fit(X_train,y_train)\n",
    "print(\"R-Squared on train dataset={}\".format(dtr.score(X_test,y_test)))\n",
    "\n",
    "dtr.fit(X_test,y_test)   \n",
    "print(\"R-Squared on test dataset={}\".format(dtr.score(X_test,y_test)))\n",
    "\n",
    "model_performance([dtr], [\"dtr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>dtr</th>\n",
       "      <th>dtr_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.445184</td>\n",
       "      <td>0.503350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.194694</td>\n",
       "      <td>0.204002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>128.453769</td>\n",
       "      <td>119.484159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>157.805276</td>\n",
       "      <td>153.235066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>157.531492</td>\n",
       "      <td>149.045256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>191.021574</td>\n",
       "      <td>189.914516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>174.806516</td>\n",
       "      <td>142.688494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>114.039335</td>\n",
       "      <td>97.181057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  dtr   dtr_tuned\n",
       "Train: Rsquare    0.445184    0.503350\n",
       "Test: Rsquare     0.194694    0.204002\n",
       "Train: MAE      128.453769  119.484159\n",
       "Test: MAE       157.805276  153.235066\n",
       "Train: RMSE     157.531492  149.045256\n",
       "Test: RMSE      191.021574  189.914516\n",
       "Train: MAPE     174.806516  142.688494\n",
       "Test: MAPE      114.039335   97.181057"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"criterion\": [\"mse\", \"mae\"],\n",
    "              \"min_samples_split\": [10, 20, 40],\n",
    "              \"max_depth\": [2, 6, 8],\n",
    "              \"min_samples_leaf\": [20, 40, 100],\n",
    "              \"max_leaf_nodes\": [5, 20, 100],\n",
    "              }\n",
    "\n",
    "## Comment in order to publish in kaggle.\n",
    "dtr_tuned = GridSearchCV(dtr, param_grid, cv=5)\n",
    "model_performance([dtr, dtr_tuned], [\"dtr\", \"dtr_tuned\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1920      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 18,561\n",
      "Trainable params: 18,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(128,\n",
    "                activation = 'relu',\n",
    "                input_shape = (14, ),\n",
    "                activity_regularizer = regularizers.l2(1e-5)))\n",
    "nn.add(Dropout(0.50))\n",
    "nn.add(Dense(128,\n",
    "                activation = 'relu', \n",
    "                activity_regularizer = regularizers.l2(1e-5)))\n",
    "nn.add(Dropout(0.50))\n",
    "nn.add(Dense(1, activation = 'relu'))\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "56/56 [==============================] - 2s 11ms/step - loss: 40034016.4912 - mse: 39564727.6316 - mae: 2068.8383 - val_loss: 563281.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 2/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 700367.6425 - mse: 224342.8328 - mae: 358.5289 - val_loss: 565043.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 3/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 671294.7829 - mse: 217929.8605 - mae: 362.4038 - val_loss: 561837.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 4/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 643776.4879 - mse: 177655.0543 - mae: 362.4202 - val_loss: 552777.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 5/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 601734.1656 - mse: 162601.2762 - mae: 344.1512 - val_loss: 542105.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 6/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 602275.3969 - mse: 167181.1752 - mae: 345.2040 - val_loss: 529406.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 7/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 610079.5477 - mse: 203733.2547 - mae: 373.0171 - val_loss: 523405.3438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 8/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 626790.8975 - mse: 231922.7593 - mae: 365.1108 - val_loss: 525097.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 9/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 570455.6436 - mse: 169844.9010 - mae: 357.5428 - val_loss: 516431.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 10/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 584706.9507 - mse: 181455.2807 - mae: 363.4421 - val_loss: 508839.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 11/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 570472.5428 - mse: 181179.7711 - mae: 369.9329 - val_loss: 507930.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 12/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 558244.0088 - mse: 163899.1406 - mae: 342.8525 - val_loss: 495069.2188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 13/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 568602.0296 - mse: 184069.2818 - mae: 362.1689 - val_loss: 502094.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 14/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 541593.4671 - mse: 173394.4178 - mae: 351.1836 - val_loss: 496868.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 15/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 542098.9282 - mse: 174570.3040 - mae: 360.8599 - val_loss: 486985.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 16/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 595217.4660 - mse: 233171.0861 - mae: 376.7051 - val_loss: 478635.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 17/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 513886.5526 - mse: 166023.5280 - mae: 349.3749 - val_loss: 466069.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 18/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 1205425.7599 - mse: 870263.8758 - mae: 400.0427 - val_loss: 476182.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 19/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 509712.6486 - mse: 165234.2678 - mae: 348.2288 - val_loss: 468927.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 20/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 513561.4348 - mse: 167003.0595 - mae: 347.5597 - val_loss: 461309.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 21/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 478579.2549 - mse: 169363.7541 - mae: 349.2470 - val_loss: 455285.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 22/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 555001.7434 - mse: 247164.3407 - mae: 368.5494 - val_loss: 455896.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 23/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 495754.7511 - mse: 178749.9883 - mae: 365.6268 - val_loss: 453820.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 24/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 489404.0932 - mse: 176379.2643 - mae: 359.5530 - val_loss: 445504.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 25/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 473880.4589 - mse: 170409.6965 - mae: 350.1935 - val_loss: 438393.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 26/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 460552.4408 - mse: 170800.1105 - mae: 354.5020 - val_loss: 431766.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 27/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 468489.5055 - mse: 183138.6893 - mae: 350.7773 - val_loss: 424206.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 28/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 444275.5603 - mse: 171444.1009 - mae: 355.2366 - val_loss: 419533.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 29/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 439045.1107 - mse: 165662.5630 - mae: 346.7993 - val_loss: 413580.2188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 30/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 468221.0883 - mse: 198513.2974 - mae: 378.8286 - val_loss: 409449.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 31/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 423497.9759 - mse: 169828.4520 - mae: 353.4414 - val_loss: 403952.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 32/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 478031.7155 - mse: 219457.1475 - mae: 373.5323 - val_loss: 406031.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 33/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 417298.0685 - mse: 163181.1593 - mae: 341.8629 - val_loss: 399786.9062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 34/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 424827.8947 - mse: 172956.4874 - mae: 358.3380 - val_loss: 394007.9688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 35/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 408488.2423 - mse: 175397.4904 - mae: 362.0199 - val_loss: 388683.2812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 36/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 376431.3147 - mse: 153524.1087 - mae: 330.8640 - val_loss: 383609.5938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 37/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 397565.2845 - mse: 171378.6897 - mae: 355.1202 - val_loss: 378736.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 38/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 395759.4359 - mse: 176946.4531 - mae: 362.3850 - val_loss: 374048.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 39/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 375553.5565 - mse: 169718.5611 - mae: 353.0767 - val_loss: 369556.2188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 40/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 375816.2549 - mse: 169761.4298 - mae: 352.6890 - val_loss: 365405.6562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 41/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 391364.7988 - mse: 182728.6488 - mae: 366.3714 - val_loss: 361433.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 42/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 387271.1360 - mse: 183323.5121 - mae: 367.3017 - val_loss: 357632.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 43/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 426032.8640 - mse: 231721.0855 - mae: 372.0006 - val_loss: 354778.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 44/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 378777.9073 - mse: 185029.5088 - mae: 355.0403 - val_loss: 351954.3438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 45/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 358879.1393 - mse: 171300.1746 - mae: 353.3002 - val_loss: 348955.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 46/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 369865.0921 - mse: 186135.1889 - mae: 371.7470 - val_loss: 346074.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 47/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 353191.4803 - mse: 171003.8898 - mae: 357.2462 - val_loss: 342059.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 48/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 354426.8986 - mse: 180632.7659 - mae: 369.8167 - val_loss: 337921.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 49/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 338268.8284 - mse: 173230.4838 - mae: 353.5552 - val_loss: 333757.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 50/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 333592.1398 - mse: 168261.8555 - mae: 351.7967 - val_loss: 330072.6562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 51/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 327744.7571 - mse: 164438.4224 - mae: 346.2305 - val_loss: 326352.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 52/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 328348.1497 - mse: 173329.1110 - mae: 360.6054 - val_loss: 322668.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 53/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 337903.6094 - mse: 183247.0639 - mae: 355.5988 - val_loss: 321065.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 54/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 333374.5143 - mse: 181991.6420 - mae: 375.1022 - val_loss: 317435.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 55/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 325716.0471 - mse: 179205.3152 - mae: 365.6758 - val_loss: 313869.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 56/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 321840.6573 - mse: 175670.1113 - mae: 361.2588 - val_loss: 310141.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 57/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 314452.5652 - mse: 173812.9380 - mae: 356.5828 - val_loss: 306669.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 58/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 301760.5970 - mse: 168449.9764 - mae: 350.1845 - val_loss: 303335.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 59/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 303148.5280 - mse: 174163.1886 - mae: 361.5233 - val_loss: 300127.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 60/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 312405.3087 - mse: 184194.5943 - mae: 369.4786 - val_loss: 296859.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 61/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 305146.5537 - mse: 182795.5809 - mae: 373.2567 - val_loss: 293548.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 62/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 287613.0836 - mse: 166804.0924 - mae: 348.3065 - val_loss: 290551.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 63/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 288035.7377 - mse: 174054.9205 - mae: 344.9265 - val_loss: 287873.9688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 64/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 287209.4633 - mse: 174514.6809 - mae: 357.9131 - val_loss: 284878.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 65/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 276120.6464 - mse: 166977.2823 - mae: 351.6943 - val_loss: 281882.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 66/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 278449.1201 - mse: 173275.2771 - mae: 351.2323 - val_loss: 278976.5312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 67/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 276844.4507 - mse: 176902.6834 - mae: 368.2843 - val_loss: 276027.9688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 68/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 274375.3213 - mse: 177597.9301 - mae: 356.6772 - val_loss: 273564.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 69/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 293481.4095 - mse: 195481.8827 - mae: 385.4484 - val_loss: 271941.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 70/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 264099.5825 - mse: 171696.5337 - mae: 355.3289 - val_loss: 269194.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 71/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 274430.3128 - mse: 184889.6968 - mae: 370.7084 - val_loss: 266382.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 72/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 258312.3660 - mse: 173245.0441 - mae: 359.2641 - val_loss: 263651.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 73/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 255680.6425 - mse: 172034.0694 - mae: 354.2178 - val_loss: 261099.1094 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 74/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 256227.9674 - mse: 174602.4235 - mae: 357.3998 - val_loss: 258374.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 75/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 244164.8492 - mse: 168423.4498 - mae: 347.1201 - val_loss: 255751.9219 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 76/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 250432.5090 - mse: 176916.5310 - mae: 361.2823 - val_loss: 252658.8906 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 77/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 243200.1187 - mse: 171583.2928 - mae: 353.9667 - val_loss: 250208.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 78/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 239456.9515 - mse: 168519.1528 - mae: 349.3616 - val_loss: 247856.1094 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 79/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 230383.6595 - mse: 163265.4816 - mae: 346.1184 - val_loss: 245391.6562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 80/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 222490.8871 - mse: 160485.7760 - mae: 339.9747 - val_loss: 242989.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 81/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 238008.0757 - mse: 179149.5932 - mae: 365.9508 - val_loss: 240867.1406 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 82/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 241820.5340 - mse: 183040.7678 - mae: 369.4833 - val_loss: 238644.3281 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 83/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 227065.2522 - mse: 171248.0603 - mae: 356.1492 - val_loss: 236499.8906 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 84/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 226756.4904 - mse: 169293.7484 - mae: 350.2739 - val_loss: 234555.4219 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 85/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 216254.7155 - mse: 161754.8432 - mae: 344.3079 - val_loss: 232584.9844 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 86/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 228736.9309 - mse: 178809.6132 - mae: 366.6228 - val_loss: 230718.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 87/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 207667.9583 - mse: 159987.5995 - mae: 342.9095 - val_loss: 228963.5469 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 88/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 213745.6815 - mse: 168788.0332 - mae: 351.0032 - val_loss: 226971.7344 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 89/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 208518.8107 - mse: 163873.7256 - mae: 343.8285 - val_loss: 224791.3281 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 90/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 215116.7516 - mse: 173089.7634 - mae: 361.8581 - val_loss: 223073.9844 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 91/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 213384.5244 - mse: 173885.0000 - mae: 358.2316 - val_loss: 221270.9219 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 92/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 200411.1938 - mse: 162526.3514 - mae: 344.0704 - val_loss: 219477.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 93/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 219998.6749 - mse: 182476.8087 - mae: 367.6270 - val_loss: 218167.0625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 94/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 202381.7371 - mse: 168315.7462 - mae: 349.1280 - val_loss: 216449.2031 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 95/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 209393.9487 - mse: 176187.1206 - mae: 365.8885 - val_loss: 215078.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 96/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 200717.8062 - mse: 169516.1562 - mae: 352.3020 - val_loss: 213736.5938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 97/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 201845.6568 - mse: 170610.5162 - mae: 350.9029 - val_loss: 212145.5781 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 98/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 199497.6768 - mse: 171528.2662 - mae: 355.5634 - val_loss: 210595.2188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 99/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 195357.7656 - mse: 167723.8130 - mae: 346.5951 - val_loss: 209112.0781 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 100/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 194665.1954 - mse: 169124.7478 - mae: 350.7458 - val_loss: 208412.2656 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 101/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 192372.7034 - mse: 167604.9386 - mae: 349.2114 - val_loss: 207147.1719 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 102/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 202335.7467 - mse: 177798.0112 - mae: 367.9140 - val_loss: 205977.3438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 103/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 194398.6358 - mse: 171977.2193 - mae: 356.8278 - val_loss: 204648.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 104/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 192777.8314 - mse: 170994.1335 - mae: 354.6515 - val_loss: 203587.1406 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 105/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 199477.6354 - mse: 178650.9559 - mae: 359.7511 - val_loss: 202862.9531 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 106/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 181126.7922 - mse: 161245.4970 - mae: 341.5323 - val_loss: 201686.5469 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 107/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 174179.4518 - mse: 155705.9331 - mae: 337.7278 - val_loss: 199949.9688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 108/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 193947.0230 - mse: 176994.0874 - mae: 360.5239 - val_loss: 198730.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 109/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 192677.1406 - mse: 176749.4657 - mae: 355.6899 - val_loss: 198646.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 110/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 181394.3771 - mse: 166139.5954 - mae: 350.1617 - val_loss: 198058.3125 - val_mse: 183530.6719 - val_mae: 375.9023\n",
      "Epoch 111/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 184242.5356 - mse: 168658.7308 - mae: 352.3224 - val_loss: 194660.6719 - val_mse: 180950.0469 - val_mae: 373.0003\n",
      "Epoch 112/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 187893.3259 - mse: 172661.5828 - mae: 356.0227 - val_loss: 195074.6875 - val_mse: 180671.7500 - val_mae: 372.3566\n",
      "Epoch 113/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 180320.5831 - mse: 164404.5513 - mae: 349.1344 - val_loss: 186445.0312 - val_mse: 172457.4531 - val_mae: 362.8154\n",
      "Epoch 114/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 166751.6397 - mse: 152573.7426 - mae: 326.1850 - val_loss: 118454.7109 - val_mse: 105222.6953 - val_mae: 270.5695\n",
      "Epoch 115/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 178385.9191 - mse: 164349.9106 - mae: 350.6194 - val_loss: 109220.1406 - val_mse: 96896.2812 - val_mae: 255.2901\n",
      "Epoch 116/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 159927.0154 - mse: 147068.5850 - mae: 324.2602 - val_loss: 85699.7031 - val_mse: 73573.1016 - val_mae: 222.1648\n",
      "Epoch 117/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 153245.5630 - mse: 141223.0896 - mae: 305.4479 - val_loss: 94013.2578 - val_mse: 82555.6875 - val_mae: 236.9369\n",
      "Epoch 118/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 143475.5983 - mse: 131888.3573 - mae: 296.3924 - val_loss: 67446.9688 - val_mse: 55950.8125 - val_mae: 198.7898\n",
      "Epoch 119/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 140430.4117 - mse: 128799.6224 - mae: 294.5513 - val_loss: 75028.1328 - val_mse: 64535.8867 - val_mae: 210.3616\n",
      "Epoch 120/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 135629.6347 - mse: 124894.6790 - mae: 293.6299 - val_loss: 71460.9922 - val_mse: 61393.0391 - val_mae: 205.5381\n",
      "Epoch 121/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 124274.8566 - mse: 113790.4428 - mae: 272.0011 - val_loss: 86546.7266 - val_mse: 76309.2109 - val_mae: 230.0221\n",
      "Epoch 122/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 139782.5837 - mse: 128987.7096 - mae: 283.1222 - val_loss: 97146.8672 - val_mse: 87569.3047 - val_mae: 246.5078\n",
      "Epoch 123/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 121570.8901 - mse: 112345.8166 - mae: 269.0069 - val_loss: 78562.6094 - val_mse: 69537.0938 - val_mae: 217.5366\n",
      "Epoch 124/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 123347.5400 - mse: 114655.5303 - mae: 270.6414 - val_loss: 82695.5312 - val_mse: 74584.1016 - val_mae: 226.2208\n",
      "Epoch 125/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 111820.1298 - mse: 104063.6239 - mae: 268.0110 - val_loss: 101895.1172 - val_mse: 94447.1172 - val_mae: 255.3990\n",
      "Epoch 126/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 113625.3214 - mse: 105469.6031 - mae: 264.7723 - val_loss: 78936.4531 - val_mse: 71377.2656 - val_mae: 220.9631\n",
      "Epoch 127/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 116168.3905 - mse: 108187.7227 - mae: 270.9872 - val_loss: 89822.2734 - val_mse: 82385.4766 - val_mae: 238.8471\n",
      "Epoch 128/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 109745.2452 - mse: 101899.5640 - mae: 261.1152 - val_loss: 85264.9609 - val_mse: 78370.2188 - val_mae: 230.9604\n",
      "Epoch 129/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 108706.4498 - mse: 101052.8562 - mae: 256.7589 - val_loss: 79578.8359 - val_mse: 72690.9922 - val_mae: 223.0428\n",
      "Epoch 130/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 117942.9458 - mse: 110927.0058 - mae: 277.2837 - val_loss: 75763.4297 - val_mse: 69268.1953 - val_mae: 216.6874\n",
      "Epoch 131/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 96163.2915 - mse: 89896.5321 - mae: 238.8785 - val_loss: 94544.4688 - val_mse: 88634.7109 - val_mae: 246.7878\n",
      "Epoch 132/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 97281.2439 - mse: 91293.0541 - mae: 240.8330 - val_loss: 99114.2422 - val_mse: 93535.7812 - val_mae: 253.5936\n",
      "Epoch 133/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 94144.4720 - mse: 88078.3824 - mae: 238.5319 - val_loss: 85852.2734 - val_mse: 80407.3438 - val_mae: 234.1005\n",
      "Epoch 134/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 107771.5567 - mse: 102074.7784 - mae: 260.1695 - val_loss: 92339.3594 - val_mse: 87143.1875 - val_mae: 245.4462\n",
      "Epoch 135/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 99378.2192 - mse: 94233.4840 - mae: 252.3301 - val_loss: 72980.4219 - val_mse: 67738.8125 - val_mae: 215.9699\n",
      "Epoch 136/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 100106.8718 - mse: 94748.0879 - mae: 249.8181 - val_loss: 93727.0078 - val_mse: 89066.3750 - val_mae: 249.3923\n",
      "Epoch 137/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 106856.3174 - mse: 101910.7474 - mae: 263.8750 - val_loss: 80597.9922 - val_mse: 75651.0547 - val_mae: 228.1289\n",
      "Epoch 138/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 130273.2654 - mse: 125182.2775 - mae: 270.5677 - val_loss: 95570.6641 - val_mse: 90721.8047 - val_mae: 251.5746\n",
      "Epoch 139/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 98656.4366 - mse: 93150.0230 - mae: 243.2013 - val_loss: 74910.4453 - val_mse: 69774.6562 - val_mae: 218.7334\n",
      "Epoch 140/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 96299.6069 - mse: 90917.5413 - mae: 237.4768 - val_loss: 67269.2891 - val_mse: 62129.0781 - val_mae: 206.8169\n",
      "Epoch 141/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 97969.6280 - mse: 92705.3089 - mae: 244.1457 - val_loss: 65621.3672 - val_mse: 60589.7461 - val_mae: 203.4798\n",
      "Epoch 142/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 104409.9348 - mse: 99090.0772 - mae: 256.9773 - val_loss: 80813.5469 - val_mse: 76088.2891 - val_mae: 227.7530\n",
      "Epoch 143/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 83800.4341 - mse: 79034.8677 - mae: 223.0331 - val_loss: 78692.0781 - val_mse: 74146.1250 - val_mae: 225.2758\n",
      "Epoch 144/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 84024.1302 - mse: 79445.8462 - mae: 231.1680 - val_loss: 88244.1172 - val_mse: 84075.9375 - val_mae: 240.9955\n",
      "Epoch 145/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 93613.5191 - mse: 89320.8487 - mae: 245.6607 - val_loss: 90163.7500 - val_mse: 85980.1641 - val_mae: 243.8257\n",
      "Epoch 146/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 86627.1700 - mse: 82050.0561 - mae: 232.4563 - val_loss: 92601.1797 - val_mse: 88352.1641 - val_mae: 247.9505\n",
      "Epoch 147/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 86970.9014 - mse: 82456.7296 - mae: 230.1466 - val_loss: 70048.6328 - val_mse: 65957.3906 - val_mae: 213.1511\n",
      "Epoch 148/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 104959.6031 - mse: 100682.4681 - mae: 246.6873 - val_loss: 86574.9609 - val_mse: 82573.7188 - val_mae: 239.2111\n",
      "Epoch 149/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 79731.0827 - mse: 75729.8399 - mae: 223.5206 - val_loss: 68202.6406 - val_mse: 64311.4141 - val_mae: 209.9749\n",
      "Epoch 150/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 88832.1621 - mse: 85139.2904 - mae: 230.4095 - val_loss: 86832.3984 - val_mse: 83407.6016 - val_mae: 240.1844\n",
      "Epoch 151/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77159.0120 - mse: 73877.8986 - mae: 220.7018 - val_loss: 71185.7266 - val_mse: 67916.8281 - val_mae: 215.5998\n",
      "Epoch 152/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 84588.4500 - mse: 81396.2670 - mae: 225.1853 - val_loss: 81490.5781 - val_mse: 78604.9297 - val_mae: 233.5374\n",
      "Epoch 153/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 85393.1885 - mse: 82420.1392 - mae: 228.6492 - val_loss: 81663.3125 - val_mse: 78806.3281 - val_mae: 232.8477\n",
      "Epoch 154/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 73892.6839 - mse: 70986.4017 - mae: 216.2801 - val_loss: 94411.1641 - val_mse: 91531.9844 - val_mae: 251.3707\n",
      "Epoch 155/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 84083.6390 - mse: 81035.2854 - mae: 229.8139 - val_loss: 95810.3359 - val_mse: 93196.4297 - val_mae: 253.4734\n",
      "Epoch 156/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77827.0266 - mse: 75182.1994 - mae: 220.9705 - val_loss: 95367.2969 - val_mse: 92818.8906 - val_mae: 253.2296\n",
      "Epoch 157/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 84817.1458 - mse: 81951.8658 - mae: 235.6144 - val_loss: 79418.4844 - val_mse: 76586.4531 - val_mae: 229.2038\n",
      "Epoch 158/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 87536.6239 - mse: 84680.4581 - mae: 237.0363 - val_loss: 78249.5312 - val_mse: 75505.3047 - val_mae: 228.0490\n",
      "Epoch 159/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68619.9004 - mse: 66040.1819 - mae: 202.1037 - val_loss: 85452.7578 - val_mse: 83025.3672 - val_mae: 238.8078\n",
      "Epoch 160/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 78539.8144 - mse: 75907.6115 - mae: 223.8902 - val_loss: 99222.9844 - val_mse: 97048.5938 - val_mae: 259.0569\n",
      "Epoch 161/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77799.7744 - mse: 75391.1924 - mae: 221.8499 - val_loss: 87248.2344 - val_mse: 84921.9219 - val_mae: 241.5059\n",
      "Epoch 162/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73290.6345 - mse: 70954.6247 - mae: 210.6048 - val_loss: 80541.3594 - val_mse: 78129.5391 - val_mae: 230.9359\n",
      "Epoch 163/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69924.7715 - mse: 67703.6630 - mae: 214.5349 - val_loss: 74803.6094 - val_mse: 72343.4219 - val_mae: 221.7267\n",
      "Epoch 164/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77350.5894 - mse: 74691.9365 - mae: 225.7809 - val_loss: 72292.0078 - val_mse: 69824.7266 - val_mae: 216.0170\n",
      "Epoch 165/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75472.2754 - mse: 73188.7185 - mae: 221.0725 - val_loss: 68806.3047 - val_mse: 66436.7656 - val_mae: 212.0099\n",
      "Epoch 166/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75546.8263 - mse: 73149.1697 - mae: 212.4652 - val_loss: 79322.5469 - val_mse: 76844.9297 - val_mae: 229.7532\n",
      "Epoch 167/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72510.8745 - mse: 70199.8574 - mae: 216.2004 - val_loss: 79361.3047 - val_mse: 77008.4219 - val_mae: 228.8281\n",
      "Epoch 168/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77796.8875 - mse: 75473.0578 - mae: 217.6711 - val_loss: 70130.8828 - val_mse: 67579.9219 - val_mae: 213.9088\n",
      "Epoch 169/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70364.9642 - mse: 68115.4583 - mae: 212.0829 - val_loss: 80443.4609 - val_mse: 78295.5391 - val_mae: 229.6066\n",
      "Epoch 170/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 74721.7483 - mse: 72507.3874 - mae: 220.0543 - val_loss: 96473.5078 - val_mse: 94253.1328 - val_mae: 256.2242\n",
      "Epoch 171/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77926.8945 - mse: 75733.5535 - mae: 222.8432 - val_loss: 88828.5469 - val_mse: 86601.3125 - val_mae: 244.9078\n",
      "Epoch 172/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 82411.4899 - mse: 79966.6804 - mae: 231.8120 - val_loss: 82920.6641 - val_mse: 80595.1328 - val_mae: 235.5688\n",
      "Epoch 173/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70121.3461 - mse: 67688.0530 - mae: 212.7863 - val_loss: 80507.1797 - val_mse: 78053.5234 - val_mae: 229.5252\n",
      "Epoch 174/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73663.9444 - mse: 71117.2344 - mae: 212.3594 - val_loss: 85481.1328 - val_mse: 83167.2344 - val_mae: 237.7625\n",
      "Epoch 175/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77903.4815 - mse: 75697.1311 - mae: 232.2849 - val_loss: 81553.1719 - val_mse: 79286.3594 - val_mae: 232.3517\n",
      "Epoch 176/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 80832.9619 - mse: 78399.3428 - mae: 224.0928 - val_loss: 86051.2266 - val_mse: 83900.6250 - val_mae: 240.1235\n",
      "Epoch 177/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73723.5616 - mse: 71513.8453 - mae: 214.0946 - val_loss: 87889.5469 - val_mse: 85823.2266 - val_mae: 242.9157\n",
      "Epoch 178/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62925.5308 - mse: 60812.3161 - mae: 193.2460 - val_loss: 69093.8438 - val_mse: 66882.8984 - val_mae: 212.9773\n",
      "Epoch 179/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 84224.3483 - mse: 82075.0812 - mae: 231.3657 - val_loss: 83325.0547 - val_mse: 81333.1562 - val_mae: 236.0649\n",
      "Epoch 180/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70880.5361 - mse: 68878.2532 - mae: 213.7629 - val_loss: 88975.8203 - val_mse: 87340.0938 - val_mae: 245.0670\n",
      "Epoch 181/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 78974.5269 - mse: 77231.5578 - mae: 225.9688 - val_loss: 93791.6094 - val_mse: 91942.4141 - val_mae: 252.1885\n",
      "Epoch 182/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68357.5745 - mse: 66383.6372 - mae: 202.9672 - val_loss: 81989.1172 - val_mse: 80007.7578 - val_mae: 234.4838\n",
      "Epoch 183/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63844.0504 - mse: 61874.2500 - mae: 200.8000 - val_loss: 84901.7656 - val_mse: 82960.2109 - val_mae: 238.7816\n",
      "Epoch 184/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70257.0846 - mse: 68413.6324 - mae: 208.2136 - val_loss: 75100.8750 - val_mse: 73016.2891 - val_mae: 223.4173\n",
      "Epoch 185/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62854.0750 - mse: 60763.1209 - mae: 197.0665 - val_loss: 81652.8906 - val_mse: 79729.3594 - val_mae: 233.9776\n",
      "Epoch 186/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69633.0347 - mse: 67775.3085 - mae: 208.5663 - val_loss: 100104.2266 - val_mse: 98324.6250 - val_mae: 261.7480\n",
      "Epoch 187/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70644.3412 - mse: 68675.8456 - mae: 208.0109 - val_loss: 94804.1719 - val_mse: 93268.2578 - val_mae: 254.5420\n",
      "Epoch 188/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64217.6997 - mse: 62583.0271 - mae: 201.1317 - val_loss: 82034.7422 - val_mse: 80314.0781 - val_mae: 235.2841\n",
      "Epoch 189/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73384.2685 - mse: 71546.8152 - mae: 218.0884 - val_loss: 80111.8359 - val_mse: 78344.8750 - val_mae: 232.5786\n",
      "Epoch 190/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61048.2320 - mse: 59310.0424 - mae: 193.3085 - val_loss: 81918.6094 - val_mse: 80012.6641 - val_mae: 234.6523\n",
      "Epoch 191/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 83310.9215 - mse: 81495.8899 - mae: 229.1597 - val_loss: 82550.1719 - val_mse: 80819.5391 - val_mae: 236.2878\n",
      "Epoch 192/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72420.4918 - mse: 70808.6930 - mae: 211.8948 - val_loss: 87778.7734 - val_mse: 86104.9531 - val_mae: 244.5014\n",
      "Epoch 193/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 63416.5101 - mse: 61641.9581 - mae: 199.6890 - val_loss: 78982.4062 - val_mse: 77335.1406 - val_mae: 230.3936\n",
      "Epoch 194/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73492.4707 - mse: 71724.4092 - mae: 212.8195 - val_loss: 97278.6641 - val_mse: 95549.2812 - val_mae: 258.3351\n",
      "Epoch 195/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 76960.2029 - mse: 75035.5422 - mae: 220.6627 - val_loss: 69897.4531 - val_mse: 68119.1875 - val_mae: 215.0473\n",
      "Epoch 196/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 74336.9104 - mse: 72569.1158 - mae: 217.3504 - val_loss: 91387.4922 - val_mse: 89678.2266 - val_mae: 249.8007\n",
      "Epoch 197/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68278.6101 - mse: 66524.1861 - mae: 210.5766 - val_loss: 85487.4141 - val_mse: 83861.2266 - val_mae: 240.8684\n",
      "Epoch 198/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 76746.4104 - mse: 74967.6831 - mae: 225.2900 - val_loss: 90045.7891 - val_mse: 88416.9922 - val_mae: 247.7253\n",
      "Epoch 199/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69571.9401 - mse: 67979.7899 - mae: 210.7571 - val_loss: 93147.1172 - val_mse: 91703.0000 - val_mae: 252.2175\n",
      "Epoch 200/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70038.6231 - mse: 68437.1190 - mae: 206.5997 - val_loss: 86261.8281 - val_mse: 84710.3750 - val_mae: 242.1507\n",
      "Epoch 201/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70120.9021 - mse: 68561.7124 - mae: 211.3119 - val_loss: 86650.0781 - val_mse: 85225.2188 - val_mae: 242.8122\n",
      "Epoch 202/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65604.5589 - mse: 64109.9141 - mae: 205.3909 - val_loss: 70093.3594 - val_mse: 68595.8906 - val_mae: 215.9493\n",
      "Epoch 203/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73703.5836 - mse: 72331.2791 - mae: 213.9796 - val_loss: 80012.0391 - val_mse: 78521.1250 - val_mae: 232.7960\n",
      "Epoch 204/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 83423.9033 - mse: 81866.4045 - mae: 235.1749 - val_loss: 95505.7188 - val_mse: 93943.3047 - val_mae: 256.3773\n",
      "Epoch 205/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67468.3483 - mse: 65956.1923 - mae: 208.3866 - val_loss: 80728.0625 - val_mse: 79311.1562 - val_mae: 233.0252\n",
      "Epoch 206/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59247.5252 - mse: 57856.4734 - mae: 196.7113 - val_loss: 83658.2969 - val_mse: 82168.6094 - val_mae: 237.7123\n",
      "Epoch 207/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60584.1701 - mse: 59121.0606 - mae: 201.1554 - val_loss: 87205.7031 - val_mse: 85777.3984 - val_mae: 243.5676\n",
      "Epoch 208/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62517.9721 - mse: 61188.9088 - mae: 196.1342 - val_loss: 93702.4531 - val_mse: 92447.1328 - val_mae: 252.9160\n",
      "Epoch 209/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65866.2892 - mse: 64596.3997 - mae: 205.9554 - val_loss: 91460.4141 - val_mse: 90225.5703 - val_mae: 250.6013\n",
      "Epoch 210/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65897.3037 - mse: 64544.0607 - mae: 200.5440 - val_loss: 95463.9297 - val_mse: 94190.2734 - val_mae: 256.2619\n",
      "Epoch 211/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 74327.6554 - mse: 72882.5462 - mae: 219.6339 - val_loss: 85517.5391 - val_mse: 84252.4844 - val_mae: 241.8760\n",
      "Epoch 212/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70723.1091 - mse: 69335.5538 - mae: 216.2118 - val_loss: 108734.7266 - val_mse: 107816.8125 - val_mae: 274.8739\n",
      "Epoch 213/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68705.0683 - mse: 67493.9842 - mae: 202.0570 - val_loss: 83623.2812 - val_mse: 82081.7578 - val_mae: 238.3867\n",
      "Epoch 214/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73788.1284 - mse: 72334.9089 - mae: 220.9250 - val_loss: 93955.1250 - val_mse: 92755.2109 - val_mae: 253.9307\n",
      "Epoch 215/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 64594.9090 - mse: 63329.9555 - mae: 205.2471 - val_loss: 90793.9531 - val_mse: 89527.7500 - val_mae: 249.1836\n",
      "Epoch 216/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 62131.9517 - mse: 60710.4794 - mae: 198.6363 - val_loss: 82444.9766 - val_mse: 81195.1719 - val_mae: 236.5744\n",
      "Epoch 217/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61838.5178 - mse: 60553.2054 - mae: 195.7490 - val_loss: 80584.1094 - val_mse: 79398.5859 - val_mae: 234.0115\n",
      "Epoch 218/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68936.7477 - mse: 67692.2054 - mae: 217.6284 - val_loss: 82647.5938 - val_mse: 81589.0938 - val_mae: 237.0354\n",
      "Epoch 219/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 83814.7322 - mse: 82766.6402 - mae: 233.0467 - val_loss: 78200.2812 - val_mse: 77037.2188 - val_mae: 230.3616\n",
      "Epoch 220/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67153.6252 - mse: 66063.5376 - mae: 209.7802 - val_loss: 88396.9531 - val_mse: 87192.8594 - val_mae: 246.2294\n",
      "Epoch 221/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64890.2819 - mse: 63711.9418 - mae: 204.8430 - val_loss: 62966.6680 - val_mse: 61720.7539 - val_mae: 205.5039\n",
      "Epoch 222/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 74055.5468 - mse: 72882.9958 - mae: 213.2566 - val_loss: 92437.4922 - val_mse: 91334.7734 - val_mae: 252.0383\n",
      "Epoch 223/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63717.8856 - mse: 62547.5096 - mae: 201.7634 - val_loss: 83258.3047 - val_mse: 82186.4922 - val_mae: 238.4034\n",
      "Epoch 224/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67016.7436 - mse: 65975.3234 - mae: 208.2805 - val_loss: 93176.1953 - val_mse: 91990.0781 - val_mae: 253.0558\n",
      "Epoch 225/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65656.7079 - mse: 64481.7458 - mae: 204.5798 - val_loss: 92476.5469 - val_mse: 91480.0156 - val_mae: 252.5969\n",
      "Epoch 226/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69489.7463 - mse: 68434.3738 - mae: 212.9112 - val_loss: 81388.9062 - val_mse: 80273.8984 - val_mae: 235.7238\n",
      "Epoch 227/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68377.3140 - mse: 67273.3106 - mae: 209.9087 - val_loss: 83253.0625 - val_mse: 82129.1172 - val_mae: 238.5874\n",
      "Epoch 228/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62221.3185 - mse: 61151.8708 - mae: 203.5539 - val_loss: 83162.4922 - val_mse: 82176.1094 - val_mae: 238.7328\n",
      "Epoch 229/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63720.3618 - mse: 62678.2826 - mae: 202.4986 - val_loss: 81864.5234 - val_mse: 80745.7812 - val_mae: 236.1480\n",
      "Epoch 230/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66806.1851 - mse: 65696.3847 - mae: 204.1023 - val_loss: 102748.2344 - val_mse: 101820.2500 - val_mae: 266.9506\n",
      "Epoch 231/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57754.1802 - mse: 56685.3335 - mae: 193.4519 - val_loss: 80617.6875 - val_mse: 79534.5859 - val_mae: 234.1441\n",
      "Epoch 232/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62261.3723 - mse: 61222.2368 - mae: 203.6484 - val_loss: 79427.1797 - val_mse: 78501.5703 - val_mae: 233.1173\n",
      "Epoch 233/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70675.7363 - mse: 69768.1099 - mae: 210.8752 - val_loss: 80852.6016 - val_mse: 79941.1797 - val_mae: 235.2431\n",
      "Epoch 234/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72527.9426 - mse: 71446.4190 - mae: 217.8039 - val_loss: 84178.8750 - val_mse: 83307.9375 - val_mae: 240.5450\n",
      "Epoch 235/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60972.8062 - mse: 60013.5077 - mae: 201.6689 - val_loss: 81427.8438 - val_mse: 80556.6875 - val_mae: 236.3419\n",
      "Epoch 236/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66366.1168 - mse: 65433.7708 - mae: 207.6270 - val_loss: 72479.6094 - val_mse: 71473.1562 - val_mae: 221.7606\n",
      "Epoch 237/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71356.5771 - mse: 70328.9087 - mae: 212.8699 - val_loss: 76436.4453 - val_mse: 75507.9297 - val_mae: 228.5131\n",
      "Epoch 238/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63325.0318 - mse: 62419.6627 - mae: 200.3870 - val_loss: 79138.2344 - val_mse: 78297.9531 - val_mae: 233.0074\n",
      "Epoch 239/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 74169.1834 - mse: 73370.1472 - mae: 219.6036 - val_loss: 92083.2500 - val_mse: 91312.8359 - val_mae: 252.7887\n",
      "Epoch 240/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62813.3155 - mse: 61938.4156 - mae: 197.8966 - val_loss: 78989.4766 - val_mse: 78099.9062 - val_mae: 232.4909\n",
      "Epoch 241/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 54611.7506 - mse: 53738.8300 - mae: 186.6499 - val_loss: 71182.0703 - val_mse: 70207.0391 - val_mae: 219.8698\n",
      "Epoch 242/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65952.0090 - mse: 64996.4615 - mae: 212.8147 - val_loss: 66338.9766 - val_mse: 65380.7852 - val_mae: 211.7036\n",
      "Epoch 243/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63533.1870 - mse: 62608.4361 - mae: 202.2907 - val_loss: 65381.9492 - val_mse: 64359.8789 - val_mae: 209.5451\n",
      "Epoch 244/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 61659.9514 - mse: 60655.2035 - mae: 194.1271 - val_loss: 86510.0469 - val_mse: 85748.1094 - val_mae: 244.3849\n",
      "Epoch 245/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63297.3337 - mse: 62488.3069 - mae: 200.7846 - val_loss: 86378.2500 - val_mse: 85619.9844 - val_mae: 244.1360\n",
      "Epoch 246/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55179.0675 - mse: 54371.6707 - mae: 185.9538 - val_loss: 83611.0938 - val_mse: 82792.6406 - val_mae: 239.7968\n",
      "Epoch 247/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66464.3663 - mse: 65641.0881 - mae: 206.2511 - val_loss: 65769.4688 - val_mse: 64891.7344 - val_mae: 210.8312\n",
      "Epoch 248/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70590.1447 - mse: 69800.2553 - mae: 217.9059 - val_loss: 67364.4375 - val_mse: 66499.5156 - val_mae: 213.3910\n",
      "Epoch 249/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 67856.4422 - mse: 67054.9189 - mae: 205.7226 - val_loss: 89130.6641 - val_mse: 88499.2188 - val_mae: 248.3260\n",
      "Epoch 250/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68519.1494 - mse: 67763.9906 - mae: 213.2222 - val_loss: 97919.1328 - val_mse: 97247.2109 - val_mae: 261.1303\n",
      "Epoch 251/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64625.9006 - mse: 63765.5127 - mae: 208.0176 - val_loss: 79518.3750 - val_mse: 78584.9531 - val_mae: 233.2349\n",
      "Epoch 252/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61134.5914 - mse: 60227.4510 - mae: 204.6489 - val_loss: 97789.9219 - val_mse: 97043.3516 - val_mae: 260.9437\n",
      "Epoch 253/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57065.9078 - mse: 56183.6576 - mae: 190.3797 - val_loss: 78462.6875 - val_mse: 77606.4844 - val_mae: 231.3575\n",
      "Epoch 254/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64248.7798 - mse: 63382.4781 - mae: 205.4716 - val_loss: 97293.6094 - val_mse: 96425.9609 - val_mae: 260.0970\n",
      "Epoch 255/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65865.2243 - mse: 64932.7717 - mae: 207.5518 - val_loss: 81497.3438 - val_mse: 80604.4766 - val_mae: 236.1914\n",
      "Epoch 256/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58574.6900 - mse: 57610.7700 - mae: 189.4617 - val_loss: 75160.5156 - val_mse: 74349.3984 - val_mae: 226.3945\n",
      "Epoch 257/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58573.9985 - mse: 57815.5358 - mae: 194.5728 - val_loss: 78314.4297 - val_mse: 77504.2578 - val_mae: 231.4476\n",
      "Epoch 258/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59619.8148 - mse: 58766.9543 - mae: 192.7722 - val_loss: 80980.2578 - val_mse: 80280.2578 - val_mae: 235.6725\n",
      "Epoch 259/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60991.2844 - mse: 60251.0019 - mae: 201.6583 - val_loss: 68516.0625 - val_mse: 67702.3047 - val_mae: 214.9413\n",
      "Epoch 260/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60753.6881 - mse: 59976.1107 - mae: 201.0014 - val_loss: 89843.2188 - val_mse: 89123.9219 - val_mae: 249.1799\n",
      "Epoch 261/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68716.1810 - mse: 67971.8648 - mae: 208.1037 - val_loss: 76627.0078 - val_mse: 75782.8828 - val_mae: 228.4608\n",
      "Epoch 262/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65500.3128 - mse: 64628.0884 - mae: 201.3157 - val_loss: 66591.9922 - val_mse: 65720.1250 - val_mae: 211.7081\n",
      "Epoch 263/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61898.1656 - mse: 61063.0292 - mae: 193.4141 - val_loss: 79542.1719 - val_mse: 78773.6953 - val_mae: 233.3684\n",
      "Epoch 264/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66888.5174 - mse: 66177.8000 - mae: 206.2089 - val_loss: 71487.3359 - val_mse: 70730.5781 - val_mae: 220.3504\n",
      "Epoch 265/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61203.0236 - mse: 60463.7407 - mae: 200.4915 - val_loss: 70105.0469 - val_mse: 69378.3438 - val_mae: 217.8261\n",
      "Epoch 266/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61277.8226 - mse: 60603.7242 - mae: 199.4080 - val_loss: 74873.4062 - val_mse: 74189.4688 - val_mae: 226.0134\n",
      "Epoch 267/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68042.1070 - mse: 67403.9655 - mae: 214.4766 - val_loss: 77451.9219 - val_mse: 76734.1875 - val_mae: 230.0004\n",
      "Epoch 268/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 61250.1236 - mse: 60499.7576 - mae: 195.3003 - val_loss: 73560.0703 - val_mse: 72738.9375 - val_mae: 223.5738\n",
      "Epoch 269/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65129.4141 - mse: 64318.8387 - mae: 200.5907 - val_loss: 86016.4219 - val_mse: 85313.6562 - val_mae: 243.4429\n",
      "Epoch 270/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72025.0014 - mse: 71272.5852 - mae: 223.5149 - val_loss: 84993.5547 - val_mse: 84239.8047 - val_mae: 241.6828\n",
      "Epoch 271/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57504.8188 - mse: 56681.4914 - mae: 197.1424 - val_loss: 84481.2422 - val_mse: 83775.9062 - val_mae: 240.9641\n",
      "Epoch 272/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59793.9629 - mse: 59015.7796 - mae: 198.2744 - val_loss: 74299.9062 - val_mse: 73551.5391 - val_mae: 224.9207\n",
      "Epoch 273/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60534.2840 - mse: 59782.5933 - mae: 200.1074 - val_loss: 85530.4922 - val_mse: 84860.6875 - val_mae: 242.6073\n",
      "Epoch 274/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63721.8652 - mse: 62836.6369 - mae: 205.8844 - val_loss: 77167.8672 - val_mse: 76352.0391 - val_mae: 229.4691\n",
      "Epoch 275/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65333.4502 - mse: 64539.3348 - mae: 206.8966 - val_loss: 75070.3281 - val_mse: 74348.9453 - val_mae: 226.3551\n",
      "Epoch 276/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64132.8904 - mse: 63386.8191 - mae: 202.0800 - val_loss: 83877.3984 - val_mse: 83089.0469 - val_mae: 240.3711\n",
      "Epoch 277/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60598.7031 - mse: 59764.8243 - mae: 191.8795 - val_loss: 73157.9531 - val_mse: 72324.8438 - val_mae: 222.7905\n",
      "Epoch 278/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62541.6404 - mse: 61750.1334 - mae: 202.9355 - val_loss: 95088.5859 - val_mse: 94389.0391 - val_mae: 257.1987\n",
      "Epoch 279/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75200.8106 - mse: 74522.4372 - mae: 225.0320 - val_loss: 84702.3047 - val_mse: 84014.9453 - val_mae: 241.9360\n",
      "Epoch 280/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60535.7252 - mse: 59777.6447 - mae: 204.8901 - val_loss: 65323.5117 - val_mse: 64453.2930 - val_mae: 210.5929\n",
      "Epoch 281/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63300.9527 - mse: 62417.6855 - mae: 202.2987 - val_loss: 72839.3281 - val_mse: 71995.1875 - val_mae: 223.0575\n",
      "Epoch 282/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58412.1338 - mse: 57579.9995 - mae: 194.6655 - val_loss: 70955.6875 - val_mse: 70040.0312 - val_mae: 219.7460\n",
      "Epoch 283/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55765.7080 - mse: 54894.2486 - mae: 186.0385 - val_loss: 84812.7109 - val_mse: 84025.2422 - val_mae: 241.9097\n",
      "Epoch 284/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60542.2161 - mse: 59708.5061 - mae: 197.8846 - val_loss: 74105.6406 - val_mse: 73286.0938 - val_mae: 225.0005\n",
      "Epoch 285/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64363.0589 - mse: 63520.2326 - mae: 204.8333 - val_loss: 65183.4727 - val_mse: 64296.9688 - val_mae: 210.0312\n",
      "Epoch 286/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65867.9964 - mse: 65020.3184 - mae: 205.2051 - val_loss: 76351.5234 - val_mse: 75607.8438 - val_mae: 228.5941\n",
      "Epoch 287/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67223.5777 - mse: 66570.9928 - mae: 210.3147 - val_loss: 75288.0234 - val_mse: 74412.1406 - val_mae: 226.5779\n",
      "Epoch 288/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58496.4994 - mse: 57637.2399 - mae: 194.6456 - val_loss: 73152.0781 - val_mse: 72379.5469 - val_mae: 223.0257\n",
      "Epoch 289/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69326.6049 - mse: 68513.0613 - mae: 213.1330 - val_loss: 86618.5469 - val_mse: 85945.1328 - val_mae: 244.4854\n",
      "Epoch 290/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59845.1686 - mse: 59046.7177 - mae: 196.1669 - val_loss: 76452.3672 - val_mse: 75644.6484 - val_mae: 228.3112\n",
      "Epoch 291/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58414.3013 - mse: 57635.2779 - mae: 188.8856 - val_loss: 73437.4297 - val_mse: 72624.5234 - val_mae: 223.4080\n",
      "Epoch 292/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 58242.6809 - mse: 57473.2889 - mae: 199.1398 - val_loss: 81127.7734 - val_mse: 80313.0469 - val_mae: 235.7384\n",
      "Epoch 293/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67183.0520 - mse: 66383.7953 - mae: 214.3250 - val_loss: 75467.4922 - val_mse: 74619.6094 - val_mae: 226.7217\n",
      "Epoch 294/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64829.9692 - mse: 64029.3394 - mae: 209.3634 - val_loss: 75618.5156 - val_mse: 74824.0234 - val_mae: 227.1230\n",
      "Epoch 295/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 54502.2601 - mse: 53642.0062 - mae: 186.0842 - val_loss: 74306.9844 - val_mse: 73544.7734 - val_mae: 224.9271\n",
      "Epoch 296/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55471.0853 - mse: 54680.4316 - mae: 189.6249 - val_loss: 76067.8359 - val_mse: 75323.0547 - val_mae: 228.0292\n",
      "Epoch 297/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70801.4075 - mse: 70081.9821 - mae: 216.1810 - val_loss: 73815.8203 - val_mse: 73066.4219 - val_mae: 224.2645\n",
      "Epoch 298/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62130.4744 - mse: 61364.2645 - mae: 204.4146 - val_loss: 80005.0625 - val_mse: 79334.5234 - val_mae: 234.2383\n",
      "Epoch 299/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63584.6718 - mse: 62799.9351 - mae: 202.4890 - val_loss: 92145.4297 - val_mse: 91485.6250 - val_mae: 252.8167\n",
      "Epoch 300/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61368.1365 - mse: 60645.0780 - mae: 199.7166 - val_loss: 65872.3359 - val_mse: 65067.6641 - val_mae: 210.8418\n"
     ]
    }
   ],
   "source": [
    "history = nn.fit(X_train, y_train, epochs=300, batch_size=8, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(learning_rate = 0.01, activation = 'relu'):\n",
    "  \n",
    "    # Create an Adam optimizer with the given learning rate\n",
    "    opt = Adam(lr=learning_rate)\n",
    "  \n",
    "    # Create your binary classification model  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, \n",
    "                    activation = activation,\n",
    "                    input_shape = (14, ),\n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(128,\n",
    "                    activation = activation, \n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(1, activation = activation))\n",
    "# Compile the model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model,\n",
    "                       verbose=1)\n",
    "\n",
    "params = {'activation': [\"relu\"],\n",
    "          'batch_size': [16, 8], \n",
    "          'epochs': [200, 300, 500],\n",
    "          'learning_rate': [0.01, 0.05, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "35/35 [==============================] - 1s 3ms/step - loss: 42773504.4722 - mse: 42342606.0278 - mae: 2173.4422\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 791243.5017 - mse: 343688.2773 - mae: 374.1100\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 599233.4635 - mse: 163702.1489 - mae: 344.5045\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 617272.2361 - mse: 175377.5738 - mae: 355.8545\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 698073.3194 - mse: 281650.0508 - mae: 388.0695\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 558879.3403 - mse: 175181.8876 - mae: 358.7147\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 538716.9418 - mse: 201056.2261 - mae: 368.6783\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 533337.6632 - mse: 215368.9128 - mae: 361.0610\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 523267.7622 - mse: 201130.1467 - mae: 375.3800\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 501195.3151 - mse: 190509.8655 - mae: 359.2660\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 571165.8967 - mse: 259401.1970 - mae: 370.7669\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 477642.9358 - mse: 179165.1584 - mae: 362.7343\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 452122.5486 - mse: 161967.3238 - mae: 345.5068\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 475909.3273 - mse: 185022.2886 - mae: 356.3519\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 773525.6936 - mse: 476644.4792 - mae: 387.6443\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 455158.8446 - mse: 170167.5200 - mae: 353.3087\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 467355.2691 - mse: 172548.1749 - mae: 360.9477\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 454286.6780 - mse: 175113.2083 - mae: 357.0916\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 484526.5495 - mse: 198315.5638 - mae: 362.6025\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 445027.5990 - mse: 174698.6888 - mae: 358.6309\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 450471.5608 - mse: 187578.0638 - mae: 360.2853\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 468040.5130 - mse: 200646.5938 - mae: 350.4988\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 459226.6953 - mse: 198371.2253 - mae: 358.2195\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 424191.7222 - mse: 174068.0586 - mae: 359.3419\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 421457.9948 - mse: 175018.4258 - mae: 356.9752\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 452886.6606 - mse: 196664.5773 - mae: 374.3170\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 415291.1128 - mse: 172765.6515 - mae: 359.6816\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 428276.8368 - mse: 189245.2565 - mae: 355.8030\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 426406.3090 - mse: 188616.3260 - mae: 364.9798\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 405713.5139 - mse: 167205.3242 - mae: 348.1327\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 388160.5130 - mse: 162450.6979 - mae: 349.5202\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 408010.5226 - mse: 179966.3598 - mae: 359.7776\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 393218.9887 - mse: 166295.9275 - mae: 350.4059\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 392318.1658 - mse: 169086.5569 - mae: 350.9521\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 397205.5859 - mse: 183378.4518 - mae: 374.7883\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 396298.1016 - mse: 177182.8294 - mae: 360.8819\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 419541.1979 - mse: 208394.4883 - mae: 375.6002\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 386835.5608 - mse: 180446.0156 - mae: 369.2658\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 381166.8524 - mse: 167340.2622 - mae: 350.8704\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 385541.3976 - mse: 182796.0929 - mae: 364.8366\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 387949.8472 - mse: 178327.0247 - mae: 361.7069\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 361697.1415 - mse: 167416.0191 - mae: 350.4571\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 361088.1016 - mse: 167522.8832 - mae: 354.8447\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 357531.1753 - mse: 169321.7070 - mae: 355.6155\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 352903.7717 - mse: 169770.7921 - mae: 353.2532\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 372330.6632 - mse: 183786.5156 - mae: 368.8762\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 364643.9583 - mse: 186652.2313 - mae: 374.1174\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 341064.3411 - mse: 162667.2947 - mae: 345.5797\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 350190.7483 - mse: 176074.9062 - mae: 364.8098\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 359203.2500 - mse: 183472.6671 - mae: 371.1960\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 349566.7786 - mse: 179631.0794 - mae: 360.2930\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 334965.3090 - mse: 171955.4162 - mae: 356.3491\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 326083.9479 - mse: 164892.6393 - mae: 345.0008\n",
      "Epoch 54/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 341410.6641 - mse: 182605.8941 - mae: 370.3749\n",
      "Epoch 55/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 335795.8637 - mse: 183265.2257 - mae: 372.2160\n",
      "Epoch 56/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 338615.1727 - mse: 179984.2704 - mae: 368.6148\n",
      "Epoch 57/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 319495.6076 - mse: 172133.0208 - mae: 357.7871\n",
      "Epoch 58/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 326133.3585 - mse: 179768.4045 - mae: 355.7496\n",
      "Epoch 59/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 314937.8958 - mse: 171802.2604 - mae: 356.2446\n",
      "Epoch 60/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 313873.3090 - mse: 169722.4097 - mae: 357.0433\n",
      "Epoch 61/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 311087.1120 - mse: 172184.6910 - mae: 358.4645\n",
      "Epoch 62/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 315916.2179 - mse: 180954.5193 - mae: 352.6691\n",
      "Epoch 63/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 296883.1602 - mse: 168406.9666 - mae: 355.5064\n",
      "Epoch 64/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 321168.6970 - mse: 189503.9887 - mae: 378.4973\n",
      "Epoch 65/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 314210.5043 - mse: 183495.3828 - mae: 371.7150\n",
      "Epoch 66/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 298124.5321 - mse: 169864.7244 - mae: 345.7901\n",
      "Epoch 67/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 301410.7144 - mse: 174695.3733 - mae: 358.9251\n",
      "Epoch 68/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 305322.7865 - mse: 180765.9961 - mae: 365.1961\n",
      "Epoch 69/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 288467.3264 - mse: 169910.0000 - mae: 353.6068\n",
      "Epoch 70/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 288974.8125 - mse: 169446.9462 - mae: 352.0932\n",
      "Epoch 71/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 285999.0386 - mse: 171995.7352 - mae: 357.2160\n",
      "Epoch 72/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 288236.3837 - mse: 169960.0204 - mae: 349.4718\n",
      "Epoch 73/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 284220.6801 - mse: 170702.2135 - mae: 357.2816\n",
      "Epoch 74/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 305903.7014 - mse: 193989.7231 - mae: 373.8127\n",
      "Epoch 75/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 289647.2969 - mse: 182429.9102 - mae: 372.2384\n",
      "Epoch 76/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 271266.6966 - mse: 167568.1788 - mae: 352.7608\n",
      "Epoch 77/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 299757.7969 - mse: 191918.8312 - mae: 380.9471\n",
      "Epoch 78/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 287283.9288 - mse: 182129.2309 - mae: 373.2940\n",
      "Epoch 79/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 273575.6758 - mse: 174657.8225 - mae: 356.4943\n",
      "Epoch 80/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 274084.5894 - mse: 178591.1810 - mae: 364.7565\n",
      "Epoch 81/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 340843.6432 - mse: 246356.2613 - mae: 382.3713\n",
      "Epoch 82/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 266659.8103 - mse: 169555.2474 - mae: 351.7243\n",
      "Epoch 83/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 262951.3798 - mse: 169434.2224 - mae: 356.1490\n",
      "Epoch 84/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 265073.1063 - mse: 173119.7142 - mae: 355.6932\n",
      "Epoch 85/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 364127.6115 - mse: 272233.5117 - mae: 376.5066\n",
      "Epoch 86/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 262666.0582 - mse: 172024.7847 - mae: 356.5249\n",
      "Epoch 87/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 258639.9562 - mse: 173184.3798 - mae: 358.9322\n",
      "Epoch 88/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 252676.2118 - mse: 167554.4314 - mae: 354.3451\n",
      "Epoch 89/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 330826.5139 - mse: 247594.8611 - mae: 374.0992\n",
      "Epoch 90/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 254013.1984 - mse: 174685.8069 - mae: 363.9677\n",
      "Epoch 91/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 249492.3134 - mse: 169692.3815 - mae: 350.4067\n",
      "Epoch 92/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 256421.5512 - mse: 177508.0642 - mae: 364.5334\n",
      "Epoch 93/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 246583.4045 - mse: 171335.1701 - mae: 353.5042\n",
      "Epoch 94/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 243245.1710 - mse: 172205.5686 - mae: 356.3981\n",
      "Epoch 95/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 249059.1098 - mse: 177865.0590 - mae: 365.4373\n",
      "Epoch 96/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 262358.0065 - mse: 194586.9987 - mae: 371.4449\n",
      "Epoch 97/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 233592.5317 - mse: 165745.1541 - mae: 347.4188\n",
      "Epoch 98/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 238899.7109 - mse: 172499.2604 - mae: 358.6798\n",
      "Epoch 99/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 244284.3142 - mse: 180170.2296 - mae: 368.1012\n",
      "Epoch 100/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 240391.2049 - mse: 178695.1016 - mae: 364.2896\n",
      "Epoch 101/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 253758.3880 - mse: 191633.4714 - mae: 378.0517\n",
      "Epoch 102/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 231628.6914 - mse: 170853.4818 - mae: 354.8042\n",
      "Epoch 103/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 232651.8559 - mse: 173023.5391 - mae: 357.4871\n",
      "Epoch 104/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 260836.5425 - mse: 202717.4336 - mae: 379.4515\n",
      "Epoch 105/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 226761.9327 - mse: 170199.4484 - mae: 354.8823\n",
      "Epoch 106/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 214472.2240 - mse: 162105.4852 - mae: 345.5330\n",
      "Epoch 107/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 219221.8976 - mse: 166811.1237 - mae: 351.4264\n",
      "Epoch 108/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 224070.5608 - mse: 173955.9692 - mae: 358.3748\n",
      "Epoch 109/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 228157.7031 - mse: 177088.0230 - mae: 362.7828\n",
      "Epoch 110/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 229853.2478 - mse: 182765.3633 - mae: 367.5549\n",
      "Epoch 111/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 226461.2843 - mse: 179521.4288 - mae: 370.4518\n",
      "Epoch 112/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 216612.7661 - mse: 170593.3108 - mae: 353.4477\n",
      "Epoch 113/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 231776.7287 - mse: 185676.6480 - mae: 375.3021\n",
      "Epoch 114/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 221495.5825 - mse: 177023.6580 - mae: 364.1768\n",
      "Epoch 115/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 205114.0490 - mse: 164185.4870 - mae: 347.1734\n",
      "Epoch 116/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 209228.9588 - mse: 169047.5291 - mae: 352.8793\n",
      "Epoch 117/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 216122.4049 - mse: 176400.1463 - mae: 361.7442\n",
      "Epoch 118/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 216093.1298 - mse: 177482.9939 - mae: 363.4332\n",
      "Epoch 119/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 218248.5174 - mse: 178832.8303 - mae: 366.6265\n",
      "Epoch 120/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 206874.7027 - mse: 169896.9362 - mae: 352.5787\n",
      "Epoch 121/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 204864.1111 - mse: 170380.5035 - mae: 354.7903\n",
      "Epoch 122/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 206266.5616 - mse: 172588.2578 - mae: 353.8304\n",
      "Epoch 123/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 208311.2075 - mse: 174300.8260 - mae: 362.2139\n",
      "Epoch 124/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 198403.5065 - mse: 165694.9536 - mae: 351.2949\n",
      "Epoch 125/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 193928.0855 - mse: 162271.1480 - mae: 343.1825\n",
      "Epoch 126/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 210952.9774 - mse: 180275.9987 - mae: 373.4135\n",
      "Epoch 127/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 220380.5178 - mse: 190476.5436 - mae: 365.1380\n",
      "Epoch 128/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 212456.4323 - mse: 182550.5490 - mae: 370.2207\n",
      "Epoch 129/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 190638.0720 - mse: 161648.6024 - mae: 345.8569\n",
      "Epoch 130/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 196054.9427 - mse: 168764.2852 - mae: 352.1857\n",
      "Epoch 131/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 193349.6853 - mse: 165844.6927 - mae: 351.4095\n",
      "Epoch 132/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 188455.0000 - mse: 161755.8594 - mae: 345.8516\n",
      "Epoch 133/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 189424.5968 - mse: 163681.2886 - mae: 344.6254\n",
      "Epoch 134/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 179756.8780 - mse: 154053.9093 - mae: 327.1440\n",
      "Epoch 135/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 181220.7114 - mse: 156204.5213 - mae: 336.0664\n",
      "Epoch 136/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 173070.5009 - mse: 148362.3257 - mae: 324.9164\n",
      "Epoch 137/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 174102.5178 - mse: 149393.4627 - mae: 323.7664\n",
      "Epoch 138/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 164869.6372 - mse: 140456.4275 - mae: 311.6980\n",
      "Epoch 139/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 173861.2945 - mse: 149716.5953 - mae: 314.8267\n",
      "Epoch 140/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 164819.9638 - mse: 139945.6840 - mae: 314.0353\n",
      "Epoch 141/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 179142.9488 - mse: 153872.2687 - mae: 330.9612\n",
      "Epoch 142/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 158201.7287 - mse: 133856.5590 - mae: 304.2462\n",
      "Epoch 143/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 170292.7231 - mse: 147067.6178 - mae: 314.3246\n",
      "Epoch 144/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 144157.7899 - mse: 121445.8210 - mae: 279.8963\n",
      "Epoch 145/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 160748.4770 - mse: 138532.7326 - mae: 307.4050\n",
      "Epoch 146/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 176753.1476 - mse: 155452.7639 - mae: 322.1634\n",
      "Epoch 147/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 148924.7882 - mse: 128328.0467 - mae: 299.9252\n",
      "Epoch 148/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 149877.9744 - mse: 129761.7185 - mae: 295.4421\n",
      "Epoch 149/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 142101.7266 - mse: 122086.0796 - mae: 287.5983\n",
      "Epoch 150/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 159609.1862 - mse: 140106.7958 - mae: 308.7117\n",
      "Epoch 151/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 150766.7218 - mse: 131911.7943 - mae: 299.0607\n",
      "Epoch 152/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 135330.8487 - mse: 117804.2561 - mae: 279.4393\n",
      "Epoch 153/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 138306.8602 - mse: 121493.1480 - mae: 285.6946\n",
      "Epoch 154/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 122053.0228 - mse: 105360.7789 - mae: 265.3136\n",
      "Epoch 155/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 147449.5265 - mse: 131148.7040 - mae: 293.3601\n",
      "Epoch 156/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 129701.4143 - mse: 114307.2860 - mae: 263.8480\n",
      "Epoch 157/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 126461.4158 - mse: 111400.3075 - mae: 267.6954\n",
      "Epoch 158/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 125407.2962 - mse: 110630.2211 - mae: 271.5475\n",
      "Epoch 159/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 118817.2454 - mse: 103967.5263 - mae: 263.6392\n",
      "Epoch 160/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 116964.7810 - mse: 102444.2441 - mae: 257.9612\n",
      "Epoch 161/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 118643.8307 - mse: 104945.5501 - mae: 258.5227\n",
      "Epoch 162/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 122721.5267 - mse: 109897.4295 - mae: 272.9707\n",
      "Epoch 163/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 113148.3852 - mse: 100970.2051 - mae: 262.5769\n",
      "Epoch 164/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 115212.3316 - mse: 103472.9731 - mae: 261.0225\n",
      "Epoch 165/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 109954.2591 - mse: 98953.4175 - mae: 252.7196\n",
      "Epoch 166/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 108683.2322 - mse: 98048.5382 - mae: 253.0473\n",
      "Epoch 167/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 114523.5729 - mse: 104621.3327 - mae: 263.2656\n",
      "Epoch 168/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 108019.0855 - mse: 98444.7849 - mae: 254.7996\n",
      "Epoch 169/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 97218.8405 - mse: 88322.1003 - mae: 244.6030\n",
      "Epoch 170/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 103241.2174 - mse: 94493.1921 - mae: 246.3308\n",
      "Epoch 171/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 101612.9464 - mse: 93021.1710 - mae: 254.1856\n",
      "Epoch 172/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 96017.0625 - mse: 87784.0404 - mae: 241.6284\n",
      "Epoch 173/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 105594.6717 - mse: 97828.2561 - mae: 251.3717\n",
      "Epoch 174/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 108105.7598 - mse: 100668.1432 - mae: 254.5116\n",
      "Epoch 175/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 107844.8911 - mse: 100716.8700 - mae: 259.6115\n",
      "Epoch 176/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 97040.4004 - mse: 90219.8579 - mae: 243.8795\n",
      "Epoch 177/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 90536.0560 - mse: 84041.3112 - mae: 234.1984\n",
      "Epoch 178/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 90690.3611 - mse: 84469.1646 - mae: 234.8773\n",
      "Epoch 179/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 92342.4036 - mse: 86588.4477 - mae: 241.3245\n",
      "Epoch 180/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 99128.1801 - mse: 93641.0458 - mae: 249.1860\n",
      "Epoch 181/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91427.2478 - mse: 85617.9542 - mae: 232.5190\n",
      "Epoch 182/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 89555.6144 - mse: 84470.9991 - mae: 234.4035\n",
      "Epoch 183/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 103635.0996 - mse: 98596.5770 - mae: 254.6733\n",
      "Epoch 184/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 82824.6019 - mse: 78018.4624 - mae: 221.0447\n",
      "Epoch 185/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 90246.9384 - mse: 85617.3129 - mae: 237.0184\n",
      "Epoch 186/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91399.9473 - mse: 87230.7496 - mae: 240.9278\n",
      "Epoch 187/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 102194.1793 - mse: 97994.0173 - mae: 250.6722\n",
      "Epoch 188/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 94650.0734 - mse: 90186.9397 - mae: 245.3249\n",
      "Epoch 189/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 92882.4408 - mse: 88491.7617 - mae: 238.8499\n",
      "Epoch 190/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91329.7007 - mse: 87306.9991 - mae: 237.3871\n",
      "Epoch 191/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 90580.2159 - mse: 86790.4800 - mae: 233.6394\n",
      "Epoch 192/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 83753.9020 - mse: 80213.1898 - mae: 225.1502\n",
      "Epoch 193/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 81999.9901 - mse: 78421.6228 - mae: 220.1134\n",
      "Epoch 194/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 84078.3375 - mse: 80632.8668 - mae: 226.5048\n",
      "Epoch 195/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 81879.5641 - mse: 78481.7784 - mae: 220.5176\n",
      "Epoch 196/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 83253.5894 - mse: 79961.6456 - mae: 227.4165\n",
      "Epoch 197/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77037.8778 - mse: 73811.3685 - mae: 219.1923\n",
      "Epoch 198/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 90169.4262 - mse: 87024.8127 - mae: 240.0064\n",
      "Epoch 199/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 84844.5000 - mse: 81813.9023 - mae: 229.3623\n",
      "Epoch 200/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 86421.1970 - mse: 83579.1940 - mae: 239.2059\n",
      "CPU times: user 32.2 s, sys: 4.7 s, total: 36.9 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(model,\n",
    "                                   param_distributions=params, n_jobs=-1)\n",
    "\n",
    "random_search_results = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'epochs': 200, 'batch_size': 16, 'activation': 'relu'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tuned = random_search_results.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (1.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.5.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error test: 146.64806607495183\n",
      "Mean Absolute Error train: 1.9142198964305546\n"
     ]
    }
   ],
   "source": [
    "xgbr = XGBRegressor()\n",
    "\n",
    "xgbr.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_metric=\"mae\",  \n",
    "    verbose=True)\n",
    "\n",
    "predictions = xgbr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error test: \" + str(mean_absolute_error(predictions, y_test)))\n",
    "print(\"Mean Absolute Error train: \" + str(mean_absolute_error(xgbr.predict(X_train), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8429075.139029678, tolerance: 2469.0218836503623\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  191.264376\n",
      "Mean MAE: 148.480 (10.995)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "lasso_reg = Lasso(alpha=0.0)\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "pred = lasso_reg.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "scores = cross_val_score(lasso_reg, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "\n",
    "rmse = np.sqrt(MSE(y_test, pred))\n",
    "print(\"RMSE : % f\" %(rmse))\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "             estimator=Lasso(), n_jobs=-1,\n",
       "             param_grid={'alpha': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99])},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search hyperparameters for lasso regression\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# define model\n",
    "model = Lasso()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = arange(0, 1, 0.01)\n",
    "# define search\n",
    "lasso_tuned = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "lasso_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 151.388 (14.254)\n"
     ]
    }
   ],
   "source": [
    "# evaluate an ridge regression model on the dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define model\n",
    "ridge_reg = Ridge(alpha=0.00)\n",
    "# define model evaluation method\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(ridge_reg, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.12581e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.34077e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.990000\n",
      "MAE: -148.477\n"
     ]
    }
   ],
   "source": [
    "from numpy import arange\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ridge_tune2 = RidgeCV(alphas=arange(0, 1, 0.01), cv=cv, scoring='neg_mean_absolute_error')\n",
    "# fit model\n",
    "ridge_tune2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = svm.SVR()\n",
    "sv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance with twitter BERT and news sentiments\n",
      "Epoch 1/200\n",
      "35/35 [==============================] - 1s 3ms/step - loss: 6922489.3542 - mse: 6434120.6319 - mae: 882.6084\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 699266.3108 - mse: 241484.8941 - mae: 384.5159\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 625268.2726 - mse: 181392.2821 - mae: 364.6694\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 582949.5729 - mse: 191810.6068 - mae: 363.0069\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 599771.9115 - mse: 252850.7834 - mae: 361.4865\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 640038.4644 - mse: 296787.6419 - mae: 379.9117\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 518589.7014 - mse: 180482.3694 - mae: 360.0575\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 497181.3585 - mse: 177047.5404 - mae: 360.7272\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 528607.2830 - mse: 208955.2365 - mae: 366.7815\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 512246.0851 - mse: 207775.3355 - mae: 361.2910\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 474065.3411 - mse: 167876.2405 - mae: 352.6073\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 555077.9583 - mse: 252670.2730 - mae: 396.5948\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 455934.1806 - mse: 167504.7370 - mae: 353.6908\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 487734.8802 - mse: 197446.7960 - mae: 358.3466\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 467041.0903 - mse: 191454.1315 - mae: 368.7094\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 452400.2977 - mse: 175163.6793 - mae: 359.7319\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 486931.3872 - mse: 215155.3216 - mae: 370.4356\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 499644.4965 - mse: 224217.2635 - mae: 376.2683\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 479118.5972 - mse: 213856.6458 - mae: 378.1206\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 452188.5582 - mse: 190886.0486 - mae: 370.1050\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 424022.7205 - mse: 166279.6471 - mae: 350.9815\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 429892.3976 - mse: 174831.9501 - mae: 363.8056\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 439434.0382 - mse: 191772.6710 - mae: 369.5966\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 410103.0720 - mse: 171007.2049 - mae: 358.0786\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 421527.0833 - mse: 183001.9878 - mae: 371.2107\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 401490.4297 - mse: 171366.1011 - mae: 354.7239\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 409153.5052 - mse: 185410.2426 - mae: 354.0654\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 437594.6736 - mse: 226773.8299 - mae: 370.3789\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 396904.6181 - mse: 179647.6029 - mae: 366.9434\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 373736.1050 - mse: 169021.9180 - mae: 355.1250\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 377765.3281 - mse: 170446.4249 - mae: 354.4043\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 370444.3793 - mse: 179292.9010 - mae: 366.4475\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 358366.6536 - mse: 173548.5260 - mae: 356.4194\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 364949.5226 - mse: 173014.7721 - mae: 354.7652\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 377235.1111 - mse: 192854.0091 - mae: 366.6943\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 355145.4792 - mse: 171640.5827 - mae: 349.9339\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 350557.5512 - mse: 172016.4106 - mae: 355.6220\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 339831.7821 - mse: 165871.2305 - mae: 349.6205\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 346029.3429 - mse: 180080.7431 - mae: 365.9115\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 338979.1398 - mse: 175363.0213 - mae: 353.5376\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 335196.6033 - mse: 172129.2635 - mae: 355.7170\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 333970.8108 - mse: 175093.7856 - mae: 359.9433\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 329097.9826 - mse: 176010.1128 - mae: 358.3941\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 324855.0095 - mse: 177640.5734 - mae: 362.3335\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 315598.6997 - mse: 172171.0148 - mae: 355.6309\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 313266.3199 - mse: 173949.8746 - mae: 359.7698\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 320137.4002 - mse: 178589.3872 - mae: 366.6985\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 299672.0165 - mse: 164032.9145 - mae: 340.9169\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 300753.1484 - mse: 167209.6788 - mae: 345.8993\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 330876.3641 - mse: 203365.4392 - mae: 367.1093\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 301736.8516 - mse: 179255.1207 - mae: 365.8409\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 290593.2127 - mse: 171970.0451 - mae: 358.2415\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 286940.9362 - mse: 171314.6576 - mae: 357.7263\n",
      "Epoch 54/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 290328.6901 - mse: 178289.5369 - mae: 365.5436\n",
      "Epoch 55/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 285494.0000 - mse: 176591.1875 - mae: 359.6690\n",
      "Epoch 56/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 280096.2378 - mse: 173583.9965 - mae: 359.1196\n",
      "Epoch 57/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 280803.3069 - mse: 179735.8728 - mae: 366.6502\n",
      "Epoch 58/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 295893.5859 - mse: 196107.3759 - mae: 356.4231\n",
      "Epoch 59/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 276328.4002 - mse: 178602.6745 - mae: 364.3768\n",
      "Epoch 60/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 259619.6931 - mse: 165697.9789 - mae: 350.8329\n",
      "Epoch 61/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 254124.7535 - mse: 164645.6155 - mae: 350.3324\n",
      "Epoch 62/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 249587.0781 - mse: 160282.0664 - mae: 347.3837\n",
      "Epoch 63/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 236736.3203 - mse: 153371.9590 - mae: 334.0447\n",
      "Epoch 64/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 242354.3967 - mse: 161588.7027 - mae: 342.7571\n",
      "Epoch 65/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 230810.8069 - mse: 150181.7943 - mae: 328.8605\n",
      "Epoch 66/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 238935.7300 - mse: 161294.1903 - mae: 337.6758\n",
      "Epoch 67/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 208337.0786 - mse: 132992.1081 - mae: 299.7700\n",
      "Epoch 68/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 216361.2821 - mse: 142092.2233 - mae: 313.9603\n",
      "Epoch 69/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 219403.1363 - mse: 146472.8819 - mae: 319.4260\n",
      "Epoch 70/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 191614.1758 - mse: 120631.1957 - mae: 287.9271\n",
      "Epoch 71/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 191518.5126 - mse: 124154.8620 - mae: 285.8097\n",
      "Epoch 72/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 189728.0464 - mse: 121435.3631 - mae: 284.3197\n",
      "Epoch 73/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 187888.2444 - mse: 121972.1005 - mae: 287.8089\n",
      "Epoch 74/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 159008.7602 - mse: 95963.1104 - mae: 249.7586\n",
      "Epoch 75/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 149940.7431 - mse: 90360.3479 - mae: 242.2274\n",
      "Epoch 76/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 170654.1523 - mse: 109892.6532 - mae: 269.1447\n",
      "Epoch 77/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 168719.4102 - mse: 110096.6942 - mae: 278.6887\n",
      "Epoch 78/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 152099.0621 - mse: 98361.4836 - mae: 246.3557\n",
      "Epoch 79/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 160007.7066 - mse: 106921.5263 - mae: 263.4141\n",
      "Epoch 80/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 157302.7133 - mse: 107895.6890 - mae: 269.0166\n",
      "Epoch 81/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 148696.8409 - mse: 100306.7012 - mae: 255.1562\n",
      "Epoch 82/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 147924.3043 - mse: 100098.3639 - mae: 261.1586\n",
      "Epoch 83/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 146208.3203 - mse: 100638.9308 - mae: 258.6948\n",
      "Epoch 84/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 150373.7882 - mse: 107062.8906 - mae: 264.0189\n",
      "Epoch 85/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 143135.3438 - mse: 100126.4501 - mae: 252.3831\n",
      "Epoch 86/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 142931.7079 - mse: 101991.9464 - mae: 261.4907\n",
      "Epoch 87/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 142811.7708 - mse: 103859.6469 - mae: 261.3511\n",
      "Epoch 88/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 126816.9996 - mse: 90383.8058 - mae: 247.7764\n",
      "Epoch 89/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 129211.0332 - mse: 94268.8553 - mae: 246.4908\n",
      "Epoch 90/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 134193.7005 - mse: 100556.4360 - mae: 252.5270\n",
      "Epoch 91/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 122637.0052 - mse: 90738.9025 - mae: 237.0146\n",
      "Epoch 92/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 123444.0434 - mse: 93224.9084 - mae: 246.7025\n",
      "Epoch 93/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 117696.6573 - mse: 88996.6417 - mae: 235.5177\n",
      "Epoch 94/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 114054.5315 - mse: 87179.4668 - mae: 240.1256\n",
      "Epoch 95/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 111617.0176 - mse: 84919.0404 - mae: 233.0883\n",
      "Epoch 96/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 110219.5076 - mse: 84836.2793 - mae: 238.5089\n",
      "Epoch 97/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 126149.0684 - mse: 102632.1508 - mae: 262.6395\n",
      "Epoch 98/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 108588.1478 - mse: 86322.0406 - mae: 233.6912\n",
      "Epoch 99/200\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 116154.5660 - mse: 94184.7498 - mae: 253.5144\n",
      "Epoch 100/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 113457.8286 - mse: 92705.9477 - mae: 248.2637\n",
      "Epoch 101/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 108835.0844 - mse: 89630.2320 - mae: 244.5416\n",
      "Epoch 102/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 112788.6120 - mse: 93595.7496 - mae: 245.6205\n",
      "Epoch 103/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 112445.6908 - mse: 94879.6447 - mae: 249.4230\n",
      "Epoch 104/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 94492.5742 - mse: 76802.4219 - mae: 220.0949\n",
      "Epoch 105/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 92076.1615 - mse: 74636.9817 - mae: 215.0270\n",
      "Epoch 106/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91038.0694 - mse: 74545.8554 - mae: 224.0672\n",
      "Epoch 107/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 95750.5980 - mse: 79537.2906 - mae: 233.8623\n",
      "Epoch 108/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 92998.1539 - mse: 77803.0625 - mae: 232.1310\n",
      "Epoch 109/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 84787.8108 - mse: 70001.1791 - mae: 213.2109\n",
      "Epoch 110/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 93942.8353 - mse: 80981.0432 - mae: 232.3928\n",
      "Epoch 111/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 90040.5503 - mse: 77315.8231 - mae: 224.1781\n",
      "Epoch 112/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 89239.9323 - mse: 76448.8848 - mae: 222.3030\n",
      "Epoch 113/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 83049.4842 - mse: 71402.9698 - mae: 216.4037\n",
      "Epoch 114/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 86472.5369 - mse: 75346.6785 - mae: 216.0795\n",
      "Epoch 115/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77543.6370 - mse: 67441.5346 - mae: 205.7854\n",
      "Epoch 116/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77888.4684 - mse: 68252.7999 - mae: 207.7052\n",
      "Epoch 117/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 79530.6182 - mse: 69626.9642 - mae: 213.8412\n",
      "Epoch 118/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 89371.7663 - mse: 79822.9342 - mae: 228.3974\n",
      "Epoch 119/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 78092.7936 - mse: 69022.6963 - mae: 214.3709\n",
      "Epoch 120/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 73797.3256 - mse: 65118.3038 - mae: 205.2534\n",
      "Epoch 121/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 85431.8594 - mse: 76830.1957 - mae: 232.1295\n",
      "Epoch 122/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 80257.0332 - mse: 72718.3730 - mae: 221.6112\n",
      "Epoch 123/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 80320.5407 - mse: 72310.7531 - mae: 215.4579\n",
      "Epoch 124/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 85903.8969 - mse: 78616.0336 - mae: 224.8145\n",
      "Epoch 125/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 81819.6756 - mse: 74867.2626 - mae: 221.1206\n",
      "Epoch 126/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 76230.5968 - mse: 69539.6803 - mae: 205.8709\n",
      "Epoch 127/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 95667.6445 - mse: 89543.4217 - mae: 231.6557\n",
      "Epoch 128/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 73439.6814 - mse: 67270.0178 - mae: 211.5928\n",
      "Epoch 129/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 79272.8555 - mse: 73028.6905 - mae: 216.3848\n",
      "Epoch 130/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 78355.2378 - mse: 72256.4123 - mae: 218.8699\n",
      "Epoch 131/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 72619.2560 - mse: 66894.0901 - mae: 207.4275\n",
      "Epoch 132/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 72113.6028 - mse: 66406.9411 - mae: 207.1057\n",
      "Epoch 133/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77342.2932 - mse: 72011.9599 - mae: 219.4647\n",
      "Epoch 134/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 77052.1454 - mse: 71635.7055 - mae: 212.4428\n",
      "Epoch 135/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 68207.3577 - mse: 63093.1487 - mae: 206.1825\n",
      "Epoch 136/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 71704.0288 - mse: 66656.8070 - mae: 210.2761\n",
      "Epoch 137/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 69288.3296 - mse: 64078.9579 - mae: 202.4273\n",
      "Epoch 138/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63215.4858 - mse: 58409.7204 - mae: 195.8997\n",
      "Epoch 139/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63460.0317 - mse: 59112.9103 - mae: 197.5386\n",
      "Epoch 140/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 64453.7267 - mse: 60093.6935 - mae: 201.1407\n",
      "Epoch 141/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 72411.1131 - mse: 68209.6793 - mae: 209.0140\n",
      "Epoch 142/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 72818.5001 - mse: 68822.7462 - mae: 205.5047\n",
      "Epoch 143/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 72302.1272 - mse: 68357.5434 - mae: 212.4559\n",
      "Epoch 144/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 70372.8370 - mse: 66319.6030 - mae: 211.3599\n",
      "Epoch 145/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71968.6187 - mse: 68113.7649 - mae: 212.1627\n",
      "Epoch 146/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65996.0433 - mse: 62145.7171 - mae: 201.8676\n",
      "Epoch 147/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 66408.1525 - mse: 62621.1756 - mae: 204.9300\n",
      "Epoch 148/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 69277.5306 - mse: 65391.9760 - mae: 207.1127\n",
      "Epoch 149/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 70445.3430 - mse: 66904.9326 - mae: 213.6736\n",
      "Epoch 150/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 64867.4593 - mse: 60922.6489 - mae: 202.1334\n",
      "Epoch 151/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63959.9395 - mse: 59994.7827 - mae: 202.5259\n",
      "Epoch 152/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 68683.6519 - mse: 65237.3828 - mae: 207.6005\n",
      "Epoch 153/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 70802.7036 - mse: 66939.9741 - mae: 207.3063\n",
      "Epoch 154/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65356.4252 - mse: 61521.4965 - mae: 200.6823\n",
      "Epoch 155/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66580.7347 - mse: 62667.9882 - mae: 201.5297\n",
      "Epoch 156/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 70239.6069 - mse: 66464.8588 - mae: 209.5785\n",
      "Epoch 157/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 69466.2842 - mse: 66035.9597 - mae: 208.6107\n",
      "Epoch 158/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 55866.9054 - mse: 52614.9867 - mae: 184.5975\n",
      "Epoch 159/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 63928.3329 - mse: 60718.9161 - mae: 199.4519\n",
      "Epoch 160/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71676.5467 - mse: 68450.4141 - mae: 216.1364\n",
      "Epoch 161/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70737.4169 - mse: 67654.2553 - mae: 211.8701\n",
      "Epoch 162/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59029.8994 - mse: 55852.5636 - mae: 193.5606\n",
      "Epoch 163/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71851.9672 - mse: 68776.1568 - mae: 219.0761\n",
      "Epoch 164/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62226.7834 - mse: 59055.3643 - mae: 193.0736\n",
      "Epoch 165/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 68136.0073 - mse: 65026.3255 - mae: 204.7958\n",
      "Epoch 166/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 67915.2567 - mse: 65035.3448 - mae: 208.4615\n",
      "Epoch 167/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60881.8594 - mse: 58217.2217 - mae: 199.6884\n",
      "Epoch 168/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 64633.6838 - mse: 62055.7253 - mae: 202.2722\n",
      "Epoch 169/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 69464.1377 - mse: 67003.5305 - mae: 205.4886\n",
      "Epoch 170/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 64548.9902 - mse: 61819.7112 - mae: 200.9999\n",
      "Epoch 171/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70145.3813 - mse: 67510.6094 - mae: 213.3379\n",
      "Epoch 172/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 64595.5804 - mse: 62057.1886 - mae: 198.4814\n",
      "Epoch 173/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60374.6609 - mse: 57740.5221 - mae: 196.0149\n",
      "Epoch 174/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65582.7269 - mse: 63164.9967 - mae: 204.9551\n",
      "Epoch 175/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62682.6148 - mse: 60369.6394 - mae: 200.6336\n",
      "Epoch 176/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67260.0547 - mse: 65201.7692 - mae: 209.9176\n",
      "Epoch 177/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63530.1527 - mse: 61541.4778 - mae: 198.2848\n",
      "Epoch 178/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 58591.4976 - mse: 56456.9664 - mae: 195.0362\n",
      "Epoch 179/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 60636.8352 - mse: 58540.2617 - mae: 198.3591\n",
      "Epoch 180/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 69490.5513 - mse: 67508.3411 - mae: 212.1065\n",
      "Epoch 181/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 62388.3049 - mse: 60249.0864 - mae: 201.1637\n",
      "Epoch 182/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 55678.0259 - mse: 53602.8201 - mae: 186.0553\n",
      "Epoch 183/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 59235.3020 - mse: 57200.0434 - mae: 197.3079\n",
      "Epoch 184/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63393.5098 - mse: 61557.6530 - mae: 203.9022\n",
      "Epoch 185/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58521.5074 - mse: 56335.1771 - mae: 193.8518\n",
      "Epoch 186/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58086.0237 - mse: 56037.1641 - mae: 191.5589\n",
      "Epoch 187/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73879.6525 - mse: 71931.6317 - mae: 215.3068\n",
      "Epoch 188/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66060.5992 - mse: 64391.9701 - mae: 207.7289\n",
      "Epoch 189/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58284.9936 - mse: 56694.2539 - mae: 194.4037\n",
      "Epoch 190/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 69103.4693 - mse: 67377.5308 - mae: 212.5501\n",
      "Epoch 191/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62988.0508 - mse: 61475.8458 - mae: 204.9390\n",
      "Epoch 192/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 64773.9597 - mse: 63204.7114 - mae: 207.3275\n",
      "Epoch 193/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58998.6011 - mse: 57416.9842 - mae: 200.2147\n",
      "Epoch 194/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58860.2151 - mse: 57366.2075 - mae: 194.5348\n",
      "Epoch 195/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 54542.1855 - mse: 53019.8039 - mae: 183.3047\n",
      "Epoch 196/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59668.1296 - mse: 58230.1879 - mae: 199.7559\n",
      "Epoch 197/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 55612.7513 - mse: 54272.2754 - mae: 188.5443\n",
      "Epoch 198/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 56091.3650 - mse: 54721.7074 - mae: 193.3454\n",
      "Epoch 199/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58342.6732 - mse: 56983.4504 - mae: 194.9195\n",
      "Epoch 200/200\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 59678.2682 - mse: 58337.2875 - mae: 200.0816\n",
      "35/35 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8429075.139029678, tolerance: 2469.0218836503623\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.12581e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.34077e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>xgb</th>\n",
       "      <th>knn</th>\n",
       "      <th>lasso</th>\n",
       "      <th>lasso_tuned</th>\n",
       "      <th>ridge</th>\n",
       "      <th>ridge_tuned</th>\n",
       "      <th>rf</th>\n",
       "      <th>sv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.999826</td>\n",
       "      <td>-0.606860</td>\n",
       "      <td>0.317213</td>\n",
       "      <td>0.317046</td>\n",
       "      <td>0.269738</td>\n",
       "      <td>0.317213</td>\n",
       "      <td>0.915634</td>\n",
       "      <td>0.031815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.245519</td>\n",
       "      <td>-0.953065</td>\n",
       "      <td>0.192646</td>\n",
       "      <td>0.194796</td>\n",
       "      <td>0.186403</td>\n",
       "      <td>0.192772</td>\n",
       "      <td>0.371580</td>\n",
       "      <td>-0.007268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>1.914220</td>\n",
       "      <td>220.258390</td>\n",
       "      <td>144.855345</td>\n",
       "      <td>144.935605</td>\n",
       "      <td>145.747793</td>\n",
       "      <td>144.862887</td>\n",
       "      <td>49.725127</td>\n",
       "      <td>179.676587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>146.648066</td>\n",
       "      <td>249.385413</td>\n",
       "      <td>158.859743</td>\n",
       "      <td>158.918618</td>\n",
       "      <td>158.726149</td>\n",
       "      <td>158.852999</td>\n",
       "      <td>138.630163</td>\n",
       "      <td>184.983394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>2.786383</td>\n",
       "      <td>268.090858</td>\n",
       "      <td>174.757338</td>\n",
       "      <td>174.778740</td>\n",
       "      <td>180.730843</td>\n",
       "      <td>174.757378</td>\n",
       "      <td>61.429472</td>\n",
       "      <td>208.100002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>184.895446</td>\n",
       "      <td>297.481813</td>\n",
       "      <td>191.264376</td>\n",
       "      <td>191.009485</td>\n",
       "      <td>192.002423</td>\n",
       "      <td>191.249440</td>\n",
       "      <td>168.743556</td>\n",
       "      <td>213.636070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>1.939257</td>\n",
       "      <td>187.416782</td>\n",
       "      <td>192.887958</td>\n",
       "      <td>190.815668</td>\n",
       "      <td>224.083392</td>\n",
       "      <td>192.753026</td>\n",
       "      <td>54.606607</td>\n",
       "      <td>316.208972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>79.440023</td>\n",
       "      <td>113.664461</td>\n",
       "      <td>128.551552</td>\n",
       "      <td>129.254625</td>\n",
       "      <td>114.745386</td>\n",
       "      <td>128.589968</td>\n",
       "      <td>84.479601</td>\n",
       "      <td>190.589457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  xgb         knn       lasso  lasso_tuned       ridge  \\\n",
       "Train: Rsquare    0.999826   -0.606860    0.317213     0.317046    0.269738   \n",
       "Test: Rsquare     0.245519   -0.953065    0.192646     0.194796    0.186403   \n",
       "Train: MAE        1.914220  220.258390  144.855345   144.935605  145.747793   \n",
       "Test: MAE       146.648066  249.385413  158.859743   158.918618  158.726149   \n",
       "Train: RMSE       2.786383  268.090858  174.757338   174.778740  180.730843   \n",
       "Test: RMSE      184.895446  297.481813  191.264376   191.009485  192.002423   \n",
       "Train: MAPE       1.939257  187.416782  192.887958   190.815668  224.083392   \n",
       "Test: MAPE       79.440023  113.664461  128.551552   129.254625  114.745386   \n",
       "\n",
       "Model           ridge_tuned          rf          sv  \n",
       "Train: Rsquare     0.317213    0.915634    0.031815  \n",
       "Test: Rsquare      0.192772    0.371580   -0.007268  \n",
       "Train: MAE       144.862887   49.725127  179.676587  \n",
       "Test: MAE        158.852999  138.630163  184.983394  \n",
       "Train: RMSE      174.757378   61.429472  208.100002  \n",
       "Test: RMSE       191.249440  168.743556  213.636070  \n",
       "Train: MAPE      192.753026   54.606607  316.208972  \n",
       "Test: MAPE       128.589968   84.479601  190.589457  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model performance with twitter BERT and news sentiments\")\n",
    "model_performance([xgbr, nn_tuned, lasso_reg, lasso_tuned, ridge_reg, ridge_tune2, rf, sv], [\"xgb\", 'knn', 'lasso', 'lasso_tuned', 'ridge', 'ridge_tuned', 'rf', 'sv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important_gain = xgbr.get_booster().get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_important_gain = xgbr.get_booster().get_score(importance_type='gain')\n",
    "keys = list(feature_important_gain.keys())\n",
    "values = list(feature_important_gain.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=['score']).sort_values(by='score', ascending=False)\n",
    "data.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
