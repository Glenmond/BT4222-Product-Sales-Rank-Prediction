{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Models with twitter sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models analysis for top 10 brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import ast\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None) # display all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/notebooks/Data')\n",
    "df_standby = pd.read_csv('aggregated_reviews_meta.csv')\n",
    "df_standby.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df = df_standby.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape is:  (7097, 17)\n",
      "Top 10 brand df's shape is:  (736, 17)\n"
     ]
    }
   ],
   "source": [
    "top10_brand_df = df[df['brand'].isin(['Sony','PWR+','Apple','Boss Audio','Polk Audio','Sangean', 'Tripp Lite', \n",
    "                    'Yamaha Audio','Belkin','Garmin'])]\n",
    "print(\"df shape is: \", df.shape)\n",
    "print(\"Top 10 brand df's shape is: \", top10_brand_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter_BERT_Sentiment data\n",
    "os.chdir('/notebooks/Data')\n",
    "df_BERT = pd.read_csv('Brand_BERT_sentiment.csv')\n",
    "#df_standby.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_bert = df_BERT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand\n",
      "Apple          0.606678\n",
      "Belkin         0.622052\n",
      "BossAudio      0.718182\n",
      "Garmin         0.595745\n",
      "PolkAudio      0.702373\n",
      "Pwr+           0.458546\n",
      "Sangean        0.540475\n",
      "Sony           0.609016\n",
      "TrippLite      0.412678\n",
      "YamahaAudio    0.695243\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Aggregate sentiment column of df_bert by brands create a new column for meta data\n",
    "bert_sentiment = pd.pivot_table(df_bert, index='Brand', values=\"Sentiment\", aggfunc=np.mean)\n",
    "print(bert_sentiment.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (top10_brand_df['brand'] == 'Apple'),\n",
    "    (top10_brand_df['brand'] == 'Belkin'),\n",
    "    (top10_brand_df['brand'] == 'Boss Audio'),\n",
    "    (top10_brand_df['brand'] == 'Garmin'),\n",
    "    (top10_brand_df['brand'] == 'Polk Audio'),\n",
    "    (top10_brand_df['brand'] == 'PWR+'),\n",
    "    (top10_brand_df['brand'] == 'Sangean'),\n",
    "    (top10_brand_df['brand'] == 'Sony'),\n",
    "    (top10_brand_df['brand'] == 'Tripp Lite'),\n",
    "    (top10_brand_df['brand'] == 'Yamaha Audio')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [0.606678, 0.622052, 0.718182, 0.595745, 0.702373, 0.458546, 0.540475, 0.609016, 0.412678, 0.695243]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "top10_brand_df['brand_sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load News_Sentiment data\n",
    "os.chdir('/notebooks/Data')\n",
    "df_news = pd.read_csv('Brand_News_Sentiment.csv')\n",
    "df_news.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (top10_brand_df['brand'] == 'Apple'),\n",
    "    (top10_brand_df['brand'] == 'Belkin'),\n",
    "    (top10_brand_df['brand'] == 'Boss Audio'),\n",
    "    (top10_brand_df['brand'] == 'Garmin'),\n",
    "    (top10_brand_df['brand'] == 'Polk Audio'),\n",
    "    (top10_brand_df['brand'] == 'PWR+'),\n",
    "    (top10_brand_df['brand'] == 'Sangean'),\n",
    "    (top10_brand_df['brand'] == 'Sony'),\n",
    "    (top10_brand_df['brand'] == 'Tripp Lite'),\n",
    "    (top10_brand_df['brand'] == 'Yamaha Audio')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [0.130569, 0.206817, 0.307327, 0.219252, 0.166181, 0.569831, 0.319515, 0.082589, 0.076493, 0.139865]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "top10_brand_df['brand_news_sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.12.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.5.4)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# do encoding for categorical variable\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "new_df = top10_brand_df\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "new_df['brand_encode'] = encoder.fit_transform(new_df['brand'], new_df['rankElectronics'])\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "new_df['main_cat_encode'] = encoder.fit_transform(new_df['main_cat'], new_df['rankElectronics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "We will be using four models for prediction of sales rank. The following are as shown:<br>\n",
    "1. XGBoost Regressor \n",
    "2. Neural Network\n",
    "3. Ridge / Lasso Regression\n",
    "4. Random Forest\n",
    "5. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df['normRankElectronics'] = np.array(sales_df.rankElectronics.rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split\n",
    "X = sales_df.drop(['rankElectronics', 'asin', 'title', 'brand', 'main_cat', 'normRankElectronics', 'brand_news_sentiment'], axis=1)\n",
    "y = sales_df.normRankElectronics\n",
    "\n",
    "X['vote'] = X['vote'].fillna(0)# to replace NaN with 0\n",
    "\n",
    "# Standard Scaler or Min Max Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4222)\n",
    "\n",
    "X_train[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_sentiment']] = scaler.fit_transform(X_train[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_sentiment']])\n",
    "X_test[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_sentiment']] = scaler.transform(X_test[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "#Defining MAPE function\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "#models is a list of the fitted models, model_names is list of model names\n",
    "\n",
    "def model_performance(models, model_names):\n",
    "    #create empty df with col names\n",
    "    df = pd.DataFrame(columns = ['Model', 'Train: Rsquare', 'Test: Rsquare', 'Train: MAE', 'Test: MAE', 'Train: RMSE', 'Test: RMSE', 'Train: MAPE', 'Test: MAPE'])\n",
    "    \n",
    "    for n, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        #prepare values for model\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        name = model_names[n] \n",
    "        rsquare_train = r2_score(y_train, y_train_pred)\n",
    "        mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "        rmse_train = math.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        \n",
    "        rsquare_test = r2_score(y_test, y_pred)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred)\n",
    "        rmse_test = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        mape_train = MAPE(y_train, y_train_pred)\n",
    "        mape_test = MAPE(y_test, y_pred)\n",
    "\n",
    "        #append row to df\n",
    "        df = df.append({'Model' \n",
    "                        : name, 'Train: Rsquare' : rsquare_train, 'Test: Rsquare' : rsquare_test, 'Train: MAE': mae_train, 'Test: MAE' : mae_test, 'Train: RMSE': rmse_train,\n",
    "                         'Test: RMSE' : rmse_test, 'Train: MAPE': mape_train, 'Test: MAPE': mape_test}, \n",
    "                    ignore_index = True)\n",
    "            \n",
    "    return df.set_index('Model').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared on train dataset=0.19469439175619685\n",
      "R-Squared on test dataset=0.5218370630022122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>dtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.445184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.194694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>128.453769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>157.805276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>157.531492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>191.021574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>174.806516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>114.039335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  dtr\n",
       "Train: Rsquare    0.445184\n",
       "Test: Rsquare     0.194694\n",
       "Train: MAE      128.453769\n",
       "Test: MAE       157.805276\n",
       "Train: RMSE     157.531492\n",
       "Test: RMSE      191.021574\n",
       "Train: MAPE     174.806516\n",
       "Test: MAPE      114.039335"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Without additional predictors\n",
    "dtr = DecisionTreeRegressor(max_depth=4,\n",
    "                           min_samples_split=5,\n",
    "                           max_leaf_nodes=10)\n",
    "\n",
    "dtr.fit(X_train,y_train)\n",
    "print(\"R-Squared on train dataset={}\".format(dtr.score(X_test,y_test)))\n",
    "\n",
    "dtr.fit(X_test,y_test)   \n",
    "print(\"R-Squared on test dataset={}\".format(dtr.score(X_test,y_test)))\n",
    "\n",
    "model_performance([dtr], [\"dtr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>dtr</th>\n",
       "      <th>dtr_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.445184</td>\n",
       "      <td>0.504141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.194694</td>\n",
       "      <td>0.205333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>128.453769</td>\n",
       "      <td>119.668736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>157.805276</td>\n",
       "      <td>153.684775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>157.531492</td>\n",
       "      <td>148.926563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>191.021574</td>\n",
       "      <td>189.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>174.806516</td>\n",
       "      <td>145.864716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>114.039335</td>\n",
       "      <td>104.405621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  dtr   dtr_tuned\n",
       "Train: Rsquare    0.445184    0.504141\n",
       "Test: Rsquare     0.194694    0.205333\n",
       "Train: MAE      128.453769  119.668736\n",
       "Test: MAE       157.805276  153.684775\n",
       "Train: RMSE     157.531492  148.926563\n",
       "Test: RMSE      191.021574  189.755600\n",
       "Train: MAPE     174.806516  145.864716\n",
       "Test: MAPE      114.039335  104.405621"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"criterion\": [\"mse\", \"mae\"],\n",
    "              \"min_samples_split\": [10, 20, 40],\n",
    "              \"max_depth\": [2, 6, 8],\n",
    "              \"min_samples_leaf\": [20, 40, 100],\n",
    "              \"max_leaf_nodes\": [5, 20, 100],\n",
    "              }\n",
    "\n",
    "## Comment in order to publish in kaggle.\n",
    "dtr_tuned = GridSearchCV(dtr, param_grid, cv=5)\n",
    "model_performance([dtr, dtr_tuned], [\"dtr\", \"dtr_tuned\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 18,689\n",
      "Trainable params: 18,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(128,\n",
    "                activation = 'relu',\n",
    "                input_shape = (15, ),\n",
    "                activity_regularizer = regularizers.l2(1e-5)))\n",
    "nn.add(Dropout(0.50))\n",
    "nn.add(Dense(128,\n",
    "                activation = 'relu', \n",
    "                activity_regularizer = regularizers.l2(1e-5)))\n",
    "nn.add(Dropout(0.50))\n",
    "nn.add(Dense(1, activation = 'relu'))\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "56/56 [==============================] - 1s 7ms/step - loss: 65002448.1754 - mse: 64563192.1404 - mae: 2708.5075 - val_loss: 561691.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 2/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 622786.4759 - mse: 183529.3317 - mae: 371.6057 - val_loss: 555091.0625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 3/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1940529.3695 - mse: 1511308.9093 - mae: 425.9632 - val_loss: 537286.0625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 4/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 586693.1727 - mse: 190473.8322 - mae: 343.5686 - val_loss: 521775.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 5/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 706611.4737 - mse: 306813.9419 - mae: 382.9889 - val_loss: 515903.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 6/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 806117.0450 - mse: 424158.0107 - mae: 393.6003 - val_loss: 522845.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 7/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 600241.8048 - mse: 206331.4534 - mae: 346.6517 - val_loss: 525623.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 8/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 553987.6793 - mse: 162625.0345 - mae: 342.8461 - val_loss: 522653.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 9/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 2905215.4945 - mse: 2508601.9013 - mae: 465.3281 - val_loss: 519895.9062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 10/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 566544.3169 - mse: 186225.5543 - mae: 372.3028 - val_loss: 512760.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 11/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 540855.8799 - mse: 173696.1828 - mae: 356.2174 - val_loss: 504807.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 12/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 554123.3618 - mse: 171237.0085 - mae: 356.4221 - val_loss: 495804.2812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 13/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 520337.1541 - mse: 167151.9112 - mae: 351.9576 - val_loss: 454355.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 14/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 530734.8893 - mse: 212320.3128 - mae: 367.9254 - val_loss: 455477.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 15/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 502161.0033 - mse: 176903.1192 - mae: 361.9013 - val_loss: 448952.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 16/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 486603.7100 - mse: 176449.9186 - mae: 361.9695 - val_loss: 440343.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 17/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 462057.3925 - mse: 172678.0636 - mae: 348.8875 - val_loss: 436876.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 18/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 526677.2971 - mse: 218948.1143 - mae: 372.2206 - val_loss: 432724.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 19/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 505265.9682 - mse: 210112.4866 - mae: 376.4084 - val_loss: 433223.9062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 20/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 471709.0466 - mse: 193509.0872 - mae: 356.1020 - val_loss: 430384.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 21/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 448366.2577 - mse: 165571.6025 - mae: 345.5557 - val_loss: 423996.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 22/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 452091.6464 - mse: 178428.7714 - mae: 366.0453 - val_loss: 417259.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 23/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 452364.4479 - mse: 176742.1055 - mae: 358.5448 - val_loss: 415356.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 24/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 699918.1875 - mse: 414254.5614 - mae: 400.8553 - val_loss: 440369.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 25/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 646504.5855 - mse: 340367.7758 - mae: 391.7537 - val_loss: 443168.9062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 26/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 478817.1201 - mse: 187487.7204 - mae: 376.3015 - val_loss: 431816.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 27/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 459371.8432 - mse: 184016.9827 - mae: 357.6278 - val_loss: 416779.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 28/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 431411.3081 - mse: 155998.9827 - mae: 334.6042 - val_loss: 407981.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 29/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 434974.2884 - mse: 171934.0732 - mae: 355.2698 - val_loss: 400600.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 30/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 427345.2708 - mse: 183388.7634 - mae: 369.0607 - val_loss: 393723.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 31/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 407497.3646 - mse: 170742.7651 - mae: 354.6473 - val_loss: 387931.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 32/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 396142.7039 - mse: 166535.5310 - mae: 344.5132 - val_loss: 384508.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 33/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 400377.1491 - mse: 172058.3328 - mae: 355.9906 - val_loss: 379422.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 34/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 418521.0691 - mse: 194587.9970 - mae: 370.6882 - val_loss: 375956.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 35/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 398543.3536 - mse: 178806.8695 - mae: 361.4821 - val_loss: 376826.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 36/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 388035.6760 - mse: 175104.5474 - mae: 357.4066 - val_loss: 373622.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 37/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 405789.9742 - mse: 193619.4287 - mae: 358.2791 - val_loss: 365739.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 38/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 767856.6941 - mse: 545185.3520 - mae: 396.9488 - val_loss: 370486.2188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 39/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 390521.0866 - mse: 178102.1099 - mae: 361.4687 - val_loss: 365127.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 40/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 372464.1513 - mse: 167436.3440 - mae: 353.7029 - val_loss: 359182.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 41/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 371516.7341 - mse: 174006.3734 - mae: 354.4272 - val_loss: 353855.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 42/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 360811.8125 - mse: 180008.5225 - mae: 363.1480 - val_loss: 351085.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 43/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 365054.3152 - mse: 175606.1414 - mae: 358.0111 - val_loss: 346514.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 44/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 361210.6239 - mse: 182914.9391 - mae: 370.0818 - val_loss: 343329.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 45/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 359369.0033 - mse: 176460.9342 - mae: 358.5571 - val_loss: 339948.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 46/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 358413.5175 - mse: 178984.1713 - mae: 363.7724 - val_loss: 336169.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 47/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 378368.0055 - mse: 202877.7799 - mae: 373.1472 - val_loss: 335660.8750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 48/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 357411.1787 - mse: 184538.4995 - mae: 372.7911 - val_loss: 331572.0625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 49/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 339122.2237 - mse: 174512.5156 - mae: 356.4596 - val_loss: 327862.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 50/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 331480.3150 - mse: 168829.9102 - mae: 343.1003 - val_loss: 328896.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 51/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 337458.0954 - mse: 169849.2684 - mae: 351.7165 - val_loss: 325227.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 52/300\n",
      "56/56 [==============================] - ETA: 0s - loss: 345441.6250 - mse: 189912.8289 - mae: 375.191 - 0s 3ms/step - loss: 340677.5839 - mse: 184251.5003 - mae: 368.7838 - val_loss: 321426.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 53/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 344001.4457 - mse: 186223.7536 - mae: 361.1550 - val_loss: 317931.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 54/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 366574.9380 - mse: 217327.6180 - mae: 362.5842 - val_loss: 313415.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 55/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 312719.9655 - mse: 168524.5540 - mae: 346.5158 - val_loss: 310156.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 56/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 298691.4781 - mse: 161111.3311 - mae: 347.7916 - val_loss: 307000.6562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 57/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 300352.3997 - mse: 165696.6883 - mae: 346.2135 - val_loss: 304470.5312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 58/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 315150.4375 - mse: 177053.8054 - mae: 362.3096 - val_loss: 301431.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 59/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 307711.7242 - mse: 175626.8947 - mae: 360.0892 - val_loss: 298375.3438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 60/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 289203.9984 - mse: 163660.9862 - mae: 344.5474 - val_loss: 295533.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 61/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 279923.0356 - mse: 156170.8035 - mae: 337.7555 - val_loss: 292732.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 62/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 276398.3191 - mse: 162005.1922 - mae: 337.8113 - val_loss: 289958.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 63/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 288379.2281 - mse: 170786.9734 - mae: 356.6485 - val_loss: 287200.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 64/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 297988.5228 - mse: 183605.2945 - mae: 359.5928 - val_loss: 285705.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 65/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 284944.1497 - mse: 175239.6647 - mae: 357.5361 - val_loss: 283442.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 66/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 286772.5883 - mse: 176914.7881 - mae: 361.9870 - val_loss: 280582.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 67/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 281846.3777 - mse: 174969.6469 - mae: 360.4688 - val_loss: 277930.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 68/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 277745.4981 - mse: 175992.7714 - mae: 361.0105 - val_loss: 275245.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 69/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 274215.7626 - mse: 177828.6519 - mae: 364.6039 - val_loss: 272990.5312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 70/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 270280.4178 - mse: 173575.6234 - mae: 356.3886 - val_loss: 270470.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 71/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 262073.3147 - mse: 171691.1508 - mae: 357.3779 - val_loss: 268147.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 72/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 274608.4589 - mse: 184944.2039 - mae: 370.5225 - val_loss: 265784.8750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 73/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 246682.1828 - mse: 159312.0669 - mae: 332.2331 - val_loss: 264465.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 74/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 268957.1486 - mse: 176829.6735 - mae: 361.5891 - val_loss: 261941.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 75/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 257875.2083 - mse: 170405.8199 - mae: 349.0713 - val_loss: 259817.4219 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 76/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 270902.3172 - mse: 184473.2903 - mae: 375.1235 - val_loss: 257417.8750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 77/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 260329.1176 - mse: 175128.0074 - mae: 356.9227 - val_loss: 255053.5469 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 78/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 234056.1519 - mse: 156669.5606 - mae: 331.5720 - val_loss: 252737.6094 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 79/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 248442.3443 - mse: 171011.7048 - mae: 351.1483 - val_loss: 250414.7031 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 80/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 242966.3750 - mse: 172750.5907 - mae: 359.9687 - val_loss: 248299.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 81/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 238824.4723 - mse: 165421.4056 - mae: 349.4277 - val_loss: 246106.9531 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 82/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 254055.5292 - mse: 186286.8215 - mae: 356.3157 - val_loss: 244307.7656 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 83/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 304714.7229 - mse: 238060.9460 - mae: 385.0827 - val_loss: 244000.0156 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 84/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 241005.7571 - mse: 174908.0535 - mae: 357.5159 - val_loss: 241863.6719 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 85/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 238197.5691 - mse: 175468.9178 - mae: 365.6572 - val_loss: 239840.5938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 86/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 223333.4734 - mse: 164114.1075 - mae: 345.5712 - val_loss: 237807.1094 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 87/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 230114.0110 - mse: 170785.0518 - mae: 352.3532 - val_loss: 236395.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 88/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 221909.6880 - mse: 164799.2632 - mae: 347.2828 - val_loss: 234825.8594 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 89/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 215944.3783 - mse: 161215.9611 - mae: 346.2009 - val_loss: 233184.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 90/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 228148.3220 - mse: 174248.3887 - mae: 355.5089 - val_loss: 232497.4219 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 91/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 226478.0696 - mse: 174003.3405 - mae: 362.2132 - val_loss: 230484.0781 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 92/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 219567.8026 - mse: 168850.8133 - mae: 346.2219 - val_loss: 228764.5156 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 93/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 229623.9126 - mse: 179776.9638 - mae: 368.0344 - val_loss: 227065.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 94/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 214781.2588 - mse: 167805.8276 - mae: 348.2983 - val_loss: 225663.6562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 95/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 208379.8461 - mse: 164056.0319 - mae: 345.6108 - val_loss: 224140.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 96/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 210219.2722 - mse: 165201.6691 - mae: 346.3196 - val_loss: 214916.4375 - val_mse: 175944.1250 - val_mae: 365.3577\n",
      "Epoch 97/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 198334.3931 - mse: 157233.4457 - mae: 336.0510 - val_loss: 210313.1875 - val_mse: 172766.0938 - val_mae: 362.3893\n",
      "Epoch 98/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 174273.2086 - mse: 134479.2802 - mae: 305.1658 - val_loss: 166752.7812 - val_mse: 130421.5703 - val_mae: 309.0075\n",
      "Epoch 99/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 190526.6985 - mse: 152902.8580 - mae: 322.4319 - val_loss: 118326.0938 - val_mse: 81843.4062 - val_mae: 234.4987\n",
      "Epoch 100/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 199534.3226 - mse: 160617.1387 - mae: 337.4805 - val_loss: 157805.0625 - val_mse: 122748.6016 - val_mae: 295.8723\n",
      "Epoch 101/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 168105.5204 - mse: 131073.6802 - mae: 301.5063 - val_loss: 164253.4375 - val_mse: 130574.4609 - val_mae: 308.8849\n",
      "Epoch 102/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 191418.8997 - mse: 156291.2536 - mae: 323.2361 - val_loss: 138890.7344 - val_mse: 105591.6250 - val_mae: 271.9633\n",
      "Epoch 103/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 168331.1516 - mse: 132263.7437 - mae: 304.9914 - val_loss: 149271.2344 - val_mse: 118003.0469 - val_mae: 288.8143\n",
      "Epoch 104/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 181285.0702 - mse: 147801.8621 - mae: 322.3824 - val_loss: 150164.4688 - val_mse: 120371.5938 - val_mae: 292.0515\n",
      "Epoch 105/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 165427.4640 - mse: 133348.9753 - mae: 304.8642 - val_loss: 148213.6562 - val_mse: 120110.1250 - val_mae: 291.7210\n",
      "Epoch 106/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 161653.9661 - mse: 132396.2338 - mae: 298.3443 - val_loss: 124827.5312 - val_mse: 98007.6562 - val_mae: 260.1079\n",
      "Epoch 107/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 163345.6900 - mse: 135512.5707 - mae: 303.8727 - val_loss: 114211.3047 - val_mse: 88326.1641 - val_mae: 244.2576\n",
      "Epoch 108/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 162451.1872 - mse: 134870.8806 - mae: 298.7440 - val_loss: 136570.4688 - val_mse: 112244.7422 - val_mae: 280.3869\n",
      "Epoch 109/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 162078.5192 - mse: 138276.3114 - mae: 310.1623 - val_loss: 123175.3047 - val_mse: 99203.3984 - val_mae: 261.8504\n",
      "Epoch 110/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 170602.7812 - mse: 144422.3199 - mae: 317.7856 - val_loss: 129050.0938 - val_mse: 105956.1172 - val_mae: 271.7210\n",
      "Epoch 111/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 151242.5593 - mse: 127481.7996 - mae: 287.1824 - val_loss: 121604.4453 - val_mse: 99962.8828 - val_mae: 263.0223\n",
      "Epoch 112/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 167644.4189 - mse: 144527.0134 - mae: 322.9318 - val_loss: 121890.8125 - val_mse: 101520.6328 - val_mae: 264.7214\n",
      "Epoch 113/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 144325.7371 - mse: 122826.0553 - mae: 281.1528 - val_loss: 124429.8359 - val_mse: 105579.8125 - val_mae: 275.2279\n",
      "Epoch 114/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 148989.8956 - mse: 128941.2689 - mae: 298.9126 - val_loss: 140407.3125 - val_mse: 122616.6016 - val_mae: 296.5057\n",
      "Epoch 115/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 137748.9424 - mse: 118854.6099 - mae: 276.9090 - val_loss: 110473.6953 - val_mse: 93406.1016 - val_mae: 252.8043\n",
      "Epoch 116/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 134112.2625 - mse: 116296.7693 - mae: 274.4990 - val_loss: 109112.9922 - val_mse: 92683.6328 - val_mae: 250.9160\n",
      "Epoch 117/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 156684.0019 - mse: 138989.8244 - mae: 311.6392 - val_loss: 112460.4141 - val_mse: 97041.3281 - val_mae: 257.6695\n",
      "Epoch 118/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 130899.8111 - mse: 115679.6639 - mae: 276.4375 - val_loss: 89044.7578 - val_mse: 73674.8594 - val_mae: 222.7274\n",
      "Epoch 119/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 135466.0169 - mse: 119215.4234 - mae: 274.7030 - val_loss: 102305.3906 - val_mse: 87562.8047 - val_mae: 246.9041\n",
      "Epoch 120/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 154418.7303 - mse: 138559.8512 - mae: 300.8994 - val_loss: 124592.8438 - val_mse: 110964.0625 - val_mae: 279.3268\n",
      "Epoch 121/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 135037.0868 - mse: 121059.1664 - mae: 279.3385 - val_loss: 101717.9531 - val_mse: 88903.2734 - val_mae: 246.5705\n",
      "Epoch 122/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 146059.0340 - mse: 133101.8540 - mae: 304.2781 - val_loss: 112259.2969 - val_mse: 100278.8906 - val_mae: 262.3778\n",
      "Epoch 123/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 143657.6227 - mse: 131176.3150 - mae: 299.7447 - val_loss: 111133.3906 - val_mse: 100412.2578 - val_mae: 263.4071\n",
      "Epoch 124/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 122770.1637 - mse: 111419.8953 - mae: 269.3601 - val_loss: 96145.9219 - val_mse: 85670.5156 - val_mae: 240.5219\n",
      "Epoch 125/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 137096.3357 - mse: 126109.3845 - mae: 289.8252 - val_loss: 96155.4141 - val_mse: 85929.1719 - val_mae: 241.5878\n",
      "Epoch 126/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 126117.8309 - mse: 115652.3743 - mae: 275.1511 - val_loss: 92221.7812 - val_mse: 82198.2344 - val_mae: 238.4777\n",
      "Epoch 127/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 127107.5404 - mse: 116881.3544 - mae: 275.8072 - val_loss: 84739.8438 - val_mse: 75607.3047 - val_mae: 228.8643\n",
      "Epoch 128/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 132128.2004 - mse: 122438.0964 - mae: 270.3194 - val_loss: 88179.6953 - val_mse: 78804.9375 - val_mae: 233.4532\n",
      "Epoch 129/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 120946.2800 - mse: 111799.8923 - mae: 276.7854 - val_loss: 94811.7578 - val_mse: 85874.1641 - val_mae: 244.0896\n",
      "Epoch 130/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 120652.1008 - mse: 111511.4067 - mae: 270.9187 - val_loss: 72639.8906 - val_mse: 63830.9531 - val_mae: 209.0807\n",
      "Epoch 131/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 117962.2855 - mse: 109139.8014 - mae: 269.7888 - val_loss: 84732.5312 - val_mse: 76899.0469 - val_mae: 230.8890\n",
      "Epoch 132/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 117638.8165 - mse: 109508.6993 - mae: 264.6396 - val_loss: 74997.2031 - val_mse: 66987.7578 - val_mae: 213.2934\n",
      "Epoch 133/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 119443.1379 - mse: 111918.0699 - mae: 268.7542 - val_loss: 78795.4609 - val_mse: 71852.0156 - val_mae: 220.7033\n",
      "Epoch 134/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 115569.7345 - mse: 108700.1528 - mae: 262.5692 - val_loss: 99197.7266 - val_mse: 92599.0703 - val_mae: 253.0402\n",
      "Epoch 135/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 111487.5876 - mse: 105002.9597 - mae: 263.6908 - val_loss: 86987.5391 - val_mse: 80534.8125 - val_mae: 233.8022\n",
      "Epoch 136/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 126839.3633 - mse: 119596.9674 - mae: 274.5761 - val_loss: 86218.5547 - val_mse: 79720.1406 - val_mae: 234.4437\n",
      "Epoch 137/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 104831.8390 - mse: 98492.9963 - mae: 251.0386 - val_loss: 77293.3516 - val_mse: 71281.1406 - val_mae: 221.5298\n",
      "Epoch 138/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 110300.8850 - mse: 104179.2349 - mae: 264.0313 - val_loss: 79999.1875 - val_mse: 74342.1250 - val_mae: 226.9320\n",
      "Epoch 139/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 105609.3120 - mse: 100039.8306 - mae: 251.4917 - val_loss: 99625.5547 - val_mse: 94562.0000 - val_mae: 256.6145\n",
      "Epoch 140/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 102114.3059 - mse: 97228.7399 - mae: 247.2125 - val_loss: 78582.9922 - val_mse: 73308.6719 - val_mae: 223.9329\n",
      "Epoch 141/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 97802.4940 - mse: 92422.9445 - mae: 246.8247 - val_loss: 69067.5625 - val_mse: 64014.2539 - val_mae: 208.6493\n",
      "Epoch 142/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 99410.9679 - mse: 94362.3898 - mae: 252.3107 - val_loss: 81190.1562 - val_mse: 76714.8047 - val_mae: 228.9878\n",
      "Epoch 143/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 85839.8285 - mse: 81130.3949 - mae: 230.0973 - val_loss: 94253.3281 - val_mse: 88439.1094 - val_mae: 247.8717\n",
      "Epoch 144/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 81197.8029 - mse: 75419.5156 - mae: 220.1037 - val_loss: 75750.0625 - val_mse: 70848.3594 - val_mae: 218.9449\n",
      "Epoch 145/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 95179.0299 - mse: 90280.9539 - mae: 238.1711 - val_loss: 93374.0391 - val_mse: 89352.8906 - val_mae: 248.7315\n",
      "Epoch 146/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71820.2379 - mse: 67691.1393 - mae: 207.4898 - val_loss: 79803.3359 - val_mse: 75906.5781 - val_mae: 228.1633\n",
      "Epoch 147/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 74059.5503 - mse: 70132.0728 - mae: 212.3878 - val_loss: 91072.9375 - val_mse: 87768.1172 - val_mae: 243.1694\n",
      "Epoch 148/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 83023.4951 - mse: 79428.4267 - mae: 226.1366 - val_loss: 84638.5547 - val_mse: 81526.1016 - val_mae: 235.6947\n",
      "Epoch 149/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 89148.8472 - mse: 85974.8129 - mae: 229.7428 - val_loss: 89390.1562 - val_mse: 86477.9453 - val_mae: 243.7713\n",
      "Epoch 150/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69685.9776 - mse: 66669.6740 - mae: 207.3508 - val_loss: 84488.3594 - val_mse: 81313.6484 - val_mae: 236.8214\n",
      "Epoch 151/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 76684.7625 - mse: 73302.4349 - mae: 217.9929 - val_loss: 77823.7188 - val_mse: 74328.4531 - val_mae: 226.4352\n",
      "Epoch 152/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 78635.8965 - mse: 75272.1427 - mae: 223.0116 - val_loss: 93339.1328 - val_mse: 90387.9844 - val_mae: 250.8789\n",
      "Epoch 153/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 85913.6797 - mse: 82535.1486 - mae: 232.9311 - val_loss: 97129.9297 - val_mse: 94193.1719 - val_mae: 256.0321\n",
      "Epoch 154/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70416.5499 - mse: 67163.5309 - mae: 210.9901 - val_loss: 79555.6875 - val_mse: 76300.5703 - val_mae: 228.8960\n",
      "Epoch 155/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71578.8916 - mse: 68198.1854 - mae: 212.2432 - val_loss: 89663.3984 - val_mse: 86671.1094 - val_mae: 244.8394\n",
      "Epoch 156/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 76942.8395 - mse: 73904.7752 - mae: 223.8997 - val_loss: 75187.7656 - val_mse: 72007.6953 - val_mae: 221.6319\n",
      "Epoch 157/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75521.7029 - mse: 72499.5383 - mae: 209.9375 - val_loss: 88666.0781 - val_mse: 86003.8281 - val_mae: 243.8365\n",
      "Epoch 158/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 68738.3839 - mse: 66018.0758 - mae: 201.7549 - val_loss: 81162.1875 - val_mse: 78524.6875 - val_mae: 232.1620\n",
      "Epoch 159/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 75524.0503 - mse: 72660.0367 - mae: 218.9851 - val_loss: 77199.5859 - val_mse: 74383.9766 - val_mae: 226.7667\n",
      "Epoch 160/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77374.5882 - mse: 74469.0568 - mae: 222.3210 - val_loss: 88083.5859 - val_mse: 85423.8672 - val_mae: 243.9997\n",
      "Epoch 161/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 80663.0928 - mse: 77830.0302 - mae: 228.1799 - val_loss: 63001.0312 - val_mse: 60037.5000 - val_mae: 202.5711\n",
      "Epoch 162/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 81522.7817 - mse: 78728.1168 - mae: 223.3905 - val_loss: 96352.4062 - val_mse: 93909.8125 - val_mae: 256.4269\n",
      "Epoch 163/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75112.1597 - mse: 72366.0196 - mae: 217.0564 - val_loss: 83094.8203 - val_mse: 79974.1562 - val_mae: 235.7757\n",
      "Epoch 164/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71671.1326 - mse: 68541.6631 - mae: 205.6412 - val_loss: 83108.3594 - val_mse: 80085.9609 - val_mae: 235.2499\n",
      "Epoch 165/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72492.6791 - mse: 69430.2329 - mae: 211.0702 - val_loss: 82681.7812 - val_mse: 79745.9922 - val_mae: 234.5491\n",
      "Epoch 166/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 67043.7164 - mse: 63963.3562 - mae: 207.5360 - val_loss: 83296.5469 - val_mse: 80591.6562 - val_mae: 236.4214\n",
      "Epoch 167/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70567.8585 - mse: 67642.2425 - mae: 203.8061 - val_loss: 86587.0391 - val_mse: 84010.6250 - val_mae: 241.3765\n",
      "Epoch 168/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72374.4142 - mse: 69711.9528 - mae: 218.5300 - val_loss: 75961.4688 - val_mse: 73596.5234 - val_mae: 224.9885\n",
      "Epoch 169/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61222.0235 - mse: 58691.4090 - mae: 195.2060 - val_loss: 94150.8281 - val_mse: 91964.5547 - val_mae: 253.3047\n",
      "Epoch 170/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66720.7291 - mse: 64415.2536 - mae: 198.2592 - val_loss: 84560.4844 - val_mse: 82121.7500 - val_mae: 238.0053\n",
      "Epoch 171/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75299.8636 - mse: 72693.7814 - mae: 218.7992 - val_loss: 92435.8594 - val_mse: 90227.3672 - val_mae: 250.2608\n",
      "Epoch 172/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60330.7909 - mse: 58044.5666 - mae: 194.1777 - val_loss: 61447.8711 - val_mse: 58860.6562 - val_mae: 200.1657\n",
      "Epoch 173/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 80265.5898 - mse: 77854.6727 - mae: 233.7886 - val_loss: 87351.6094 - val_mse: 85235.8281 - val_mae: 242.6941\n",
      "Epoch 174/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 74795.5581 - mse: 72732.2469 - mae: 214.1721 - val_loss: 88083.9297 - val_mse: 86149.2422 - val_mae: 244.5783\n",
      "Epoch 175/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75059.7581 - mse: 72996.5461 - mae: 221.5246 - val_loss: 89469.1094 - val_mse: 87449.3750 - val_mae: 246.6154\n",
      "Epoch 176/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 68113.5526 - mse: 65896.3463 - mae: 205.5605 - val_loss: 78514.3984 - val_mse: 76416.5703 - val_mae: 229.7606\n",
      "Epoch 177/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69137.7409 - mse: 67077.2158 - mae: 207.6885 - val_loss: 94291.4766 - val_mse: 92415.6328 - val_mae: 253.7173\n",
      "Epoch 178/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 74498.6242 - mse: 72440.7632 - mae: 220.9352 - val_loss: 91483.5547 - val_mse: 89523.5938 - val_mae: 248.9966\n",
      "Epoch 179/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73942.7225 - mse: 71812.5200 - mae: 221.2112 - val_loss: 96943.3594 - val_mse: 95098.2578 - val_mae: 256.6794\n",
      "Epoch 180/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66450.3440 - mse: 64562.0742 - mae: 204.2422 - val_loss: 84725.8594 - val_mse: 82851.8984 - val_mae: 237.6291\n",
      "Epoch 181/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69705.4518 - mse: 67784.1476 - mae: 213.0726 - val_loss: 76168.9844 - val_mse: 73955.9844 - val_mae: 224.4507\n",
      "Epoch 182/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 92947.5507 - mse: 91014.1716 - mae: 232.6368 - val_loss: 87163.6953 - val_mse: 85135.3906 - val_mae: 241.8691\n",
      "Epoch 183/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77893.7738 - mse: 75698.2559 - mae: 222.1966 - val_loss: 80216.1797 - val_mse: 78212.2188 - val_mae: 232.1079\n",
      "Epoch 184/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73459.4213 - mse: 71457.3002 - mae: 212.5699 - val_loss: 83724.9219 - val_mse: 81751.3594 - val_mae: 238.4575\n",
      "Epoch 185/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58112.5775 - mse: 56013.5326 - mae: 190.4526 - val_loss: 86486.7031 - val_mse: 84445.4922 - val_mae: 242.5156\n",
      "Epoch 186/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72048.8943 - mse: 70040.2201 - mae: 214.6722 - val_loss: 86179.3047 - val_mse: 84142.4688 - val_mae: 241.8918\n",
      "Epoch 187/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 68823.3874 - mse: 66723.2910 - mae: 211.8378 - val_loss: 84656.9609 - val_mse: 82703.2266 - val_mae: 239.4212\n",
      "Epoch 188/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 67724.2059 - mse: 65646.4550 - mae: 206.9294 - val_loss: 77458.6875 - val_mse: 75390.6953 - val_mae: 227.8646\n",
      "Epoch 189/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70438.9251 - mse: 68251.8094 - mae: 214.8427 - val_loss: 90072.9844 - val_mse: 88228.6250 - val_mae: 247.6312\n",
      "Epoch 190/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63897.3753 - mse: 61938.8687 - mae: 205.1863 - val_loss: 90450.5391 - val_mse: 88647.5703 - val_mae: 247.6684\n",
      "Epoch 191/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60484.8464 - mse: 58569.4463 - mae: 196.5765 - val_loss: 82316.9922 - val_mse: 80701.8672 - val_mae: 235.5075\n",
      "Epoch 192/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69471.0565 - mse: 67771.8372 - mae: 209.9496 - val_loss: 95654.5469 - val_mse: 94265.9844 - val_mae: 255.8388\n",
      "Epoch 193/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69311.0032 - mse: 67796.0868 - mae: 210.8052 - val_loss: 102510.1250 - val_mse: 101323.7891 - val_mae: 266.0436\n",
      "Epoch 194/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64113.2877 - mse: 62775.9492 - mae: 195.6936 - val_loss: 74803.8984 - val_mse: 73339.5703 - val_mae: 223.9658\n",
      "Epoch 195/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70784.1993 - mse: 69158.5712 - mae: 211.2781 - val_loss: 94018.1797 - val_mse: 92616.4297 - val_mae: 253.1553\n",
      "Epoch 196/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69105.8141 - mse: 67563.1112 - mae: 207.2421 - val_loss: 78249.8359 - val_mse: 76621.2734 - val_mae: 228.4421\n",
      "Epoch 197/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60241.4157 - mse: 58547.3456 - mae: 198.6631 - val_loss: 64343.3438 - val_mse: 62446.7461 - val_mae: 206.1335\n",
      "Epoch 198/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67332.8472 - mse: 65613.9376 - mae: 212.5908 - val_loss: 73148.1250 - val_mse: 71388.5391 - val_mae: 221.0801\n",
      "Epoch 199/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64678.2519 - mse: 63000.6882 - mae: 200.4927 - val_loss: 70946.6719 - val_mse: 69366.0234 - val_mae: 217.7185\n",
      "Epoch 200/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67784.7483 - mse: 66130.5481 - mae: 206.9550 - val_loss: 85649.8906 - val_mse: 84250.1719 - val_mae: 241.6745\n",
      "Epoch 201/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60910.0578 - mse: 59386.9611 - mae: 198.1931 - val_loss: 88997.0547 - val_mse: 87571.3438 - val_mae: 246.6684\n",
      "Epoch 202/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69908.2168 - mse: 68535.1488 - mae: 208.6597 - val_loss: 87761.0156 - val_mse: 86503.8984 - val_mae: 245.0099\n",
      "Epoch 203/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 75259.1040 - mse: 73840.3348 - mae: 221.2698 - val_loss: 87155.7891 - val_mse: 85589.2109 - val_mae: 243.7842\n",
      "Epoch 204/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58808.3405 - mse: 57094.3575 - mae: 190.9362 - val_loss: 88730.8984 - val_mse: 87280.5781 - val_mae: 246.1107\n",
      "Epoch 205/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 65247.0336 - mse: 63644.3052 - mae: 204.9686 - val_loss: 77138.6484 - val_mse: 75570.4766 - val_mae: 227.9650\n",
      "Epoch 206/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73141.0102 - mse: 71518.8391 - mae: 210.3913 - val_loss: 81219.8359 - val_mse: 79706.5859 - val_mae: 234.8671\n",
      "Epoch 207/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63363.8331 - mse: 61813.6201 - mae: 203.7142 - val_loss: 75773.9453 - val_mse: 74233.8906 - val_mae: 225.9938\n",
      "Epoch 208/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56570.5874 - mse: 55082.6184 - mae: 187.6400 - val_loss: 85127.7734 - val_mse: 83814.5469 - val_mae: 241.4681\n",
      "Epoch 209/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61673.3568 - mse: 60282.5151 - mae: 197.2151 - val_loss: 92529.5703 - val_mse: 91348.2344 - val_mae: 252.5816\n",
      "Epoch 210/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 78411.0000 - mse: 77088.7841 - mae: 218.6427 - val_loss: 84103.5781 - val_mse: 82707.6250 - val_mae: 239.7269\n",
      "Epoch 211/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 76209.5766 - mse: 74806.1334 - mae: 218.9336 - val_loss: 80580.2734 - val_mse: 79221.1797 - val_mae: 233.5807\n",
      "Epoch 212/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57766.1547 - mse: 56358.9539 - mae: 190.7945 - val_loss: 79805.9531 - val_mse: 78539.7734 - val_mae: 232.6925\n",
      "Epoch 213/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60834.5069 - mse: 59423.8042 - mae: 196.3786 - val_loss: 74821.5938 - val_mse: 73462.4922 - val_mae: 224.2751\n",
      "Epoch 214/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60780.6492 - mse: 59465.0535 - mae: 196.5576 - val_loss: 82046.0156 - val_mse: 80823.3359 - val_mae: 236.5294\n",
      "Epoch 215/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63170.9988 - mse: 61929.3071 - mae: 205.8885 - val_loss: 80649.8828 - val_mse: 79361.6875 - val_mae: 233.9330\n",
      "Epoch 216/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 53651.2280 - mse: 52348.0082 - mae: 188.8366 - val_loss: 68849.6250 - val_mse: 67512.2500 - val_mae: 214.5489\n",
      "Epoch 217/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61446.8013 - mse: 60191.7635 - mae: 200.6323 - val_loss: 72526.7422 - val_mse: 71348.2734 - val_mae: 221.1455\n",
      "Epoch 218/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64402.2194 - mse: 63227.9344 - mae: 208.6575 - val_loss: 79746.4922 - val_mse: 78577.0234 - val_mae: 232.5708\n",
      "Epoch 219/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60452.1631 - mse: 59251.9783 - mae: 200.2853 - val_loss: 93094.9453 - val_mse: 92078.4453 - val_mae: 253.3038\n",
      "Epoch 220/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61227.3464 - mse: 60141.4983 - mae: 200.1726 - val_loss: 71318.4766 - val_mse: 70186.1875 - val_mae: 219.3910\n",
      "Epoch 221/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58397.7614 - mse: 57243.5569 - mae: 188.9849 - val_loss: 76834.8203 - val_mse: 75801.1641 - val_mae: 227.8004\n",
      "Epoch 222/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69803.7832 - mse: 68774.6545 - mae: 212.1546 - val_loss: 85366.3047 - val_mse: 84324.6562 - val_mae: 241.7483\n",
      "Epoch 223/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58996.0404 - mse: 57860.8376 - mae: 196.8575 - val_loss: 73300.9062 - val_mse: 72019.1328 - val_mae: 222.3106\n",
      "Epoch 224/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67858.0848 - mse: 66580.8319 - mae: 201.3063 - val_loss: 88705.8594 - val_mse: 87664.1875 - val_mae: 246.8701\n",
      "Epoch 225/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60151.0039 - mse: 58958.0488 - mae: 200.6124 - val_loss: 76142.7656 - val_mse: 75077.9844 - val_mae: 227.3787\n",
      "Epoch 226/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66261.9375 - mse: 65218.5101 - mae: 204.1894 - val_loss: 79709.9062 - val_mse: 78758.2891 - val_mae: 233.3403\n",
      "Epoch 227/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58585.6709 - mse: 57636.4546 - mae: 193.6046 - val_loss: 82639.2891 - val_mse: 81660.2266 - val_mae: 237.5910\n",
      "Epoch 228/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58286.3164 - mse: 57133.3464 - mae: 195.2916 - val_loss: 70543.3516 - val_mse: 69366.1250 - val_mae: 218.1331\n",
      "Epoch 229/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63699.7488 - mse: 62563.5887 - mae: 204.9439 - val_loss: 72109.7266 - val_mse: 71122.1094 - val_mae: 220.9517\n",
      "Epoch 230/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61424.6401 - mse: 60406.6874 - mae: 196.3256 - val_loss: 86120.7422 - val_mse: 85140.2578 - val_mae: 243.3392\n",
      "Epoch 231/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61381.6710 - mse: 60365.4762 - mae: 196.9263 - val_loss: 71393.8359 - val_mse: 70378.2812 - val_mae: 219.7066\n",
      "Epoch 232/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 54084.1862 - mse: 53005.0318 - mae: 186.1699 - val_loss: 86436.9297 - val_mse: 85523.9297 - val_mae: 242.5091\n",
      "Epoch 233/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57897.8405 - mse: 56909.6711 - mae: 192.4630 - val_loss: 78179.7109 - val_mse: 77279.8594 - val_mae: 230.6365\n",
      "Epoch 234/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 51743.8940 - mse: 50769.5480 - mae: 181.4703 - val_loss: 76601.6406 - val_mse: 75671.3594 - val_mae: 227.4702\n",
      "Epoch 235/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61201.2253 - mse: 60234.7988 - mae: 196.0699 - val_loss: 74465.3750 - val_mse: 73513.4453 - val_mae: 224.2608\n",
      "Epoch 236/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64436.5617 - mse: 63471.3915 - mae: 209.1656 - val_loss: 81449.1172 - val_mse: 80553.8594 - val_mae: 235.8196\n",
      "Epoch 237/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56023.2772 - mse: 55092.6791 - mae: 193.0702 - val_loss: 68024.0938 - val_mse: 67172.7578 - val_mae: 214.4625\n",
      "Epoch 238/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61526.6589 - mse: 60622.3555 - mae: 197.4003 - val_loss: 69037.6250 - val_mse: 68131.1328 - val_mae: 215.5122\n",
      "Epoch 239/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66768.7695 - mse: 65922.0169 - mae: 209.7492 - val_loss: 60653.2148 - val_mse: 59739.3906 - val_mae: 202.0529\n",
      "Epoch 240/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63221.2699 - mse: 62308.4438 - mae: 196.6007 - val_loss: 67771.0781 - val_mse: 66806.9531 - val_mae: 213.8387\n",
      "Epoch 241/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 53716.9129 - mse: 52831.3510 - mae: 187.3102 - val_loss: 72989.5625 - val_mse: 72163.3125 - val_mae: 222.6527\n",
      "Epoch 242/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60904.1741 - mse: 60031.9572 - mae: 196.3723 - val_loss: 71960.2656 - val_mse: 71128.5391 - val_mae: 220.2506\n",
      "Epoch 243/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57782.1667 - mse: 56956.4882 - mae: 195.5866 - val_loss: 69619.7578 - val_mse: 68725.0938 - val_mae: 216.5990\n",
      "Epoch 244/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58339.4489 - mse: 57424.9973 - mae: 194.6056 - val_loss: 69848.7500 - val_mse: 68937.8047 - val_mae: 216.8045\n",
      "Epoch 245/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58917.8545 - mse: 58020.5773 - mae: 198.1885 - val_loss: 71993.5391 - val_mse: 71117.8984 - val_mae: 220.4439\n",
      "Epoch 246/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59308.1667 - mse: 58418.9335 - mae: 200.9133 - val_loss: 77807.4844 - val_mse: 76804.3438 - val_mae: 229.6825\n",
      "Epoch 247/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61026.7394 - mse: 59976.8660 - mae: 197.2020 - val_loss: 64589.0938 - val_mse: 63585.6875 - val_mae: 208.4955\n",
      "Epoch 248/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 66359.9752 - mse: 65420.6225 - mae: 206.1102 - val_loss: 75609.6094 - val_mse: 74760.7188 - val_mae: 226.8046\n",
      "Epoch 249/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66920.2771 - mse: 66086.1398 - mae: 206.1938 - val_loss: 83023.5234 - val_mse: 82083.4844 - val_mae: 238.4381\n",
      "Epoch 250/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60471.0169 - mse: 59518.6286 - mae: 196.9522 - val_loss: 79229.9219 - val_mse: 78377.2266 - val_mae: 232.8387\n",
      "Epoch 251/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 53752.6312 - mse: 52908.3472 - mae: 188.4363 - val_loss: 62765.5117 - val_mse: 61845.0625 - val_mae: 204.5958\n",
      "Epoch 252/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69299.6371 - mse: 68335.7482 - mae: 208.9343 - val_loss: 85269.5156 - val_mse: 84547.4219 - val_mae: 242.2412\n",
      "Epoch 253/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61028.6689 - mse: 60219.6184 - mae: 198.3018 - val_loss: 79177.2344 - val_mse: 78441.9531 - val_mae: 232.9505\n",
      "Epoch 254/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65084.9341 - mse: 64263.3967 - mae: 209.1035 - val_loss: 70581.4844 - val_mse: 69778.0000 - val_mae: 218.6780\n",
      "Epoch 255/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60106.7895 - mse: 59251.7057 - mae: 196.8650 - val_loss: 70166.0391 - val_mse: 69396.3125 - val_mae: 217.9355\n",
      "Epoch 256/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63167.8816 - mse: 62352.7469 - mae: 205.6262 - val_loss: 68781.6953 - val_mse: 67877.9375 - val_mae: 215.3648\n",
      "Epoch 257/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61641.3278 - mse: 60787.7087 - mae: 196.2740 - val_loss: 77777.1406 - val_mse: 76977.3906 - val_mae: 230.4983\n",
      "Epoch 258/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65740.0058 - mse: 64918.8261 - mae: 208.6882 - val_loss: 70261.5156 - val_mse: 69364.6953 - val_mae: 217.9184\n",
      "Epoch 259/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60791.7044 - mse: 59948.0471 - mae: 196.6226 - val_loss: 81980.0078 - val_mse: 81366.9922 - val_mae: 237.5135\n",
      "Epoch 260/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66858.5807 - mse: 66171.3167 - mae: 213.0596 - val_loss: 81336.6484 - val_mse: 80685.9453 - val_mae: 236.5643\n",
      "Epoch 261/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 58439.9584 - mse: 57701.1157 - mae: 199.4214 - val_loss: 65536.7266 - val_mse: 64682.3477 - val_mae: 210.3703\n",
      "Epoch 262/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58715.7741 - mse: 57913.3981 - mae: 197.8173 - val_loss: 84945.3359 - val_mse: 84369.2188 - val_mae: 242.2639\n",
      "Epoch 263/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60313.1734 - mse: 59619.2551 - mae: 197.2500 - val_loss: 68461.7734 - val_mse: 67671.3906 - val_mae: 215.3334\n",
      "Epoch 264/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65624.5609 - mse: 64837.5894 - mae: 209.8482 - val_loss: 66043.3359 - val_mse: 65194.3711 - val_mae: 211.2057\n",
      "Epoch 265/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61727.0069 - mse: 60937.5049 - mae: 200.8344 - val_loss: 72297.2969 - val_mse: 71584.0938 - val_mae: 221.6830\n",
      "Epoch 266/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61032.3755 - mse: 60263.5421 - mae: 201.2342 - val_loss: 73221.3438 - val_mse: 72532.9062 - val_mae: 223.2328\n",
      "Epoch 267/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56561.9927 - mse: 55907.0028 - mae: 193.2410 - val_loss: 67089.8359 - val_mse: 66323.7188 - val_mae: 213.0775\n",
      "Epoch 268/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63099.3348 - mse: 62370.4502 - mae: 203.9471 - val_loss: 93422.9375 - val_mse: 92937.0469 - val_mae: 255.0394\n",
      "Epoch 269/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62404.2397 - mse: 61782.8693 - mae: 200.4370 - val_loss: 68585.1562 - val_mse: 67827.1250 - val_mae: 215.5354\n",
      "Epoch 270/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55166.4692 - mse: 54423.4368 - mae: 189.6445 - val_loss: 73068.8281 - val_mse: 72365.2422 - val_mae: 222.5613\n",
      "Epoch 271/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59834.8141 - mse: 59160.5743 - mae: 199.0418 - val_loss: 80811.7031 - val_mse: 80217.5469 - val_mae: 235.6482\n",
      "Epoch 272/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63134.1044 - mse: 62436.5767 - mae: 202.8864 - val_loss: 78354.3125 - val_mse: 77693.8047 - val_mae: 231.8345\n",
      "Epoch 273/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59865.9796 - mse: 59160.6495 - mae: 197.9265 - val_loss: 76967.5859 - val_mse: 76352.4297 - val_mae: 229.6235\n",
      "Epoch 274/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65333.6364 - mse: 64672.3625 - mae: 204.4153 - val_loss: 91795.1875 - val_mse: 91343.7656 - val_mae: 252.6953\n",
      "Epoch 275/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57149.8577 - mse: 56523.0986 - mae: 191.2025 - val_loss: 86647.1641 - val_mse: 85976.6406 - val_mae: 244.6173\n",
      "Epoch 276/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 55086.5886 - mse: 54306.6647 - mae: 189.2806 - val_loss: 85262.9297 - val_mse: 84642.0781 - val_mae: 242.6567\n",
      "Epoch 277/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59542.0034 - mse: 58869.0297 - mae: 202.1050 - val_loss: 82282.8438 - val_mse: 81615.4219 - val_mae: 237.9264\n",
      "Epoch 278/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64029.6434 - mse: 63335.9968 - mae: 200.1828 - val_loss: 75602.0391 - val_mse: 74955.6328 - val_mae: 227.3503\n",
      "Epoch 279/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66733.3300 - mse: 66014.3539 - mae: 206.5397 - val_loss: 69107.2266 - val_mse: 68453.7109 - val_mae: 216.3082\n",
      "Epoch 280/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56371.7660 - mse: 55638.3540 - mae: 190.3224 - val_loss: 95173.6484 - val_mse: 94621.4609 - val_mae: 257.3790\n",
      "Epoch 281/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64352.2309 - mse: 63748.8895 - mae: 208.9304 - val_loss: 66405.4688 - val_mse: 65572.8047 - val_mae: 211.5410\n",
      "Epoch 282/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72138.3090 - mse: 71346.5391 - mae: 217.0710 - val_loss: 83276.5312 - val_mse: 82626.0938 - val_mae: 239.4677\n",
      "Epoch 283/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64363.1641 - mse: 63692.5924 - mae: 209.2672 - val_loss: 77083.5781 - val_mse: 76304.8281 - val_mae: 228.7778\n",
      "Epoch 284/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59372.8912 - mse: 58561.3857 - mae: 199.8828 - val_loss: 72295.9219 - val_mse: 71536.8203 - val_mae: 221.5331\n",
      "Epoch 285/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 68025.0601 - mse: 67310.7671 - mae: 206.7765 - val_loss: 76570.4219 - val_mse: 75894.3281 - val_mae: 228.7602\n",
      "Epoch 286/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58205.7500 - mse: 57526.3125 - mae: 193.7260 - val_loss: 83574.9609 - val_mse: 82954.4922 - val_mae: 239.8289\n",
      "Epoch 287/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58416.1769 - mse: 57763.2327 - mae: 194.5520 - val_loss: 69227.6094 - val_mse: 68465.0938 - val_mae: 216.4128\n",
      "Epoch 288/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61241.5197 - mse: 60512.7344 - mae: 202.4513 - val_loss: 85195.3594 - val_mse: 84593.3594 - val_mae: 242.3510\n",
      "Epoch 289/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 51094.6235 - mse: 50440.0807 - mae: 179.4671 - val_loss: 76682.8906 - val_mse: 76142.8125 - val_mae: 229.1146\n",
      "Epoch 290/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56859.7578 - mse: 56217.0450 - mae: 193.2976 - val_loss: 78643.8594 - val_mse: 78078.4531 - val_mae: 232.2082\n",
      "Epoch 291/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 55532.4176 - mse: 54896.9229 - mae: 185.3304 - val_loss: 68329.0078 - val_mse: 67653.3125 - val_mae: 215.3864\n",
      "Epoch 292/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 54772.2015 - mse: 54028.6956 - mae: 186.8683 - val_loss: 74610.7656 - val_mse: 73938.5547 - val_mae: 225.8185\n",
      "Epoch 293/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56936.6614 - mse: 56254.4918 - mae: 194.3023 - val_loss: 74261.0703 - val_mse: 73515.0391 - val_mae: 225.0125\n",
      "Epoch 294/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 55724.4422 - mse: 55034.4542 - mae: 190.4294 - val_loss: 68667.7969 - val_mse: 67920.7578 - val_mae: 215.8619\n",
      "Epoch 295/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65722.8705 - mse: 64982.4680 - mae: 208.9257 - val_loss: 86815.0703 - val_mse: 86204.9219 - val_mae: 245.1028\n",
      "Epoch 296/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69421.3170 - mse: 68769.3503 - mae: 219.7285 - val_loss: 68965.1172 - val_mse: 68298.6250 - val_mae: 216.5137\n",
      "Epoch 297/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63631.7721 - mse: 62927.1621 - mae: 201.7521 - val_loss: 73868.3281 - val_mse: 73236.8828 - val_mae: 224.7268\n",
      "Epoch 298/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63259.2373 - mse: 62583.8113 - mae: 203.3201 - val_loss: 72466.2344 - val_mse: 71789.4922 - val_mae: 222.4081\n",
      "Epoch 299/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56971.4038 - mse: 56307.7566 - mae: 190.2111 - val_loss: 71389.5547 - val_mse: 70712.8047 - val_mae: 220.5066\n",
      "Epoch 300/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57595.4195 - mse: 56900.1571 - mae: 196.1229 - val_loss: 80420.0234 - val_mse: 79849.3672 - val_mae: 235.2631\n"
     ]
    }
   ],
   "source": [
    "history = nn.fit(X_train, y_train, epochs=300, batch_size=8, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAq0UlEQVR4nO3de3xdZZ3v8c9vX7J3Lr2kaSj0RguDUiilLbHCgAqDYkEBEbBlxBEP2hkODDpHfQ3ojDiMnoOjo4yKAmq9HQSxinTmgFwURAfBtlhqW261FJuUtuklbdrc9uV3/lgryU6yE5KS1Z2k3/frlVfWep619vqt7CS//TzPWs8yd0dERKS3WKkDEBGRkUkJQkREilKCEBGRopQgRESkKCUIEREpSglCRESKUoIQGQZm9j0z+9wgt91iZm9/va8jEjUlCBERKUoJQkREilKCkCNG2LXzSTNbZ2YHzew7ZjbFzB40s2Yze9TMqgu2v8jMNphZk5k9bmZzCuoWmNkz4X4/BtK9jvVuM1sb7vukmc07xJg/YmabzGyPma00s6lhuZnZV8xsp5ntN7M/mtncsO4CM9sYxtZgZp84pB+YHPGUIORIcynwDuANwIXAg8CngFqCv4frAczsDcDdwMfCugeA/zSzMjMrA34O/BCYBPwkfF3CfRcAy4G/BWqAO4CVZpYaSqBm9lfA/wHeBxwDvALcE1afB7w1PI8J4Ta7w7rvAH/r7uOAucCvhnJckU5jLkGY2fLwU9X6QWz7lfBT3loze9HMmg5DiFJaX3P3He7eAPwGeNrd/+DubcB9wIJwuyXA/3P3R9w9A3wJKAf+EjgdSAK3unvG3VcAqwqOsQy4w92fdvecu38faA/3G4r3A8vd/Rl3bwduBM4ws1lABhgHnAiYuz/n7q+G+2WAk8xsvLvvdfdnhnhcEWAMJgjge8DiwWzo7v/g7vPdfT7wNeBnEcYlI8OOguXWIutV4fJUgk/sALh7HtgKTAvrGrznTJevFCwfC3w87F5qCj94zAj3G4reMRwgaCVMc/dfAV8HbgN2mtmdZjY+3PRS4ALgFTP7tZmdMcTjigBjMEG4+xPAnsIyMzvezH5hZmvM7DdmdmKRXa8g6FIQAdhG8I8eCPr8Cf7JNwCvAtPCsk4zC5a3Ap9394kFXxXuPtTfr94xVBJ0WTUAuPtX3f004CSCrqZPhuWr3P1i4CiCrrB7h3hcEWAMJoh+3An8ffjH9AngG4WVZnYsMBv11Uq3e4F3mdm5ZpYEPk7QTfQk8DsgC1xvZkkzey+wqGDfbwF/Z2ZvDgeTK83sXWY2bogx3A18yMzmh+MX/5ugS2yLmb0pfP0kcBBoA/LhGMn7zWxC2DW2H8i/jp+DHMHGfIIwsyqCfuOfmNlaggHDY3ptthRY4e65wxyejFDu/gJwJUHX4y6CAe0L3b3D3TuA9wJXEbRWl1DQPenuq4GPEHQB7QU2hdsONYZHgX8GfkrQajme4HcVYDxBItpL0A21G/hiWPcBYIuZ7Qf+jmAsQ2TIbCw+MCgcxPsvd58b9su+4O69k0Lh9n8ArnX3Jw9XjCIiI92Yb0G4+37gZTO7HLquHz+1sz4cj6gm6DYQEZHQmEsQZnY3wT/7N5pZvZldTdDEvtrMngU2ABcX7LIUuMfHYlNKROR1GJNdTCIi8vqNuRaEiIgMj0SpAxhOkydP9lmzZpU6DBGRUWPNmjW73L22WN2YShCzZs1i9erVpQ5DRGTUMLNX+qtTF5OIiBSlBCEiIkUpQYiISFFjagyimEwmQ319PW1tbaUOZUxIp9NMnz6dZDJZ6lBEJGJjPkHU19czbtw4Zs2aRc/JN2Wo3J3du3dTX1/P7NmzSx2OiERszHcxtbW1UVNTo+QwDMyMmpoatcZEjhBjPkEASg7DSD9LkSPHEZEgXsuO/W00t2VKHYaIyIiiBAE0NrdzoD0byWs3NTXxjW9847U37OWCCy6gqalp+AMSERkkJYhQVHMW9pcgstmBE9IDDzzAxIkTowlKRGQQIruKycyWA+8Gdrr73CL1n6T7SVcJYA5Q6+57zGwL0AzkgKy710UVZ9RuuOEG/vSnPzF//nySySTpdJrq6mqef/55XnzxRd7znvewdetW2tra+OhHP8qyZcuA7mlDDhw4wPnnn89ZZ53Fk08+ybRp07j//vspLy8v8ZmJyFgX5WWu3yN45OIPilW6+xcJH5FoZhcC/+Duewo2Ocfddw1nQP/ynxvYuG1/n/KWjiyJWIyyxNAbVCdNHc9NF57cb/0tt9zC+vXrWbt2LY8//jjvete7WL9+fddlosuXL2fSpEm0trbypje9iUsvvZSampoer/HSSy9x9913861vfYv3ve99/PSnP+XKK68ccqwiIkMRWReTuz9B8LzewbiC4AHtY96iRYt63EPw1a9+lVNPPZXTTz+drVu38tJLL/XZZ/bs2cyfPx+A0047jS1bthymaEXkSFbyG+XMrAJYDFxXUOzAw2bmwB3ufucA+y8DlgHMnDlzwGP190l/w7Z9VFeUMXVi9N02lZWVXcuPP/44jz76KL/73e+oqKjg7LPPLnqPQSqV6lqOx+O0trZGHqeIyEgYpL4Q+O9e3UtnuftC4HzgWjN7a387u/ud7l7n7nW1tUWnNB+UqJ6rN27cOJqbm4vW7du3j+rqaioqKnj++ed56qmnIopCRGToSt6CIHgmdI/uJXdvCL/vNLP7gEXAE1EFYBBZhqipqeHMM89k7ty5lJeXM2XKlK66xYsXc/vttzNnzhze+MY3cvrpp0cThIjIIShpgjCzCcDbgCsLyiqBmLs3h8vnATdHHAnRtSHgRz/6UdHyVCrFgw8+WLSuc5xh8uTJrF+/vqv8E5/4xLDHJyJSTJSXud4NnA1MNrN64CYgCeDut4ebXQI87O4HC3adAtwXTumQAH7k7r+IKk4RESkusgTh7lcMYpvvEVwOW1i2GTg1mqgGiOVwH1BEZIQbCYPUJaf550RE+lKC6KQmhIhID0oQIiJSlBJESA0IEZGelCAI74MYIaqqqgDYtm0bl112WdFtzj77bFavXj3g69x66620tLR0rWv6cBEZKiWIEWrq1KmsWLHikPfvnSA0fbiIDJUSBETahLjhhhu47bbbutY/+9nP8rnPfY5zzz2XhQsXcsopp3D//ff32W/Lli3MnRvMkt7a2srSpUuZM2cOl1xySY+5mK655hrq6uo4+eSTuemmm4BgAsBt27ZxzjnncM455wDB9OG7dgWT4375y19m7ty5zJ07l1tvvbXreHPmzOEjH/kIJ598Muedd57mfBI5wo2EqTYOnwdvgO1/7FM8syNLLGaQiA/9NY8+Bc6/pd/qJUuW8LGPfYxrr70WgHvvvZeHHnqI66+/nvHjx7Nr1y5OP/10Lrroon6f9/zNb36TiooKnnvuOdatW8fChQu76j7/+c8zadIkcrkc5557LuvWreP666/ny1/+Mo899hiTJ0/u8Vpr1qzhu9/9Lk8//TTuzpvf/Gbe9ra3UV1drWnFRaQHtSAitmDBAnbu3Mm2bdt49tlnqa6u5uijj+ZTn/oU8+bN4+1vfzsNDQ3s2LGj39d44oknuv5Rz5s3j3nz5nXV3XvvvSxcuJAFCxawYcMGNm7cOGA8v/3tb7nkkkuorKykqqqK9773vfzmN78BNK24iPR0ZLUg+vmkv3V7M+XJODNrKiI57OWXX86KFSvYvn07S5Ys4a677qKxsZE1a9aQTCaZNWtW0Wm+X8vLL7/Ml770JVatWkV1dTVXXXXVIb1OJ00rLiKF1ILoEt2FrkuWLOGee+5hxYoVXH755ezbt4+jjjqKZDLJY489xiuvvDLg/m9961u7Jvxbv34969atA2D//v1UVlYyYcIEduzY0WPiv/6mGX/LW97Cz3/+c1paWjh48CD33Xcfb3nLW4bxbEVkrDiyWhADiPI+iJNPPpnm5mamTZvGMcccw/vf/34uvPBCTjnlFOrq6jjxxBMH3P+aa67hQx/6EHPmzGHOnDmcdtppAJx66qksWLCAE088kRkzZnDmmWd27bNs2TIWL17M1KlTeeyxx7rKFy5cyFVXXcWiRYsA+PCHP8yCBQvUnSQifZj72LlFrK6uznvfH/Dcc88xZ86cAfd7YXsz6WSMY2sqB9xOAoP5mYrI6GBma9y9rlidupgYWTfKiYiMFEoQoAwhIlLEEZEgBtONNoZ62iI1lrokRWRgYz5BpNNpdu/erX9sw8Dd2b17N+l0utShiMhhMOavYpo+fTr19fU0Njb2u83O/W3EY0ZrY6rfbSSQTqeZPn16qcMQkcMgymdSLwfeDex097lF6s8G7gdeDot+5u43h3WLgf8A4sC33b3/uSxeQzKZZPbs2QNu84mv/oajx6f5zlXzD/UwIiJjTpRdTN8DFr/GNr9x9/nhV2dyiAO3AecDJwFXmNlJEcaJmZ4HISLSW2QJwt2fAPYcwq6LgE3uvtndO4B7gIuHNbheYmYaoxAR6aXUg9RnmNmzZvagmZ0clk0DthZsUx+WFWVmy8xstZmtHmicYSAG5JUfRER6KGWCeAY41t1PBb4G/PxQXsTd73T3Onevq62tPaRAzExdTCIivZQsQbj7fnc/EC4/ACTNbDLQAMwo2HR6WBYZM13fLyLSW8kShJkdbeETcsxsURjLbmAVcIKZzTazMmApsDLSWNCNciIivUV5mevdwNnAZDOrB24CkgDufjtwGXCNmWWBVmCpBx/js2Z2HfAQwWWuy919Q1RxQjhIrU4mEZEeIksQ7n7Fa9R/Hfh6P3UPAA9EEVcxZpDPH66jiYiMDqW+imlEMNSCEBHpTQmCzkHqUkchIjKyKEGgBCEiUowSBOpiEhEpRgkCiMXUghAR6U0JgqAFkVeGEBHpQQkCzeYqIlKMEgTBXEyarE9EpCclCIKpNjQIISLSkxIEEFMXk4hIH0oQdHYxKUWIiBRSgkCzuYqIFKMEQfjAICUIEZEelCAIZ3NVhhAR6UEJgvAqJhER6UEJgvCBQWpAiIj0oASBuphERIpRgqDzkaMiIlJICQJALQgRkT4iSxBmttzMdprZ+n7q329m68zsj2b2pJmdWlC3JSxfa2aro4qx63igW6lFRHqJsgXxPWDxAPUvA29z91OAfwXu7FV/jrvPd/e6iOLroi4mEZG+ElG9sLs/YWazBqh/smD1KWB6VLG8Fg1Si4j0NVLGIK4GHixYd+BhM1tjZssG2tHMlpnZajNb3djYeEgH11QbIiJ9RdaCGCwzO4cgQZxVUHyWuzeY2VHAI2b2vLs/UWx/d7+TsHuqrq7ukP7NB11MyhAiIoVK2oIws3nAt4GL3X13Z7m7N4TfdwL3AYuiDQTy+UiPICIy6pQsQZjZTOBnwAfc/cWC8kozG9e5DJwHFL0Sathi0WQbIiJ9RNbFZGZ3A2cDk82sHrgJSAK4++3AZ4Aa4BtmBpANr1iaAtwXliWAH7n7L6KKE8IHBmkQQkSkhyivYrriNeo/DHy4SPlm4NS+e0QnuIrpcB5RRGTkGylXMZWUBqlFRPpSgkAtCBGRYpQgANB03yIivSlBEAxSazImEZGelCBQF5OISDFKEAT3QegyVxGRnpQgCO+DKHUQIiIjjBIEYGbk1cckItKDEkRI6UFEpCclCIIb5ZQhRER6UoJADwwSESlGCQINUouIFKMEQThIrRaEiEgPShDokaMiIsUoQRC0IJQfRER6UoIgGKTWndQiIj0pQaAuJhGRYpQg6HxgkIiIFFKCQPdBiIgUE2mCMLPlZrbTzNb3U29m9lUz22Rm68xsYUHdB83spfDrg5HGibqYRER6i7oF8T1g8QD15wMnhF/LgG8CmNkk4CbgzcAi4CYzq44qSDMDNFAtIlIo0gTh7k8AewbY5GLgBx54CphoZscA7wQecfc97r4XeISBE83rEuYHtSJERAqUegxiGrC1YL0+LOuvvA8zW2Zmq81sdWNj4yEFYYQtiEPaW0RkbCp1gnjd3P1Od69z97ra2tpDeo1Y2ILQQLWISLdSJ4gGYEbB+vSwrL/ySKiLSUSkr1IniJXA34RXM50O7HP3V4GHgPPMrDocnD4vLItE1yC1OplERLokonxxM7sbOBuYbGb1BFcmJQHc/XbgAeACYBPQAnworNtjZv8KrApf6mZ3H2iw+3XGGXxXC0JEpNugEoSZfRT4LtAMfBtYANzg7g8PtJ+7X/Ea9Q5c20/dcmD5YOJ7vboGqZUgRES6DLaL6X+4+36Crp5q4APALZFFdZh1DlKri0lEpNtgE0T4L5QLgB+6+4aCslHPuq5iKm0cIiIjyWATxBoze5ggQTxkZuOAfHRhHV7dXUzKECIinQY7SH01MB/Y7O4t4VQYH4osqsOsa5C6tGGIiIwog21BnAG84O5NZnYl8E/AvujCOry6LnMdM20iEZHXb7AJ4ptAi5mdCnwc+BPwg8iiOsw6B1M0SC0i0m2wCSIbXpJ6MfB1d78NGBddWIdXTPdBiIj0MdgxiGYzu5Hg8ta3mFmM8Ia3saCzi0lzMYmIdBtsC2IJ0E5wP8R2grmRvhhZVIdZTIPUIiJ9DCpBhEnhLmCCmb0baHP3MTMGgVoQIiJ9DCpBmNn7gN8DlwPvA542s8uiDOxw6rrjT/lBRKTLYMcgPg28yd13AphZLfAosCKqwA6nmOmBQSIivQ12DCLWmRxCu4ew74hnemCQiEgfg21B/MLMHgLuDteXEEzVPSZ03Qeh/CAi0mVQCcLdP2lmlwJnhkV3uvt90YV1eKmLSUSkr0E/MMjdfwr8NMJYSqezi0nTuYqIdBkwQZhZM8U/WBvB837GRxLVYTZm5i0XERlGAyYIdx8z02kMpKuLSQ0IEZEukV6JZGaLzewFM9tkZjcUqf+Kma0Nv140s6aCulxB3cpo4wy+6yomEZFugx6DGCoziwO3Ae8A6oFVZrbS3Td2buPu/1Cw/d8TPOu6U6u7z48qvkIapBYR6SvKFsQiYJO7b3b3DuAegtlg+3MF3ZfRHlZqQYiI9BVlgpgGbC1Yrw/L+jCzY4HZwK8KitNmttrMnjKz9/R3EDNbFm63urGx8XUFrPwgItJtpNwNvRRY4e65grJj3b0O+GvgVjM7vtiO7n6nu9e5e11tbe0hHbyzi0mdTCIi3aJMEA3AjIL16WFZMUvp1b3k7g3h983A4/QcnxhW3V1MUR1BRGT0iTJBrAJOMLPZZlZGkAT6XI1kZicC1cDvCsqqzSwVLk8muIN7Y+99h4uhy1xFRHqL7Comd8+a2XXAQ0AcWO7uG8zsZmC1u3cmi6XAPeEjTTvNAe4wszxBErul8Oqn4db9wCBlCBGRTpElCAB3f4Bek/q5+2d6rX+2yH5PAqdEGVuhri6m/OE6oojIyDdSBqlLrPM+CLUgREQ6KUFQ0MWk/CAi0kUJAjDNxSQi0ocSBBqkFhEpRgkC3QchIlKMEgSF90EoQ4iIdFKCoLsFofQgItJNCYLCQWqlCBGRTkoQdD9yVPlBRKSbEgR6YJCISDFKEBROtaEUISLSSQmCgi6mkkYhIjKyKEGgO6lFRIpRgqDgMldlCBGRLkoQaJBaRKQYJQgKp9pQihAR6aQEge6DEBEpRgmCgkHqEschIjKSKEGgLiYRkWIiTRBmttjMXjCzTWZ2Q5H6q8ys0czWhl8fLqj7oJm9FH59MNI4OxeUH0REuiSiemEziwO3Ae8A6oFVZrbS3Tf22vTH7n5dr30nATcBdQT/tteE++6NItbuq5iUIUREOkXZglgEbHL3ze7eAdwDXDzIfd8JPOLue8Kk8AiwOKI4C6baiOoIIiKjT5QJYhqwtWC9Pizr7VIzW2dmK8xsxhD3xcyWmdlqM1vd2Nh4SIF2PTDokPYWERmbSj1I/Z/ALHefR9BK+P5QX8Dd73T3Onevq62tPaQgdCe1iEhfUSaIBmBGwfr0sKyLu+929/Zw9dvAaYPddzjpmdQiIn1FmSBWASeY2WwzKwOWAisLNzCzYwpWLwKeC5cfAs4zs2ozqwbOC8si0TlIrU4mEZFukV3F5O5ZM7uO4B97HFju7hvM7GZgtbuvBK43s4uALLAHuCrcd4+Z/StBkgG42d33RBWrWhAiIn1FliAA3P0B4IFeZZ8pWL4RuLGffZcDy6OMr1PXILUShIhIl1IPUo8Isc5BanUxiYh0UYJAXUwiIsUoQQB0dTEpQ4iIdFKCoLuLSUREuilB0D3dt2ZzFRHppgSBHhgkIlKMEgQFs7kqQYiIdFGCQA8MEhEpRgmCgsn6ShuGiMiIogRBwTOp1YIQEemiBIEGqUVEilGCoPCRoyIi0kkJAg1Si4gUowSBuphERIpRgqBgkLrEcYiIjCRKEOiZ1CIixShBoC4mEZFilCAonGpDGUJEpJMSBHpgkIhIMZEmCDNbbGYvmNkmM7uhSP3/MrONZrbOzH5pZscW1OXMbG34tTLiOAENUouIFEpE9cJmFgduA94B1AOrzGylu28s2OwPQJ27t5jZNcC/AUvCulZ3nx9VfD1jDb6ri0lEpFuULYhFwCZ33+zuHcA9wMWFG7j7Y+7eEq4+BUyPMJ5+aZBaRKSvKBPENGBrwXp9WNafq4EHC9bTZrbazJ4ys/f0t5OZLQu3W93Y2HhIgXZPtaEMISLSKbIupqEwsyuBOuBtBcXHunuDmR0H/MrM/ujuf+q9r7vfCdwJUFdXd0j/4TVILSLSV5QtiAZgRsH69LCsBzN7O/Bp4CJ3b+8sd/eG8Ptm4HFgQVSBGnqinIhIb1EmiFXACWY228zKgKVAj6uRzGwBcAdBcthZUF5tZqlweTJwJlA4uD2suh8YpAwhItIpsi4md8+a2XXAQ0AcWO7uG8zsZmC1u68EvghUAT8JLzX9s7tfBMwB7jCzPEESu6XX1U/DqvsqpqiOICIy+kQ6BuHuDwAP9Cr7TMHy2/vZ70nglChjK9TdxaQMISLSSXdSAzG1IERE+lCCoPtOal3FJCLSTQmCghaEBqlFRLooQaAWhIhIMUoQhTQIISLSRQkiFDPN5ioiUkgJImRm5PtpQfzjinX8x6MvHeaIRERKSwkiZPTfw/TL53fw6xd3Fq8UERmjRsRkfSNBzIy1W5vYub+No8anu8rbMjl2Hegg3nmpk4jIEUItiJDjPPmn3fz7wy/2KG9oagVgZ3M7Hdl8KUITESkJJYhQJhf0L63ftq9HecPeIEG4w/Z9bYc9LhGRUlEXUy8v7ThARzbPz9c2MHNSBfVhggCob2phZk1FCaMTETl8lCB66cjl+e2mRm782R+ZOamCxXOP7qrb1qQWhIgcOdTFVMTH732WXN55eddB7nrqFWrHpYDu7iYRkSOBEgTA03ewNP4rzohtYCLN7G3JcP7cozl2QoKDbe3UVJZROy7FM3/ey5ObdrF1T8ugX3rrnhbuX9tAezYX4QmIiAw/dTG5w8P/xC3Jjq6ibCxN/GXHcu2QhkxzOQcoZ/eWNHu3VPFg/gTWpN7MpIkTqK2eSHpcNX8+mGTzvjzlqTLmHDOek6aOZ8q4FJ+89xnam7bz7fFpLjhjHslEkvcunM6kyrIBQnKe+fNeXt7VwrvnHUM6GT8cPwkRkR5sLD0kp66uzlevXj20ndwh0wotu2H3S7BjAzRvh1gcysaB56F9P7nW/RzcvwcO7KSqcQ0x779F0O5JminngJczwQ5SbQcAyHqMRiay3SexyyaxP1mDJ6sgVUU2XkEuUUE8keDV5jzP7spTRpaptZM47YTpTJ40iaqqCcTSVby415kwfjxTqytobsty3ORKaqpS5POOWffkgyIir8XM1rh7XbE6tSDMoKwi+Jo4A47/q6KbxYHxnSv7t0HjC5DrgI6D0L4f2vZDpgXyORKZVmJNe0m37CNZWQWz68g7tO/eSkXTNibveIXpbTuo6HiOVGsLidYiyaazgbEP6JXzTgbybrRSRjtJMiRoIEm7x8mSIBMrI2NlHMwlSaTSJFIV5OMp2klBIkUiVUE8mcaS5cTK0jS2GYmyNFMmjieWTBFLlhFPpogn0ySSKcpSKTyWImNxUqlyUqk06XQ5qXQai6cgnux6buv2fW00NLWwcGa1EpXIKBdpgjCzxcB/EPx//ba739KrPgX8ADgN2A0scfctYd2NwNVADrje3R+KMtYhGT81+OpHHJjUqywGVIbLE3rvkG0PEk3HQchng8TTth8SZXimleb9+9izdw9tB5vJtTczJZ2lo+UArQf2kYrlaD54kI72dlKWJZbvIJltJ5Vrp4YOsh17iLW8StI7SNNB0jso8wwpywzbjwOgwxNkSFBGgukk2G5JcrEycrEkOUvQ4UkO5mIkU2ksXobHkpAIkovFy7BECi+rYt8bLuXV5EwmlCeprYyTtziTq1LEY0Y255SXxalKJagoC7rdcnknHjMlI5EIRJYgzCwO3Aa8A6gHVpnZSnffWLDZ1cBed/8LM1sKfAFYYmYnAUsJPixPBR41sze4D9CvM5olgk/2VPROK8EcUeMpaL0MA3entSNLa2sLra0HmFSW52BrC7v2NpPLtJPraCebbSeXaSefaSfT0U4sn6GMTFCfaSeXbSef6YBcO7F8kJji+Qzl8RyViTy7mpohF5TF8hkSlmVCWY5sx0HiHXuJe4a4Z0l4hiRZkmSpopX45uX8d/5kTo1tZry18GJ+Gi3EqaIViPGMz+QVn8Is28FBgqvL8sTIWIqymDPNdtEYn8KOxFTK4rCn7BhScaPmwAu0epqO5DhS3k5zbDz7krUkkyliqSriySQV3ko2UUmm4igmpRxLVZForqc8s5dsRS3tniSVLme8tTIh10Sm9mSSlROY2L6NyvwBGHc0+f3boWISqUwz7bE0eWKMr0hBvAya/gzZdiqnzCaWbSdfWQMteyFeRj5ZCW1N+IQZxHduhLJKSFURiydIJssglgh+G5q24MTI5TLEa46jpbWFTNN20glIVVVjHQegvTnYPpYIukpjye51iwEedK3Gk8HywV3Bh5JcJuhujSWg5nho2RN8L68O6nLtsK8BPBd0vboHLcfK2mD/TGvwISfbDqkqSE8A6zV+1pnIPR905U6YEcTRuhdS44PvyfLg7yHbFrxGshKat0EiHbx2th1S4yCZLjjP8Nw6Xz+fD47RKdcRnGuyIjhGe3P42uXQtDV4rbLK4FixeFCfSAU/m1gCKicHPQfjpwbr+VywXa4Ddm6ESccH+2fbINMWfM+2BbHGk8Fxk+VQVgU7N8C4Y4J489ngvPNZaNsH6fHBPrlMcL5llcF+uUxwbq1NwTHHTw1i8HxwnLYmmDhzGP9LBKJsQSwCNrn7ZgAzuwe4GChMEBcDnw2XVwBft+Cj4MXAPe7eDrxsZpvC1/tdhPEeMcyM8lSS8tQEmBi0Z8qBydOH7xh/MYRtc3mnLZOjae928v/9Nc54+X6ap7yDLalp1DRtpC0XI5OoJOEZztq7nne2PcO+1FTi+Q7c4pjniefbcIymxGQWtv+aVDa8Z+Vg8C1LggRZaB++cywVI/jDzblRadGPIbaTJMXwtjijkiFISEmKf5bMkCBJtms9S5xEP9sWymPE8K7vnXLEiBPtFDy9j1nMbquh5qbNw37sKBPENGBrwXo98Ob+tnH3rJntA2rC8qd67TstulCllOIxozKVoPLo6XDpF4AvUA1U97eDO9X9dClVQfBpK9sGWDBWFE+QqJ0DHQeCT19lVXBgR/cn544DwT7p8XCgEVr3kLEycu0HSIyfglcdQ7a5kThZOtpaafMELcmJWOPzeFsz+9NTOWiVJA/uIFs5hXj7XloTE0h5O2ZOa1sHiVwbrRXTyMWTWNNWMrEUFZm9tCUnEiNLKttCJlHOpIMvs6vqDeQx4rk2yGfp6OjAPEeCPAfTRxOLxYjFYkxo2kgsVUXb+Nm05Rxr20c7ZRxMTAwuoshnMc9h+QyWzwXLngv/nRkJD67ca07UkLMk+ViCTCxFOneASe0NHEzWMKm9nnGZ3bTFq8hbgn1lU8jFEjgGGEaeqkwT2ViSjliaTCxN1spI51tJ5Q9g7hiOu5PNQy4fHD1m0JKspibzKo7RGh9Hea6Z1vg44vkO4p4jEyujIruPRL6DA8lJxPMd5CxBe6ycVK6FRL6DGDliHn6Ro8xyxAxa8wncuq/iz5EgRp507gD7kzW0xaqozDZRkd/PjtQs4p4lmWslmWshHcvTYhVUxrMciE+EXIbKjkb2xCczIbcn+L2yOHGyGHka08dRk2kgl3NIpuggSWu+jFhZmnysDM9liOfaSOZbKc8dZFfZVCZkGslakkwsTXnuAFmL0xarpDK3n0ysjCxlJDxDKt9CMt9OJlaGeZ6O8Oc7LrcHYkkS8RjtnqC1fAqXvu6/xL5G/SC1mS0DlgHMnDn8TSwZgV5rvCGeDLtPgOmndZcnCrrwUlVB90k/kuFX4ToE1w5UdZW+azDRioxaUd4o1wDMKFifHpYV3cbMEgTjt7sHuS8A7n6nu9e5e11tbe0whS4iIlEmiFXACWY228zKCAadV/baZiXwwXD5MuBXHtyYsRJYamYpM5sNnAD8PsJYRUSkl8i6mMIxheuAhwiu/Fzu7hvM7GZgtbuvBL4D/DAchN5DkEQIt7uXYEA7C1w7Zq9gEhEZoXQntYjIEWygO6k1WZ+IiBSlBCEiIkUpQYiISFFKECIiUtSYGqQ2s0bglUPcfTKwaxjDKSWdy8gzVs4DdC4j1aGey7HuXvQmsjGVIF4PM1vd30j+aKNzGXnGynmAzmWkiuJc1MUkIiJFKUGIiEhRShDd7ix1AMNI5zLyjJXzAJ3LSDXs56IxCBERKUotCBERKUoJQkREijriE4SZLTazF8xsk5ndUOp4hsrMtpjZH81srZmtDssmmdkjZvZS+L3fh7OVkpktN7OdZra+oKxo7Bb4avg+rTOzhaWLvK9+zuWzZtYQvjdrzeyCgrobw3N5wczeWZqoizOzGWb2mJltNLMNZvbRsHzUvTcDnMuoe2/MLG1mvzezZ8Nz+ZewfLaZPR3G/OPw8QqEj0v4cVj+tJnNGvJB3f2I/SKYhvxPwHEEDwt7Fjip1HEN8Ry2AJN7lf0bcEO4fAPwhVLH2U/sbwUWAutfK3bgAuBBgkcynw48Xer4B3EunwU+UWTbk8LftRQwO/wdjJf6HAriOwZYGC6PA14MYx51780A5zLq3pvw51sVLieBp8Of973A0rD8duCacPl/AreHy0uBHw/1mEd6C2IRsMndN7t7B3APcHGJYxoOFwPfD5e/D7yndKH0z92fIHgOSKH+Yr8Y+IEHngImmtkxhyXQQejnXPpzMXCPu7e7+8vAJoLfxRHB3V9192fC5WbgOYJnwo+692aAc+nPiH1vwp/vgXC186m4DvwVsCIs7/2+dL5fK4BzzV7reb09HekJYhqwtWC9noF/eUYiBx42szXh87kBprj7q+HydmBKaUI7JP3FPlrfq+vCbpflBV19o+Zcwm6JBQSfVkf1e9PrXGAUvjdmFjeztcBO4BGCFk6Tu2fDTQrj7TqXsH4fUDOU4x3pCWIsOMvdFwLnA9ea2VsLKz1oX47Ka5lHc+yhbwLHA/OBV4F/L2k0Q2RmVcBPgY+5+/7CutH23hQ5l1H53rh7zt3nA9MJWjYnRnm8Iz1BNAAzCtanh2Wjhrs3hN93AvcR/NLs6Gzih993li7CIesv9lH3Xrn7jvAPOg98i+6uihF/LmaWJPiHepe7/ywsHpXvTbFzGc3vDYC7NwGPAWcQdOl1Pj66MN6ucwnrJwC7h3KcIz1BrAJOCK8CKCMYyFlZ4pgGzcwqzWxc5zJwHrCe4Bw+GG72QeD+0kR4SPqLfSXwN+EVM6cD+wq6O0akXv3wlxC8NxCcy9LwKpPZwAnA7w93fP0J+6m/Azzn7l8uqBp1701/5zIa3xszqzWzieFyOfAOgjGVx4DLws16vy+d79dlwK/Clt/glXpkvtRfBFdgvEjQl/fpUsczxNiPI7ji4llgQ2f8BP2MvwReAh4FJpU61n7iv5ugeZ8h6Du9ur/YCa7guC18n/4I1JU6/kGcyw/DWNeFf6zHFGz/6fBcXgDOL3X8vc7lLILuo3XA2vDrgtH43gxwLqPuvQHmAX8IY14PfCYsP44giW0CfgKkwvJ0uL4prD9uqMfUVBsiIlLUkd7FJCIi/VCCEBGRopQgRESkKCUIEREpSglCRESKUoIQGQHM7Gwz+69SxyFSSAlCRESKUoIQGQIzuzKck3+tmd0RTp52wMy+Es7R/0szqw23nW9mT4UTwt1X8PyEvzCzR8N5/Z8xs+PDl68ysxVm9ryZ3TXUmTdFhpsShMggmdkcYAlwpgcTpuWA9wOVwGp3Pxn4NXBTuMsPgH9093kEd+12lt8F3ObupwJ/SXAHNgQzjX6M4JkExwFnRnxKIgNKvPYmIhI6FzgNWBV+uC8nmLAuD/w43Ob/Aj8zswnARHf/dVj+feAn4dxZ09z9PgB3bwMIX+/37l4frq8FZgG/jfysRPqhBCEyeAZ8391v7FFo9s+9tjvU+WvaC5Zz6O9TSkxdTCKD90vgMjM7Crqe0Xwswd9R52yafw381t33AXvN7C1h+QeAX3vwVLN6M3tP+BopM6s4nCchMlj6hCIySO6+0cz+ieAJfjGCmVuvBQ4Ci8K6nQTjFBBMtXx7mAA2Ax8Kyz8A3GFmN4evcflhPA2RQdNsriKvk5kdcPeqUschMtzUxSQiIkWpBSEiIkWpBSEiIkUpQYiISFFKECIiUpQShIiIFKUEISIiRf1/jfBJMPmPcwkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(learning_rate = 0.01, activation = 'relu'):\n",
    "  \n",
    "    # Create an Adam optimizer with the given learning rate\n",
    "    opt = Adam(lr=learning_rate)\n",
    "  \n",
    "    # Create your binary classification model  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, \n",
    "                    activation = activation,\n",
    "                    input_shape = (15, ),\n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(128,\n",
    "                    activation = activation, \n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(1, activation = activation))\n",
    "# Compile the model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model,\n",
    "                       verbose=1)\n",
    "\n",
    "params = {'activation': [\"relu\"],\n",
    "          'batch_size': [16, 8], \n",
    "          'epochs': [200, 300, 500],\n",
    "          'learning_rate': [0.01, 0.05, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "35/35 [==============================] - 1s 3ms/step - loss: 44563702.2500 - mse: 44067152.7500 - mae: 2085.4756\n",
      "Epoch 2/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 684295.0990 - mse: 165381.1975 - mae: 347.4688\n",
      "Epoch 3/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 674824.0122 - mse: 172100.0252 - mae: 360.3958\n",
      "Epoch 4/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 656277.7014 - mse: 178089.8772 - mae: 363.7807\n",
      "Epoch 5/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 651521.1597 - mse: 183603.0825 - mae: 373.0169\n",
      "Epoch 6/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 611777.8194 - mse: 180622.8967 - mae: 361.2565\n",
      "Epoch 7/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 598552.1562 - mse: 173341.8229 - mae: 351.6198\n",
      "Epoch 8/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 603223.0990 - mse: 178501.9957 - mae: 364.5510\n",
      "Epoch 9/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 580410.4566 - mse: 174117.8633 - mae: 361.7295\n",
      "Epoch 10/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 1061071.7804 - mse: 672219.6272 - mae: 390.5050\n",
      "Epoch 11/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 589711.7361 - mse: 186886.6558 - mae: 376.1300\n",
      "Epoch 12/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 561307.3958 - mse: 172740.9635 - mae: 356.9658\n",
      "Epoch 13/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 568637.5573 - mse: 185914.0399 - mae: 364.3467\n",
      "Epoch 14/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 543989.6849 - mse: 169419.9353 - mae: 354.8158\n",
      "Epoch 15/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1174197.5200 - mse: 815033.1094 - mae: 403.4569\n",
      "Epoch 16/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 554725.3863 - mse: 183814.0612 - mae: 372.7604\n",
      "Epoch 17/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 542545.5017 - mse: 177988.2283 - mae: 363.7844\n",
      "Epoch 18/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 533941.7439 - mse: 191204.0213 - mae: 362.8594\n",
      "Epoch 19/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 509919.3663 - mse: 170822.1228 - mae: 356.5394\n",
      "Epoch 20/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 514888.3663 - mse: 173499.0499 - mae: 359.7404\n",
      "Epoch 21/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 503962.0938 - mse: 176844.0260 - mae: 357.7932\n",
      "Epoch 22/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 546968.0156 - mse: 220437.5443 - mae: 383.5313\n",
      "Epoch 23/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 484673.3455 - mse: 163406.6189 - mae: 345.2486\n",
      "Epoch 24/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 489580.7639 - mse: 174125.7027 - mae: 356.8244\n",
      "Epoch 25/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 473951.8776 - mse: 165291.5660 - mae: 347.7262\n",
      "Epoch 26/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 467609.3325 - mse: 178276.3303 - mae: 360.0753\n",
      "Epoch 27/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 468074.2752 - mse: 177903.1372 - mae: 364.4157\n",
      "Epoch 28/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 442631.5182 - mse: 164659.1717 - mae: 346.3393\n",
      "Epoch 29/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 466667.7839 - mse: 190744.5239 - mae: 359.7725\n",
      "Epoch 30/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 451327.4696 - mse: 170608.3338 - mae: 355.2439\n",
      "Epoch 31/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 455592.9167 - mse: 188415.9592 - mae: 378.7359\n",
      "Epoch 32/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 457278.8542 - mse: 194313.0638 - mae: 366.6201\n",
      "Epoch 33/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 435667.3542 - mse: 186830.9549 - mae: 367.2019\n",
      "Epoch 34/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 541696.0095 - mse: 295357.3338 - mae: 368.3075\n",
      "Epoch 35/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 435352.7708 - mse: 176707.6120 - mae: 361.6356\n",
      "Epoch 36/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 430727.5043 - mse: 185886.5851 - mae: 372.0622\n",
      "Epoch 37/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 409799.8273 - mse: 174840.6701 - mae: 359.6798\n",
      "Epoch 38/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 419882.9080 - mse: 190917.6515 - mae: 375.7389\n",
      "Epoch 39/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 392680.4740 - mse: 171189.4479 - mae: 346.9933\n",
      "Epoch 40/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 401032.4488 - mse: 174013.8876 - mae: 358.7486\n",
      "Epoch 41/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 395428.1519 - mse: 181394.0143 - mae: 367.5565\n",
      "Epoch 42/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 403831.2465 - mse: 190001.3503 - mae: 373.2582\n",
      "Epoch 43/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 378179.5009 - mse: 174349.6784 - mae: 357.5305\n",
      "Epoch 44/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 373053.0990 - mse: 174215.7361 - mae: 357.0534\n",
      "Epoch 45/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 388324.0911 - mse: 183965.5174 - mae: 370.7119\n",
      "Epoch 46/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 375421.4557 - mse: 179695.8893 - mae: 365.3853\n",
      "Epoch 47/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 357051.2352 - mse: 174489.9627 - mae: 361.9690\n",
      "Epoch 48/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 362440.7587 - mse: 177298.1450 - mae: 366.5400\n",
      "Epoch 49/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 357480.3759 - mse: 176445.1150 - mae: 363.3717\n",
      "Epoch 50/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 351480.4714 - mse: 175079.6892 - mae: 357.3644\n",
      "Epoch 51/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 381844.0087 - mse: 204319.1649 - mae: 375.3908\n",
      "Epoch 52/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 339988.6840 - mse: 166740.5256 - mae: 350.5993\n",
      "Epoch 53/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 340798.5651 - mse: 170734.7166 - mae: 355.3780\n",
      "Epoch 54/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 335672.2066 - mse: 172241.1489 - mae: 352.8338\n",
      "Epoch 55/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 334811.1615 - mse: 171278.9640 - mae: 356.2803\n",
      "Epoch 56/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 314312.3819 - mse: 158425.0725 - mae: 337.1156\n",
      "Epoch 57/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 353161.3411 - mse: 205216.2895 - mae: 384.0057\n",
      "Epoch 58/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 328843.1536 - mse: 179496.7018 - mae: 365.8845\n",
      "Epoch 59/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 309423.0039 - mse: 169132.2860 - mae: 352.5183\n",
      "Epoch 60/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 343923.1745 - mse: 198089.6181 - mae: 377.0927\n",
      "Epoch 61/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 363530.4557 - mse: 219251.1892 - mae: 372.9694\n",
      "Epoch 62/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 307094.4583 - mse: 164853.0990 - mae: 346.8928\n",
      "Epoch 63/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 310625.4688 - mse: 177515.2144 - mae: 365.0576\n",
      "Epoch 64/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 299201.4878 - mse: 169149.8663 - mae: 356.1486\n",
      "Epoch 65/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 301779.1979 - mse: 174294.4501 - mae: 362.9228\n",
      "Epoch 66/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 301470.8863 - mse: 171071.5660 - mae: 354.9323\n",
      "Epoch 67/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 293794.9193 - mse: 171256.8355 - mae: 355.4441\n",
      "Epoch 68/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 291582.3316 - mse: 171476.2405 - mae: 357.0554\n",
      "Epoch 69/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 285599.3116 - mse: 167277.8906 - mae: 352.9534\n",
      "Epoch 70/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 287622.8077 - mse: 173575.5304 - mae: 360.8153\n",
      "Epoch 71/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 283924.0117 - mse: 175920.8898 - mae: 367.7411\n",
      "Epoch 72/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 285083.0560 - mse: 175722.2522 - mae: 360.3535\n",
      "Epoch 73/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 282925.3954 - mse: 175751.8142 - mae: 360.7811\n",
      "Epoch 74/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 282629.3767 - mse: 177438.5317 - mae: 364.1600\n",
      "Epoch 75/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 294876.6319 - mse: 194222.4631 - mae: 370.8991\n",
      "Epoch 76/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 265219.6636 - mse: 167665.3355 - mae: 346.9954\n",
      "Epoch 77/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 268079.8806 - mse: 175105.3611 - mae: 358.5433\n",
      "Epoch 78/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 259358.0451 - mse: 167743.9010 - mae: 351.4303\n",
      "Epoch 79/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 261878.5864 - mse: 172776.2886 - mae: 360.4001\n",
      "Epoch 80/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 257652.4961 - mse: 169463.6033 - mae: 355.0621\n",
      "Epoch 81/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 252593.3112 - mse: 168846.1387 - mae: 354.6527\n",
      "Epoch 82/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 254232.5547 - mse: 173590.8624 - mae: 359.2445\n",
      "Epoch 83/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 260166.1246 - mse: 180008.6558 - mae: 367.9757\n",
      "Epoch 84/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 266443.9362 - mse: 187172.6113 - mae: 363.5787\n",
      "Epoch 85/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 258048.9093 - mse: 179744.2595 - mae: 366.2526\n",
      "Epoch 86/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 255996.8997 - mse: 176689.4002 - mae: 363.4767\n",
      "Epoch 87/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 245225.3754 - mse: 171534.4180 - mae: 357.1464\n",
      "Epoch 88/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 253754.1241 - mse: 180135.3698 - mae: 370.7276\n",
      "Epoch 89/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 257597.7613 - mse: 186555.9805 - mae: 358.3735\n",
      "Epoch 90/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 242619.4727 - mse: 170390.7040 - mae: 354.5616\n",
      "Epoch 91/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 237149.7044 - mse: 169062.3273 - mae: 343.9003\n",
      "Epoch 92/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 235701.0109 - mse: 170026.9887 - mae: 355.5833\n",
      "Epoch 93/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 248363.9557 - mse: 182957.6780 - mae: 369.3437\n",
      "Epoch 94/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 240114.2500 - mse: 176086.6806 - mae: 362.2784\n",
      "Epoch 95/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 236589.0812 - mse: 175298.7869 - mae: 361.7286\n",
      "Epoch 96/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 234923.1146 - mse: 176275.5872 - mae: 362.0193\n",
      "Epoch 97/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 228605.9002 - mse: 171560.3989 - mae: 352.1694\n",
      "Epoch 98/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 225510.4349 - mse: 168705.5738 - mae: 353.9078\n",
      "Epoch 99/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 236994.9401 - mse: 181739.6315 - mae: 367.9485\n",
      "Epoch 100/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 235022.8229 - mse: 180918.8724 - mae: 368.1357\n",
      "Epoch 101/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 232457.2526 - mse: 180544.2309 - mae: 356.6883\n",
      "Epoch 102/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 227119.7643 - mse: 175895.6549 - mae: 360.4515\n",
      "Epoch 103/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 224283.3859 - mse: 174920.8251 - mae: 360.6329\n",
      "Epoch 104/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 219666.7431 - mse: 171762.9753 - mae: 354.7737\n",
      "Epoch 105/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 221919.2292 - mse: 175354.3424 - mae: 361.4244\n",
      "Epoch 106/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 221189.2956 - mse: 175732.1745 - mae: 352.7355\n",
      "Epoch 107/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 214049.0586 - mse: 169328.3043 - mae: 352.3601\n",
      "Epoch 108/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 213807.7027 - mse: 171693.6063 - mae: 355.5396\n",
      "Epoch 109/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 218748.3850 - mse: 177253.5903 - mae: 367.7584\n",
      "Epoch 110/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 222061.3286 - mse: 179910.9631 - mae: 368.4060\n",
      "Epoch 111/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 225076.9444 - mse: 184423.4410 - mae: 374.1161\n",
      "Epoch 112/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 242633.2465 - mse: 204119.7910 - mae: 363.9678\n",
      "Epoch 113/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 222161.5924 - mse: 177574.8932 - mae: 365.3929\n",
      "Epoch 114/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 216332.2860 - mse: 171486.8754 - mae: 358.2037\n",
      "Epoch 115/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 212816.6706 - mse: 169511.4158 - mae: 353.0202\n",
      "Epoch 116/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 212718.3203 - mse: 172851.3069 - mae: 359.1252\n",
      "Epoch 117/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 207922.5608 - mse: 169919.4544 - mae: 352.7100\n",
      "Epoch 118/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 214121.6918 - mse: 175856.5139 - mae: 361.9785\n",
      "Epoch 119/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 197545.2591 - mse: 162475.6293 - mae: 343.0181\n",
      "Epoch 120/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 190900.1814 - mse: 158050.3646 - mae: 339.3201\n",
      "Epoch 121/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 204303.1220 - mse: 171795.8668 - mae: 359.7052\n",
      "Epoch 122/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 208090.2135 - mse: 176196.9162 - mae: 359.7200\n",
      "Epoch 123/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 213305.6923 - mse: 183637.0690 - mae: 372.6141\n",
      "Epoch 124/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 201964.2292 - mse: 173671.1228 - mae: 359.3574\n",
      "Epoch 125/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 199269.9757 - mse: 171643.6124 - mae: 357.0573\n",
      "Epoch 126/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 205386.8299 - mse: 179976.4991 - mae: 367.3622\n",
      "Epoch 127/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 207937.1576 - mse: 181367.3950 - mae: 369.1472\n",
      "Epoch 128/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 197429.7912 - mse: 172810.3264 - mae: 359.0432\n",
      "Epoch 129/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 188741.3733 - mse: 165030.1254 - mae: 345.6318\n",
      "Epoch 130/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 198269.5608 - mse: 175569.3906 - mae: 360.0937\n",
      "Epoch 131/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 198149.8490 - mse: 176238.4089 - mae: 360.9699\n",
      "Epoch 132/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 200242.9883 - mse: 178857.6463 - mae: 368.7709\n",
      "Epoch 133/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 181464.5582 - mse: 161572.2704 - mae: 341.6005\n",
      "Epoch 134/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 190606.3329 - mse: 171714.8498 - mae: 357.7479\n",
      "Epoch 135/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 186199.1146 - mse: 168145.4601 - mae: 349.8414\n",
      "Epoch 136/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 188587.0907 - mse: 170969.1341 - mae: 356.3827\n",
      "Epoch 137/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 186229.3108 - mse: 168914.3320 - mae: 351.7636\n",
      "Epoch 138/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 189806.0330 - mse: 173240.7170 - mae: 352.2152\n",
      "Epoch 139/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 183172.3003 - mse: 166986.8572 - mae: 349.3031\n",
      "Epoch 140/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 181898.5204 - mse: 166225.4106 - mae: 351.0787\n",
      "Epoch 141/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 186239.3385 - mse: 170008.4987 - mae: 350.0047\n",
      "Epoch 142/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 174721.9796 - mse: 159490.9355 - mae: 339.7718\n",
      "Epoch 143/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 174093.7118 - mse: 159176.8420 - mae: 336.2273\n",
      "Epoch 144/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 170220.0126 - mse: 154811.7912 - mae: 333.5630\n",
      "Epoch 145/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 170340.8238 - mse: 156410.5065 - mae: 335.4131\n",
      "Epoch 146/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 141571.2724 - mse: 127916.5836 - mae: 294.5006\n",
      "Epoch 147/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 168170.4371 - mse: 155463.1489 - mae: 332.9658\n",
      "Epoch 148/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 158042.9800 - mse: 145772.9883 - mae: 317.0658\n",
      "Epoch 149/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 155987.0929 - mse: 143806.6046 - mae: 311.8506\n",
      "Epoch 150/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 133526.8891 - mse: 120976.7054 - mae: 285.5819\n",
      "Epoch 151/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 147349.0662 - mse: 136338.0189 - mae: 300.4176\n",
      "Epoch 152/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 146756.1380 - mse: 136025.3904 - mae: 304.6635\n",
      "Epoch 153/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 145475.0855 - mse: 135341.7010 - mae: 301.7878\n",
      "Epoch 154/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 145005.9696 - mse: 133931.7734 - mae: 295.3386\n",
      "Epoch 155/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 137386.1799 - mse: 126576.5616 - mae: 288.8213\n",
      "Epoch 156/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 131880.3307 - mse: 121513.6413 - mae: 284.7187\n",
      "Epoch 157/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 115087.0295 - mse: 104739.4881 - mae: 261.9173\n",
      "Epoch 158/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 114302.1048 - mse: 104457.1322 - mae: 263.6121\n",
      "Epoch 159/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 121441.8941 - mse: 111540.8500 - mae: 268.1763\n",
      "Epoch 160/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 123209.3249 - mse: 113882.3225 - mae: 268.4888\n",
      "Epoch 161/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 107531.7431 - mse: 98705.8709 - mae: 253.2841\n",
      "Epoch 162/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 116967.5540 - mse: 108367.7418 - mae: 263.5587\n",
      "Epoch 163/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 106163.2185 - mse: 98080.0332 - mae: 255.0636\n",
      "Epoch 164/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 100169.4978 - mse: 92565.0690 - mae: 244.2256\n",
      "Epoch 165/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 97744.6610 - mse: 89859.6831 - mae: 241.0354\n",
      "Epoch 166/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 85174.0001 - mse: 77732.8771 - mae: 219.9924\n",
      "Epoch 167/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 107436.6257 - mse: 100256.8711 - mae: 251.5131\n",
      "Epoch 168/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 105104.9039 - mse: 98117.0907 - mae: 250.9727\n",
      "Epoch 169/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 82649.5123 - mse: 76562.0518 - mae: 221.0611\n",
      "Epoch 170/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 88426.1233 - mse: 82713.6016 - mae: 230.7050\n",
      "Epoch 171/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 86752.0297 - mse: 81319.9594 - mae: 232.4551\n",
      "Epoch 172/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91777.9579 - mse: 86412.3062 - mae: 238.6559\n",
      "Epoch 173/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 83799.7880 - mse: 78546.4915 - mae: 229.5351\n",
      "Epoch 174/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 88454.7344 - mse: 83684.5256 - mae: 236.1934\n",
      "Epoch 175/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 82470.4310 - mse: 77341.3647 - mae: 221.3938\n",
      "Epoch 176/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 86045.5315 - mse: 81174.5464 - mae: 228.8640\n",
      "Epoch 177/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 85909.5419 - mse: 81312.3924 - mae: 234.3938\n",
      "Epoch 178/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 76105.5782 - mse: 71232.7690 - mae: 212.5671\n",
      "Epoch 179/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 79384.3075 - mse: 74395.6276 - mae: 219.8270\n",
      "Epoch 180/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 85567.7805 - mse: 80324.8572 - mae: 229.2107\n",
      "Epoch 181/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 84207.8129 - mse: 79287.5651 - mae: 225.1939\n",
      "Epoch 182/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 79739.7641 - mse: 75289.8741 - mae: 222.4306\n",
      "Epoch 183/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 86865.0354 - mse: 82691.3090 - mae: 232.3064\n",
      "Epoch 184/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73729.3003 - mse: 69715.3870 - mae: 214.1385\n",
      "Epoch 185/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 75455.6554 - mse: 71688.0473 - mae: 212.7609\n",
      "Epoch 186/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 87057.0343 - mse: 82909.1775 - mae: 236.7887\n",
      "Epoch 187/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 76330.5382 - mse: 71846.4161 - mae: 214.3187\n",
      "Epoch 188/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 71769.5034 - mse: 67496.5273 - mae: 212.1011\n",
      "Epoch 189/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73990.7483 - mse: 70060.9614 - mae: 213.0567\n",
      "Epoch 190/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71433.1587 - mse: 67690.7944 - mae: 209.4935\n",
      "Epoch 191/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 75145.5833 - mse: 71486.2638 - mae: 215.6285\n",
      "Epoch 192/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77530.3912 - mse: 74031.1461 - mae: 222.5683\n",
      "Epoch 193/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 79547.0764 - mse: 76203.2023 - mae: 225.7454\n",
      "Epoch 194/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 78534.2070 - mse: 75077.0725 - mae: 219.5195\n",
      "Epoch 195/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 75206.4156 - mse: 71988.8027 - mae: 215.8617\n",
      "Epoch 196/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 70586.8537 - mse: 67696.9053 - mae: 205.5233\n",
      "Epoch 197/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 69666.4431 - mse: 66681.3227 - mae: 205.6294\n",
      "Epoch 198/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63472.3734 - mse: 60665.4684 - mae: 201.9191\n",
      "Epoch 199/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 69442.1000 - mse: 66733.1378 - mae: 208.0763\n",
      "Epoch 200/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61810.1595 - mse: 58888.2461 - mae: 198.9542\n",
      "Epoch 201/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 68270.6202 - mse: 65685.4415 - mae: 207.3591\n",
      "Epoch 202/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67651.6531 - mse: 64937.1093 - mae: 203.6258\n",
      "Epoch 203/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71845.0197 - mse: 69286.6545 - mae: 215.6764\n",
      "Epoch 204/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 68823.7311 - mse: 66284.1086 - mae: 207.7043\n",
      "Epoch 205/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 64899.5696 - mse: 62314.8478 - mae: 198.0073\n",
      "Epoch 206/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 78422.0482 - mse: 75872.6076 - mae: 228.0531\n",
      "Epoch 207/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 67620.0010 - mse: 64935.3822 - mae: 209.9933\n",
      "Epoch 208/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73967.5645 - mse: 71261.6924 - mae: 217.1056\n",
      "Epoch 209/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 74920.2865 - mse: 72229.1415 - mae: 222.3767\n",
      "Epoch 210/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61930.2580 - mse: 59319.7382 - mae: 201.3448\n",
      "Epoch 211/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73217.8105 - mse: 70636.5323 - mae: 217.6662\n",
      "Epoch 212/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71178.0915 - mse: 68799.8770 - mae: 209.3197\n",
      "Epoch 213/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 70204.5030 - mse: 67670.6233 - mae: 207.5423\n",
      "Epoch 214/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60868.6173 - mse: 58491.8057 - mae: 192.4820\n",
      "Epoch 215/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 64241.3532 - mse: 61886.8206 - mae: 197.6896\n",
      "Epoch 216/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 62530.8439 - mse: 59951.3099 - mae: 196.2946\n",
      "Epoch 217/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61572.1645 - mse: 59103.8086 - mae: 197.6568\n",
      "Epoch 218/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67538.3184 - mse: 65255.1360 - mae: 207.6601\n",
      "Epoch 219/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61186.9225 - mse: 59031.6178 - mae: 195.7292\n",
      "Epoch 220/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63679.9297 - mse: 61481.1015 - mae: 200.0280\n",
      "Epoch 221/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66048.4571 - mse: 63747.8487 - mae: 199.5234\n",
      "Epoch 222/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67630.0862 - mse: 65635.1051 - mae: 210.6600\n",
      "Epoch 223/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62762.7529 - mse: 60140.2234 - mae: 200.3807\n",
      "Epoch 224/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61411.0264 - mse: 59185.2232 - mae: 198.8396\n",
      "Epoch 225/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67348.9070 - mse: 65085.7123 - mae: 207.7835\n",
      "Epoch 226/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 74356.2337 - mse: 72250.6700 - mae: 215.4483\n",
      "Epoch 227/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66896.5978 - mse: 64695.7204 - mae: 205.7432\n",
      "Epoch 228/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60660.9484 - mse: 58657.8899 - mae: 193.2526\n",
      "Epoch 229/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 66544.8098 - mse: 64612.8585 - mae: 210.1987\n",
      "Epoch 230/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 70930.6509 - mse: 69107.7803 - mae: 214.5912\n",
      "Epoch 231/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 80927.7582 - mse: 79078.9112 - mae: 225.5200\n",
      "Epoch 232/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 59474.5888 - mse: 57602.2023 - mae: 197.7635\n",
      "Epoch 233/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61576.2538 - mse: 59654.4541 - mae: 198.7874\n",
      "Epoch 234/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60064.1076 - mse: 58204.7363 - mae: 191.1630\n",
      "Epoch 235/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 64762.3902 - mse: 62771.7051 - mae: 205.0340\n",
      "Epoch 236/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65381.6823 - mse: 63507.8864 - mae: 204.2660\n",
      "Epoch 237/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67818.2035 - mse: 65926.1682 - mae: 205.7997\n",
      "Epoch 238/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 63107.5561 - mse: 61159.0151 - mae: 204.3267\n",
      "Epoch 239/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65565.9670 - mse: 63873.4036 - mae: 210.8037\n",
      "Epoch 240/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62361.3850 - mse: 60472.1838 - mae: 197.6287\n",
      "Epoch 241/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66913.2604 - mse: 65070.3023 - mae: 205.2940\n",
      "Epoch 242/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60202.6230 - mse: 58480.9991 - mae: 196.4269\n",
      "Epoch 243/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67117.2871 - mse: 65268.2831 - mae: 209.8175\n",
      "Epoch 244/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66700.8311 - mse: 64896.7769 - mae: 201.7790\n",
      "Epoch 245/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60364.9561 - mse: 58861.9456 - mae: 194.9949\n",
      "Epoch 246/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60395.5369 - mse: 58813.2848 - mae: 195.1909\n",
      "Epoch 247/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58932.1438 - mse: 57380.8559 - mae: 197.7282\n",
      "Epoch 248/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63013.6431 - mse: 61400.2942 - mae: 201.7440\n",
      "Epoch 249/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73299.3314 - mse: 71759.0204 - mae: 217.4496\n",
      "Epoch 250/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 70550.3479 - mse: 68988.5265 - mae: 213.7223\n",
      "Epoch 251/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 63019.4966 - mse: 61501.8296 - mae: 200.9886\n",
      "Epoch 252/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62734.9068 - mse: 61024.8052 - mae: 202.4807\n",
      "Epoch 253/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61288.2618 - mse: 59666.9568 - mae: 198.5782\n",
      "Epoch 254/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63818.4948 - mse: 61943.6325 - mae: 201.5503\n",
      "Epoch 255/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 63562.1024 - mse: 61587.3353 - mae: 202.8669\n",
      "Epoch 256/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 68079.9821 - mse: 66084.4229 - mae: 208.8087\n",
      "Epoch 257/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 64983.7030 - mse: 62979.9818 - mae: 204.7250\n",
      "Epoch 258/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 53028.9200 - mse: 51103.9141 - mae: 180.8481\n",
      "Epoch 259/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 67016.3624 - mse: 65348.6243 - mae: 210.0210\n",
      "Epoch 260/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60193.1143 - mse: 58461.8216 - mae: 197.4502\n",
      "Epoch 261/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60251.9792 - mse: 58530.3219 - mae: 197.8524\n",
      "Epoch 262/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 63312.9620 - mse: 61508.6653 - mae: 197.4111\n",
      "Epoch 263/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56973.1598 - mse: 55347.8410 - mae: 189.8757\n",
      "Epoch 264/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 63766.1520 - mse: 62249.8260 - mae: 205.0634\n",
      "Epoch 265/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 63006.8495 - mse: 61547.6679 - mae: 195.3539\n",
      "Epoch 266/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59916.5308 - mse: 58545.1645 - mae: 196.0097\n",
      "Epoch 267/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 57236.3362 - mse: 55700.1513 - mae: 193.9293\n",
      "Epoch 268/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60223.0340 - mse: 58767.4061 - mae: 197.2923\n",
      "Epoch 269/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56944.7393 - mse: 55545.2620 - mae: 190.9139\n",
      "Epoch 270/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71225.5991 - mse: 69963.8250 - mae: 217.9262\n",
      "Epoch 271/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56599.7783 - mse: 55281.9498 - mae: 194.0801\n",
      "Epoch 272/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58585.0345 - mse: 57093.8421 - mae: 192.5825\n",
      "Epoch 273/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66594.6797 - mse: 65175.9837 - mae: 208.3398\n",
      "Epoch 274/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 57050.9202 - mse: 55691.5282 - mae: 193.4282\n",
      "Epoch 275/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58698.3382 - mse: 57360.1391 - mae: 197.1545\n",
      "Epoch 276/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61273.8497 - mse: 60022.8974 - mae: 201.8172\n",
      "Epoch 277/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 52893.8760 - mse: 51582.0090 - mae: 181.0272\n",
      "Epoch 278/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 57849.3895 - mse: 56632.2637 - mae: 192.9168\n",
      "Epoch 279/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59523.8224 - mse: 58125.6414 - mae: 196.1365\n",
      "Epoch 280/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60362.0315 - mse: 58990.4301 - mae: 192.8139\n",
      "Epoch 281/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 59092.0738 - mse: 57769.1497 - mae: 196.6450\n",
      "Epoch 282/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56634.5130 - mse: 55377.7027 - mae: 189.7593\n",
      "Epoch 283/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58682.1327 - mse: 57388.3480 - mae: 201.9827\n",
      "Epoch 284/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 57887.5913 - mse: 56783.0556 - mae: 193.0074\n",
      "Epoch 285/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 55115.5850 - mse: 53903.0905 - mae: 182.8483\n",
      "Epoch 286/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62674.6197 - mse: 61362.7437 - mae: 202.0273\n",
      "Epoch 287/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 55223.9747 - mse: 53937.7461 - mae: 186.7107\n",
      "Epoch 288/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61076.3732 - mse: 59883.6986 - mae: 197.4604\n",
      "Epoch 289/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60042.4744 - mse: 58735.5853 - mae: 190.9751\n",
      "Epoch 290/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59141.9829 - mse: 58005.0295 - mae: 197.7043\n",
      "Epoch 291/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60689.4650 - mse: 59507.9670 - mae: 199.7218\n",
      "Epoch 292/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 64317.0997 - mse: 63083.4476 - mae: 207.2115\n",
      "Epoch 293/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 50746.3617 - mse: 49729.5292 - mae: 178.9195\n",
      "Epoch 294/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 65189.8487 - mse: 64081.8901 - mae: 207.3019\n",
      "Epoch 295/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 54794.1936 - mse: 53488.5091 - mae: 186.4252\n",
      "Epoch 296/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 64584.3553 - mse: 63467.7287 - mae: 205.5488\n",
      "Epoch 297/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56498.2094 - mse: 55514.8925 - mae: 193.3849\n",
      "Epoch 298/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63549.0499 - mse: 62557.6054 - mae: 202.4054\n",
      "Epoch 299/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 53852.7037 - mse: 52652.0242 - mae: 186.2415\n",
      "Epoch 300/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 49523.0786 - mse: 48378.9047 - mae: 178.4879\n",
      "CPU times: user 41.2 s, sys: 6.03 s, total: 47.2 s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(model,\n",
    "                                   param_distributions=params, n_jobs=-1)\n",
    "\n",
    "random_search_results = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05, 'epochs': 300, 'batch_size': 16, 'activation': 'relu'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tuned = random_search_results.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (1.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.5.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error test: 148.27969196827516\n",
      "Mean Absolute Error train: 1.6522873959679534\n"
     ]
    }
   ],
   "source": [
    "xgbr = XGBRegressor()\n",
    "\n",
    "xgbr.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_metric=\"mae\",  \n",
    "    verbose=True)\n",
    "\n",
    "predictions = xgbr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error test: \" + str(mean_absolute_error(predictions, y_test)))\n",
    "print(\"Mean Absolute Error train: \" + str(mean_absolute_error(xgbr.predict(X_train), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  190.814337\n",
      "Mean MAE: 148.775 (10.384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8376284.286574147, tolerance: 2469.0218836503623\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "lasso_reg = Lasso(alpha=0.0)\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "pred = lasso_reg.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "scores = cross_val_score(lasso_reg, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "\n",
    "rmse = np.sqrt(MSE(y_test, pred))\n",
    "print(\"RMSE : % f\" %(rmse))\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "             estimator=Lasso(), n_jobs=-1,\n",
       "             param_grid={'alpha': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99])},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search hyperparameters for lasso regression\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# define model\n",
    "model = Lasso()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = arange(0, 1, 0.01)\n",
    "# define search\n",
    "lasso_tuned = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "lasso_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 153.777 (16.460)\n"
     ]
    }
   ],
   "source": [
    "# evaluate an ridge regression model on the dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define model\n",
    "ridge_reg = Ridge(alpha=0.00)\n",
    "# define model evaluation method\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(ridge_reg, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.12579e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.34075e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99]),\n",
       "        cv=RepeatedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "        scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import arange\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ridge_tune2 = RidgeCV(alphas=arange(0, 1, 0.01), cv=cv, scoring='neg_mean_absolute_error')\n",
    "# fit model\n",
    "ridge_tune2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = svm.SVR()\n",
    "sv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance with twitter BERT and news sentiments\n",
      "Epoch 1/300\n",
      "35/35 [==============================] - 1s 2ms/step - loss: 257940782.4444 - mse: 257522678.2222 - mae: 7371.7287\n",
      "Epoch 2/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 596842.8663 - mse: 186235.8129 - mae: 373.3532\n",
      "Epoch 3/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 594561.4271 - mse: 198972.5638 - mae: 363.9986\n",
      "Epoch 4/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 613408.8924 - mse: 229783.7522 - mae: 379.2255\n",
      "Epoch 5/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 559031.0451 - mse: 176188.8021 - mae: 365.1141\n",
      "Epoch 6/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 564678.7257 - mse: 185677.4718 - mae: 356.5214\n",
      "Epoch 7/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 517208.6962 - mse: 165359.0790 - mae: 346.3767\n",
      "Epoch 8/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 682548.3533 - mse: 322738.1046 - mae: 388.5444\n",
      "Epoch 9/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 519727.9497 - mse: 171519.5907 - mae: 360.7794\n",
      "Epoch 10/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 512761.9002 - mse: 166113.9759 - mae: 352.5849\n",
      "Epoch 11/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 551128.4653 - mse: 192458.8047 - mae: 367.4902\n",
      "Epoch 12/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 547289.4653 - mse: 189635.5560 - mae: 371.1897\n",
      "Epoch 13/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 574415.7431 - mse: 224562.1984 - mae: 360.3240\n",
      "Epoch 14/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 535956.0443 - mse: 174885.5799 - mae: 361.6329\n",
      "Epoch 15/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 557224.9931 - mse: 186872.7075 - mae: 358.6149\n",
      "Epoch 16/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 536271.4227 - mse: 176689.1749 - mae: 363.0894\n",
      "Epoch 17/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 544647.8438 - mse: 196202.0122 - mae: 348.4791\n",
      "Epoch 18/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 523481.8984 - mse: 175782.0352 - mae: 361.0636\n",
      "Epoch 19/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 517479.0608 - mse: 170327.0816 - mae: 355.1376\n",
      "Epoch 20/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 667547.9010 - mse: 322301.4905 - mae: 378.2495\n",
      "Epoch 21/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 521703.6788 - mse: 181960.6714 - mae: 371.0949\n",
      "Epoch 22/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 506972.6198 - mse: 177383.5473 - mae: 363.2639\n",
      "Epoch 23/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 489732.0634 - mse: 166720.6476 - mae: 351.2793\n",
      "Epoch 24/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 487573.6936 - mse: 176316.7370 - mae: 363.1787\n",
      "Epoch 25/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 505649.4748 - mse: 178186.0660 - mae: 364.1353\n",
      "Epoch 26/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 508143.9939 - mse: 176293.9340 - mae: 359.5726\n",
      "Epoch 27/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 490836.6667 - mse: 186819.1489 - mae: 355.5575\n",
      "Epoch 28/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 491240.9505 - mse: 174859.9805 - mae: 362.8450\n",
      "Epoch 29/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 485527.4783 - mse: 175983.3316 - mae: 359.4658\n",
      "Epoch 30/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 483629.7431 - mse: 168299.7083 - mae: 352.5754\n",
      "Epoch 31/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 474929.3655 - mse: 166114.6220 - mae: 348.5636\n",
      "Epoch 32/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 479311.2153 - mse: 170963.8594 - mae: 356.8000\n",
      "Epoch 33/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 509765.7318 - mse: 202989.7812 - mae: 368.9597\n",
      "Epoch 34/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 487925.9792 - mse: 181781.8659 - mae: 367.9190\n",
      "Epoch 35/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 506475.4306 - mse: 205832.4271 - mae: 369.5685\n",
      "Epoch 36/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 481078.0365 - mse: 177542.4635 - mae: 362.1049\n",
      "Epoch 37/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 487974.8750 - mse: 180980.8889 - mae: 369.7859\n",
      "Epoch 38/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 468828.2170 - mse: 174161.5078 - mae: 361.6918\n",
      "Epoch 39/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 461781.1840 - mse: 173345.3407 - mae: 359.5386\n",
      "Epoch 40/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 448477.3238 - mse: 177541.6884 - mae: 355.4426\n",
      "Epoch 41/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 499881.6293 - mse: 239256.7478 - mae: 372.4993\n",
      "Epoch 42/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 455538.4089 - mse: 216807.3481 - mae: 364.4976\n",
      "Epoch 43/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 424473.4245 - mse: 190024.2387 - mae: 366.2906\n",
      "Epoch 44/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 426323.3611 - mse: 176807.2426 - mae: 361.6756\n",
      "Epoch 45/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 431324.6589 - mse: 184543.4844 - mae: 372.5258\n",
      "Epoch 46/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 434082.4566 - mse: 192258.2426 - mae: 362.2227\n",
      "Epoch 47/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 405992.1493 - mse: 168635.9258 - mae: 350.5670\n",
      "Epoch 48/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 411771.5990 - mse: 178307.3980 - mae: 366.3680\n",
      "Epoch 49/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 410399.2552 - mse: 179464.4067 - mae: 365.4834\n",
      "Epoch 50/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 400598.0200 - mse: 177761.4167 - mae: 368.6509\n",
      "Epoch 51/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 472221.3507 - mse: 243810.0256 - mae: 379.9174\n",
      "Epoch 52/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 402306.7925 - mse: 174059.4349 - mae: 358.8205\n",
      "Epoch 53/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 413922.2743 - mse: 199558.5208 - mae: 354.7399\n",
      "Epoch 54/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 372180.9991 - mse: 162365.7849 - mae: 340.8020\n",
      "Epoch 55/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 437607.2283 - mse: 222742.0933 - mae: 374.3844\n",
      "Epoch 56/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 392905.9688 - mse: 183021.2565 - mae: 370.0408\n",
      "Epoch 57/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 390688.1979 - mse: 184923.8572 - mae: 354.9030\n",
      "Epoch 58/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 414683.9314 - mse: 215877.0061 - mae: 353.8627\n",
      "Epoch 59/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 418823.6311 - mse: 200824.6385 - mae: 346.8395\n",
      "Epoch 60/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 388280.1944 - mse: 171027.9444 - mae: 351.5829\n",
      "Epoch 61/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 415703.9844 - mse: 201707.1558 - mae: 375.2103\n",
      "Epoch 62/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 402378.9479 - mse: 172330.6862 - mae: 357.2308\n",
      "Epoch 63/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 393254.0885 - mse: 168084.5091 - mae: 349.8448\n",
      "Epoch 64/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 402213.9436 - mse: 177918.3837 - mae: 355.4503\n",
      "Epoch 65/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 388207.1875 - mse: 176813.5951 - mae: 363.2470\n",
      "Epoch 66/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 381219.2014 - mse: 168003.9484 - mae: 350.3154\n",
      "Epoch 67/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 392265.4731 - mse: 181654.4388 - mae: 367.3239\n",
      "Epoch 68/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 373478.0590 - mse: 169445.3429 - mae: 353.3171\n",
      "Epoch 69/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 367777.3203 - mse: 170645.0156 - mae: 355.8980\n",
      "Epoch 70/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 383579.3064 - mse: 181133.4809 - mae: 369.9309\n",
      "Epoch 71/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 470428.6484 - mse: 263090.3316 - mae: 383.5356\n",
      "Epoch 72/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 374933.1675 - mse: 175934.3411 - mae: 363.2049\n",
      "Epoch 73/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 388229.5990 - mse: 189120.5990 - mae: 378.3573\n",
      "Epoch 74/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 382610.9714 - mse: 192754.5195 - mae: 365.9793\n",
      "Epoch 75/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 376284.8672 - mse: 185463.3602 - mae: 368.0512\n",
      "Epoch 76/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 356557.7049 - mse: 171529.1354 - mae: 348.6015\n",
      "Epoch 77/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 352455.1988 - mse: 169449.9336 - mae: 358.5634\n",
      "Epoch 78/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 372562.9505 - mse: 196421.4670 - mae: 375.3833\n",
      "Epoch 79/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 344925.1736 - mse: 171876.7535 - mae: 358.5189\n",
      "Epoch 80/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 339787.4123 - mse: 171350.5052 - mae: 356.1792\n",
      "Epoch 81/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 335138.7839 - mse: 165329.2765 - mae: 345.9097\n",
      "Epoch 82/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 363955.1042 - mse: 191836.8468 - mae: 374.8551\n",
      "Epoch 83/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 363308.8637 - mse: 198573.9705 - mae: 374.6566\n",
      "Epoch 84/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 352079.9418 - mse: 174493.6905 - mae: 358.5485\n",
      "Epoch 85/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 347743.6719 - mse: 179541.7270 - mae: 365.5630\n",
      "Epoch 86/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 412465.4080 - mse: 254896.2296 - mae: 366.4840\n",
      "Epoch 87/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 328535.2066 - mse: 166222.7053 - mae: 349.4637\n",
      "Epoch 88/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 335925.9939 - mse: 174175.1736 - mae: 359.4739\n",
      "Epoch 89/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 325915.7917 - mse: 173821.8845 - mae: 360.0919\n",
      "Epoch 90/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 315907.7023 - mse: 165814.5660 - mae: 352.1047\n",
      "Epoch 91/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 324766.0000 - mse: 172620.6810 - mae: 355.4393\n",
      "Epoch 92/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 334422.9262 - mse: 184695.5122 - mae: 360.8171\n",
      "Epoch 93/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 317400.4366 - mse: 167832.3095 - mae: 354.4571\n",
      "Epoch 94/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 334656.8602 - mse: 185842.1667 - mae: 370.1751\n",
      "Epoch 95/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 320102.6823 - mse: 177457.0499 - mae: 367.3231\n",
      "Epoch 96/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 323994.5113 - mse: 176313.2365 - mae: 363.0040\n",
      "Epoch 97/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 316903.2387 - mse: 176879.0352 - mae: 366.8869\n",
      "Epoch 98/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 314669.9497 - mse: 173428.5282 - mae: 354.5256\n",
      "Epoch 99/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 306830.8021 - mse: 172750.7075 - mae: 356.8479\n",
      "Epoch 100/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 477423.3733 - mse: 340011.3815 - mae: 384.6364\n",
      "Epoch 101/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 322019.0191 - mse: 184910.0200 - mae: 370.1278\n",
      "Epoch 102/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 308267.2639 - mse: 172778.1480 - mae: 352.5814\n",
      "Epoch 103/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 324798.0191 - mse: 188081.8255 - mae: 368.0655\n",
      "Epoch 104/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 309077.8924 - mse: 176746.2873 - mae: 365.7599\n",
      "Epoch 105/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 309879.3351 - mse: 177409.9813 - mae: 355.7332\n",
      "Epoch 106/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 311365.6580 - mse: 177962.1780 - mae: 363.2288\n",
      "Epoch 107/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 301355.7031 - mse: 177274.6584 - mae: 363.9621\n",
      "Epoch 108/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 291646.5955 - mse: 166732.4492 - mae: 342.4630\n",
      "Epoch 109/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 290505.5477 - mse: 170254.1654 - mae: 355.6864\n",
      "Epoch 110/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 281473.2296 - mse: 158526.2144 - mae: 341.2104\n",
      "Epoch 111/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 287707.0408 - mse: 169118.0868 - mae: 352.9497\n",
      "Epoch 112/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 289725.7830 - mse: 173609.4974 - mae: 357.3223\n",
      "Epoch 113/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 281991.5799 - mse: 168619.3277 - mae: 351.0761\n",
      "Epoch 114/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 290537.0373 - mse: 178704.0365 - mae: 364.3475\n",
      "Epoch 115/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 276803.2062 - mse: 166231.3746 - mae: 348.7736\n",
      "Epoch 116/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 274803.0820 - mse: 172287.5425 - mae: 352.6115\n",
      "Epoch 117/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 288376.7257 - mse: 186082.7530 - mae: 371.1058\n",
      "Epoch 118/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 268801.7240 - mse: 167860.9622 - mae: 349.5674\n",
      "Epoch 119/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 269951.9887 - mse: 167356.4275 - mae: 349.3077\n",
      "Epoch 120/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 278782.4653 - mse: 180189.1701 - mae: 369.9137\n",
      "Epoch 121/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 267979.1393 - mse: 170936.1207 - mae: 355.1012\n",
      "Epoch 122/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 265335.6159 - mse: 170590.7096 - mae: 343.9409\n",
      "Epoch 123/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 269379.1541 - mse: 174294.8802 - mae: 359.9988\n",
      "Epoch 124/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 261283.6328 - mse: 171351.1398 - mae: 359.0755\n",
      "Epoch 125/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 270584.7413 - mse: 178714.0456 - mae: 358.5322\n",
      "Epoch 126/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 251307.8546 - mse: 166532.3954 - mae: 351.3623\n",
      "Epoch 127/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 252537.7904 - mse: 171030.4249 - mae: 355.3455\n",
      "Epoch 128/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 269566.9158 - mse: 184468.7309 - mae: 370.4038\n",
      "Epoch 129/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 263120.7140 - mse: 179852.6740 - mae: 362.7342\n",
      "Epoch 130/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 259625.1194 - mse: 180083.2083 - mae: 360.8943\n",
      "Epoch 131/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 254516.9510 - mse: 176191.7622 - mae: 354.8208\n",
      "Epoch 132/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 250274.0799 - mse: 173507.4499 - mae: 348.3486\n",
      "Epoch 133/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 249297.8533 - mse: 172748.7483 - mae: 356.8681\n",
      "Epoch 134/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 254152.4214 - mse: 179744.0760 - mae: 368.5152\n",
      "Epoch 135/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 246936.8542 - mse: 174039.0265 - mae: 360.0911\n",
      "Epoch 136/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 255665.9132 - mse: 182765.0113 - mae: 363.7059\n",
      "Epoch 137/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 243905.7956 - mse: 174910.4948 - mae: 355.6155\n",
      "Epoch 138/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 239971.2739 - mse: 171080.8359 - mae: 356.3685\n",
      "Epoch 139/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 248227.6866 - mse: 181988.3976 - mae: 370.5173\n",
      "Epoch 140/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 331492.4948 - mse: 264534.0820 - mae: 376.3689\n",
      "Epoch 141/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 240299.3060 - mse: 174907.8424 - mae: 360.5669\n",
      "Epoch 142/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 233856.0660 - mse: 171078.1806 - mae: 356.9637\n",
      "Epoch 143/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 229208.2760 - mse: 167213.1098 - mae: 347.2244\n",
      "Epoch 144/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 230132.5056 - mse: 169345.3724 - mae: 355.2134\n",
      "Epoch 145/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 243269.2370 - mse: 183003.2782 - mae: 368.3307\n",
      "Epoch 146/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 216281.7439 - mse: 160512.3477 - mae: 341.8361\n",
      "Epoch 147/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 229380.5425 - mse: 170240.1593 - mae: 355.6238\n",
      "Epoch 148/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 222389.2834 - mse: 167076.7947 - mae: 351.2059\n",
      "Epoch 149/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 221279.0612 - mse: 166101.7465 - mae: 348.7505\n",
      "Epoch 150/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 229554.6884 - mse: 175720.0152 - mae: 359.3128\n",
      "Epoch 151/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 215648.0026 - mse: 164524.1276 - mae: 346.4899\n",
      "Epoch 152/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 222381.8767 - mse: 171287.6944 - mae: 352.8738\n",
      "Epoch 153/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 230654.2860 - mse: 181347.2726 - mae: 370.8329\n",
      "Epoch 154/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 217530.7196 - mse: 168579.4874 - mae: 354.0709\n",
      "Epoch 155/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 225523.9970 - mse: 178542.9253 - mae: 364.7691\n",
      "Epoch 156/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 228004.9670 - mse: 181314.0378 - mae: 368.1946\n",
      "Epoch 157/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 215343.8238 - mse: 171180.0951 - mae: 360.0849\n",
      "Epoch 158/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 215811.0260 - mse: 172912.6319 - mae: 361.6277\n",
      "Epoch 159/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 213463.1350 - mse: 170120.6202 - mae: 356.4331\n",
      "Epoch 160/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 213259.6554 - mse: 171142.2595 - mae: 353.0475\n",
      "Epoch 161/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 209126.4102 - mse: 167901.1997 - mae: 353.5381\n",
      "Epoch 162/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 206075.9466 - mse: 166249.9462 - mae: 348.2827\n",
      "Epoch 163/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 206868.8624 - mse: 168778.1237 - mae: 352.7398\n",
      "Epoch 164/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 210090.6714 - mse: 172648.2808 - mae: 356.5335\n",
      "Epoch 165/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 206487.5234 - mse: 170042.7465 - mae: 351.7944\n",
      "Epoch 166/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 201737.3529 - mse: 167057.6068 - mae: 350.6849\n",
      "Epoch 167/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 203376.0200 - mse: 170188.7986 - mae: 356.4358\n",
      "Epoch 168/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 200302.6758 - mse: 167678.8390 - mae: 348.4672\n",
      "Epoch 169/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 204600.7873 - mse: 172066.0482 - mae: 356.0000\n",
      "Epoch 170/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 215262.1610 - mse: 182654.0451 - mae: 367.9200\n",
      "Epoch 171/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 207508.2161 - mse: 176482.2313 - mae: 360.6200\n",
      "Epoch 172/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 204893.7140 - mse: 174944.5629 - mae: 362.4710\n",
      "Epoch 173/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 193093.7704 - mse: 163584.6098 - mae: 348.8514\n",
      "Epoch 174/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 199029.8602 - mse: 169955.9961 - mae: 352.7935\n",
      "Epoch 175/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 202803.2174 - mse: 175013.3238 - mae: 364.7270\n",
      "Epoch 176/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 201379.7266 - mse: 175059.5308 - mae: 358.4624\n",
      "Epoch 177/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 204928.0169 - mse: 178593.5482 - mae: 370.0843\n",
      "Epoch 178/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 198488.5729 - mse: 172959.6966 - mae: 356.6685\n",
      "Epoch 179/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 198422.1701 - mse: 173233.8290 - mae: 354.7668\n",
      "Epoch 180/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 203173.4023 - mse: 179537.8724 - mae: 370.1781\n",
      "Epoch 181/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 183403.7266 - mse: 160801.0052 - mae: 341.3676\n",
      "Epoch 182/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 184131.3535 - mse: 161340.9667 - mae: 339.5922\n",
      "Epoch 183/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 198913.3442 - mse: 176628.0707 - mae: 358.1791\n",
      "Epoch 184/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 200313.6975 - mse: 178150.4440 - mae: 365.8800\n",
      "Epoch 185/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 198246.9544 - mse: 176724.4405 - mae: 362.8845\n",
      "Epoch 186/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 186655.1102 - mse: 167683.7893 - mae: 353.1127\n",
      "Epoch 187/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 189762.1228 - mse: 170661.6298 - mae: 355.5256\n",
      "Epoch 188/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 187025.7543 - mse: 168004.0543 - mae: 352.1021\n",
      "Epoch 189/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 195070.3216 - mse: 176487.5030 - mae: 362.1212\n",
      "Epoch 190/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 181025.8103 - mse: 163231.5260 - mae: 347.3243\n",
      "Epoch 191/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 183993.4709 - mse: 166630.4154 - mae: 347.7682\n",
      "Epoch 192/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 180065.7891 - mse: 163157.7274 - mae: 346.9707\n",
      "Epoch 193/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 187837.6406 - mse: 171445.4484 - mae: 353.4793\n",
      "Epoch 194/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 165318.6836 - mse: 149685.8034 - mae: 330.3107\n",
      "Epoch 195/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 166975.4518 - mse: 152255.6172 - mae: 327.8513\n",
      "Epoch 196/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 150581.8066 - mse: 136507.7941 - mae: 305.3510\n",
      "Epoch 197/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 164431.2361 - mse: 149834.6862 - mae: 319.2162\n",
      "Epoch 198/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 144718.2452 - mse: 130843.9251 - mae: 303.0275\n",
      "Epoch 199/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 152548.2311 - mse: 138880.1695 - mae: 310.7596\n",
      "Epoch 200/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 155328.9449 - mse: 141370.9846 - mae: 309.5524\n",
      "Epoch 201/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 135601.1439 - mse: 121622.9865 - mae: 281.3161\n",
      "Epoch 202/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 152776.0812 - mse: 139536.4729 - mae: 308.7847\n",
      "Epoch 203/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 156148.6259 - mse: 143307.8264 - mae: 313.4830\n",
      "Epoch 204/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 148983.2934 - mse: 136595.7812 - mae: 308.7552\n",
      "Epoch 205/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 155739.4084 - mse: 143996.9575 - mae: 314.0536\n",
      "Epoch 206/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 130053.1434 - mse: 118312.3008 - mae: 282.1214\n",
      "Epoch 207/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 146575.4034 - mse: 134469.8468 - mae: 308.6782\n",
      "Epoch 208/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 127269.4143 - mse: 116341.1710 - mae: 279.4136\n",
      "Epoch 209/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 134189.8468 - mse: 123898.3257 - mae: 293.2420\n",
      "Epoch 210/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 142138.2342 - mse: 131845.1962 - mae: 300.4066\n",
      "Epoch 211/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 126876.9223 - mse: 117094.3290 - mae: 282.7617\n",
      "Epoch 212/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 116426.5729 - mse: 107018.6578 - mae: 266.3606\n",
      "Epoch 213/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 128683.5393 - mse: 119025.7602 - mae: 281.1330\n",
      "Epoch 214/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 123863.5775 - mse: 114416.3038 - mae: 275.6729\n",
      "Epoch 215/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 137285.3086 - mse: 128427.4442 - mae: 299.5929\n",
      "Epoch 216/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 119942.5749 - mse: 111284.9976 - mae: 271.1360\n",
      "Epoch 217/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 108779.1425 - mse: 100359.3468 - mae: 257.5668\n",
      "Epoch 218/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 122140.5974 - mse: 113869.6771 - mae: 278.5036\n",
      "Epoch 219/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 101919.3073 - mse: 94322.6370 - mae: 248.5837\n",
      "Epoch 220/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 103455.2309 - mse: 95796.5278 - mae: 251.9351\n",
      "Epoch 221/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 105889.5569 - mse: 98846.0753 - mae: 254.8162\n",
      "Epoch 222/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 114078.3900 - mse: 107474.4234 - mae: 267.1865\n",
      "Epoch 223/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 102033.7222 - mse: 95555.8142 - mae: 246.6412\n",
      "Epoch 224/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 109952.8997 - mse: 103697.0312 - mae: 263.9807\n",
      "Epoch 225/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 102980.8995 - mse: 96829.7337 - mae: 249.3576\n",
      "Epoch 226/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 93171.5085 - mse: 87148.8563 - mae: 238.7363\n",
      "Epoch 227/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 93425.5334 - mse: 87280.4837 - mae: 233.5216\n",
      "Epoch 228/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 96323.6096 - mse: 90960.5697 - mae: 243.7403\n",
      "Epoch 229/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 114993.4594 - mse: 109768.7385 - mae: 273.3615\n",
      "Epoch 230/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 98351.3890 - mse: 93508.4814 - mae: 244.3387\n",
      "Epoch 231/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 103736.4451 - mse: 99079.2209 - mae: 254.9978\n",
      "Epoch 232/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 105631.9023 - mse: 101090.6960 - mae: 259.2696\n",
      "Epoch 233/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 93233.9900 - mse: 88936.0063 - mae: 245.6078\n",
      "Epoch 234/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 97648.9196 - mse: 93720.2582 - mae: 248.8047\n",
      "Epoch 235/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 99473.1654 - mse: 95673.5417 - mae: 251.4632\n",
      "Epoch 236/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 103592.5336 - mse: 99425.8594 - mae: 255.9794\n",
      "Epoch 237/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 110121.9097 - mse: 106123.8557 - mae: 267.8915\n",
      "Epoch 238/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 90752.3816 - mse: 86749.4875 - mae: 235.7187\n",
      "Epoch 239/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 101207.7624 - mse: 97380.4132 - mae: 253.8525\n",
      "Epoch 240/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 96228.8646 - mse: 92856.8322 - mae: 244.5436\n",
      "Epoch 241/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 85660.9301 - mse: 82330.4774 - mae: 232.5437\n",
      "Epoch 242/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 93040.1148 - mse: 90035.4481 - mae: 236.7894\n",
      "Epoch 243/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 102931.8045 - mse: 100068.5816 - mae: 257.3781\n",
      "Epoch 244/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 93701.6687 - mse: 90728.9051 - mae: 240.5375\n",
      "Epoch 245/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 96363.8201 - mse: 93522.9835 - mae: 244.0195\n",
      "Epoch 246/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 100107.6762 - mse: 97054.6165 - mae: 256.9563\n",
      "Epoch 247/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 92574.6560 - mse: 89324.2650 - mae: 238.6142\n",
      "Epoch 248/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 103160.5312 - mse: 99979.3741 - mae: 256.3677\n",
      "Epoch 249/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 89767.2087 - mse: 87028.9492 - mae: 236.0686\n",
      "Epoch 250/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 84265.5382 - mse: 81459.7571 - mae: 228.2027\n",
      "Epoch 251/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 104751.9542 - mse: 102190.7051 - mae: 261.1077\n",
      "Epoch 252/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 97282.0816 - mse: 94906.8570 - mae: 252.7116\n",
      "Epoch 253/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 84804.0571 - mse: 82462.2964 - mae: 235.1844\n",
      "Epoch 254/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 93929.9927 - mse: 91578.3599 - mae: 246.3408\n",
      "Epoch 255/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 81586.4290 - mse: 79289.1230 - mae: 226.9283\n",
      "Epoch 256/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 81083.4414 - mse: 78877.4952 - mae: 227.9677\n",
      "Epoch 257/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 85758.9900 - mse: 83611.8010 - mae: 232.1967\n",
      "Epoch 258/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 79914.7196 - mse: 77853.8700 - mae: 226.7981\n",
      "Epoch 259/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 78242.5455 - mse: 76462.7362 - mae: 218.9015\n",
      "Epoch 260/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77620.8957 - mse: 75890.7380 - mae: 221.3171\n",
      "Epoch 261/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 81728.1341 - mse: 79879.6120 - mae: 232.2094\n",
      "Epoch 262/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91175.7901 - mse: 89358.6803 - mae: 244.7483\n",
      "Epoch 263/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 74595.8395 - mse: 72851.6467 - mae: 214.4284\n",
      "Epoch 264/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 89069.9427 - mse: 87352.7906 - mae: 239.0712\n",
      "Epoch 265/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73923.4928 - mse: 72276.9400 - mae: 217.8843\n",
      "Epoch 266/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 79435.2849 - mse: 77818.2669 - mae: 224.6204\n",
      "Epoch 267/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 83936.1701 - mse: 82335.9485 - mae: 229.1402\n",
      "Epoch 268/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 85097.9586 - mse: 83581.5473 - mae: 230.4911\n",
      "Epoch 269/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 83905.4429 - mse: 82450.1122 - mae: 229.6725\n",
      "Epoch 270/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 82162.4774 - mse: 80521.4963 - mae: 228.5018\n",
      "Epoch 271/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 86518.8114 - mse: 84981.0875 - mae: 235.9269\n",
      "Epoch 272/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 87287.3060 - mse: 85946.5809 - mae: 233.3769\n",
      "Epoch 273/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 97833.3164 - mse: 96373.0061 - mae: 249.2221\n",
      "Epoch 274/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77269.5263 - mse: 75886.8217 - mae: 221.3133\n",
      "Epoch 275/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 78617.4084 - mse: 77027.1539 - mae: 228.2593\n",
      "Epoch 276/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 80299.2456 - mse: 78892.0411 - mae: 220.0772\n",
      "Epoch 277/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 87977.1625 - mse: 86604.9384 - mae: 236.5355\n",
      "Epoch 278/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 78200.8848 - mse: 76857.7945 - mae: 226.7950\n",
      "Epoch 279/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 80894.1157 - mse: 79547.8019 - mae: 227.4671\n",
      "Epoch 280/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 87384.3203 - mse: 86055.4685 - mae: 238.5865\n",
      "Epoch 281/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77119.0798 - mse: 75872.7939 - mae: 227.8891\n",
      "Epoch 282/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77973.9516 - mse: 76630.4885 - mae: 221.7463\n",
      "Epoch 283/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 79315.5078 - mse: 78245.5884 - mae: 224.3447\n",
      "Epoch 284/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 87058.9844 - mse: 85798.3377 - mae: 234.2913\n",
      "Epoch 285/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 74972.2362 - mse: 73634.0256 - mae: 213.4535\n",
      "Epoch 286/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71962.0357 - mse: 70590.1617 - mae: 211.5907\n",
      "Epoch 287/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 80043.8419 - mse: 78963.4580 - mae: 227.1501\n",
      "Epoch 288/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 81188.4386 - mse: 80177.1263 - mae: 228.5061\n",
      "Epoch 289/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 68317.1163 - mse: 66983.0798 - mae: 206.2448\n",
      "Epoch 290/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 74057.2730 - mse: 73014.6361 - mae: 215.6548\n",
      "Epoch 291/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77108.7951 - mse: 75963.5089 - mae: 220.1341\n",
      "Epoch 292/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 78562.8500 - mse: 77476.2498 - mae: 222.7279\n",
      "Epoch 293/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 81296.1645 - mse: 80346.6521 - mae: 230.3089\n",
      "Epoch 294/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73618.0084 - mse: 72675.5061 - mae: 214.3579\n",
      "Epoch 295/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 74272.2061 - mse: 73262.8171 - mae: 211.1739\n",
      "Epoch 296/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 69258.0119 - mse: 68425.5757 - mae: 213.7186\n",
      "Epoch 297/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 68876.5974 - mse: 68015.3056 - mae: 206.1804\n",
      "Epoch 298/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 64847.7909 - mse: 63996.3970 - mae: 200.2583\n",
      "Epoch 299/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70515.4157 - mse: 69629.6175 - mae: 215.1881\n",
      "Epoch 300/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 72043.9839 - mse: 71254.5395 - mae: 214.0751\n",
      "35/35 [==============================] - 0s 800us/step\n",
      "12/12 [==============================] - 0s 999us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8376284.286574147, tolerance: 2469.0218836503623\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.12579e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.34075e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>xgb</th>\n",
       "      <th>knn</th>\n",
       "      <th>lasso</th>\n",
       "      <th>lasso_tuned</th>\n",
       "      <th>ridge</th>\n",
       "      <th>ridge_tuned</th>\n",
       "      <th>rf</th>\n",
       "      <th>sv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.999871</td>\n",
       "      <td>-0.586683</td>\n",
       "      <td>0.321490</td>\n",
       "      <td>0.321069</td>\n",
       "      <td>0.288789</td>\n",
       "      <td>0.321489</td>\n",
       "      <td>0.916870</td>\n",
       "      <td>0.031581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.216961</td>\n",
       "      <td>-0.950417</td>\n",
       "      <td>0.196441</td>\n",
       "      <td>0.199087</td>\n",
       "      <td>0.189613</td>\n",
       "      <td>0.196590</td>\n",
       "      <td>0.379572</td>\n",
       "      <td>-0.007643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>1.652287</td>\n",
       "      <td>218.920043</td>\n",
       "      <td>144.803388</td>\n",
       "      <td>144.831781</td>\n",
       "      <td>145.642336</td>\n",
       "      <td>144.808127</td>\n",
       "      <td>49.100009</td>\n",
       "      <td>179.702344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>148.279692</td>\n",
       "      <td>249.260724</td>\n",
       "      <td>159.207571</td>\n",
       "      <td>159.085240</td>\n",
       "      <td>156.910879</td>\n",
       "      <td>159.196911</td>\n",
       "      <td>136.997065</td>\n",
       "      <td>185.019818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>2.400220</td>\n",
       "      <td>266.402328</td>\n",
       "      <td>174.209231</td>\n",
       "      <td>174.263163</td>\n",
       "      <td>178.357810</td>\n",
       "      <td>174.209300</td>\n",
       "      <td>60.977820</td>\n",
       "      <td>208.125108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>188.362141</td>\n",
       "      <td>297.280042</td>\n",
       "      <td>190.814337</td>\n",
       "      <td>190.499937</td>\n",
       "      <td>191.623250</td>\n",
       "      <td>190.796603</td>\n",
       "      <td>167.667012</td>\n",
       "      <td>213.675802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>1.450727</td>\n",
       "      <td>191.586927</td>\n",
       "      <td>188.651973</td>\n",
       "      <td>187.474166</td>\n",
       "      <td>189.808552</td>\n",
       "      <td>188.540082</td>\n",
       "      <td>52.335212</td>\n",
       "      <td>316.076221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>80.007616</td>\n",
       "      <td>116.999949</td>\n",
       "      <td>129.383616</td>\n",
       "      <td>129.874216</td>\n",
       "      <td>121.174434</td>\n",
       "      <td>129.416170</td>\n",
       "      <td>83.456416</td>\n",
       "      <td>190.543998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  xgb         knn       lasso  lasso_tuned       ridge  \\\n",
       "Train: Rsquare    0.999871   -0.586683    0.321490     0.321069    0.288789   \n",
       "Test: Rsquare     0.216961   -0.950417    0.196441     0.199087    0.189613   \n",
       "Train: MAE        1.652287  218.920043  144.803388   144.831781  145.642336   \n",
       "Test: MAE       148.279692  249.260724  159.207571   159.085240  156.910879   \n",
       "Train: RMSE       2.400220  266.402328  174.209231   174.263163  178.357810   \n",
       "Test: RMSE      188.362141  297.280042  190.814337   190.499937  191.623250   \n",
       "Train: MAPE       1.450727  191.586927  188.651973   187.474166  189.808552   \n",
       "Test: MAPE       80.007616  116.999949  129.383616   129.874216  121.174434   \n",
       "\n",
       "Model           ridge_tuned          rf          sv  \n",
       "Train: Rsquare     0.321489    0.916870    0.031581  \n",
       "Test: Rsquare      0.196590    0.379572   -0.007643  \n",
       "Train: MAE       144.808127   49.100009  179.702344  \n",
       "Test: MAE        159.196911  136.997065  185.019818  \n",
       "Train: RMSE      174.209300   60.977820  208.125108  \n",
       "Test: RMSE       190.796603  167.667012  213.675802  \n",
       "Train: MAPE      188.540082   52.335212  316.076221  \n",
       "Test: MAPE       129.416170   83.456416  190.543998  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model performance with twitter BERT and news sentiments\")\n",
    "model_performance([xgbr, nn_tuned, lasso_reg, lasso_tuned, ridge_reg, ridge_tune2, rf, sv], [\"xgb\", 'knn', 'lasso', 'lasso_tuned', 'ridge', 'ridge_tuned', 'rf', 'sv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important_gain = xgbr.get_booster().get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_important_gain = xgbr.get_booster().get_score(importance_type='gain')\n",
    "keys = list(feature_important_gain.keys())\n",
    "values = list(feature_important_gain.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=['score']).sort_values(by='score', ascending=False)\n",
    "data.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
