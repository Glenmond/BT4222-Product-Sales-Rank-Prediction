{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brave-pizza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: afinn in /Users/joshuatam/opt/anaconda3/lib/python3.7/site-packages (0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressive-header",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuatam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/joshuatam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joshuatam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joshuatam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/joshuatam/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enormous-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer_apple = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "individual-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "excluded = ['no', 'not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tight-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "compatible-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "def remove_hashtags(sample):\n",
    "    return re.sub(r\"#\\S+\", \"\", sample)\n",
    "\n",
    "def remove_atMarks(sample):\n",
    "    return re.sub(r\"@\\S+\", \"\", sample).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "august-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "import os\n",
    "\n",
    "#Apple datasets\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/Apple/Combined')\n",
    "df_apple = pd.read_csv('apple_combined.csv')\n",
    "\n",
    "#Pwr+ datasets\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/Pwr+/Combined')\n",
    "df_pwr = pd.read_csv('Pwr+_combined.csv')\n",
    "\n",
    "#Sony datasets\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/Sony/Combined')\n",
    "df_sony = pd.read_csv('Sony_combined.csv')\n",
    "\n",
    "#Garmin datasets\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/Garmin/Combined')\n",
    "df_garmin = pd.read_csv('Garmin_combined.csv')\n",
    "\n",
    "#YamahaAudio datasets\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/YamahaAudio/Combined')\n",
    "df_yamaha = pd.read_csv('YamahaAudio_combined.csv')\n",
    "\n",
    "#Belkin datasets\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/Belkin/Combined')\n",
    "df_belkin = pd.read_csv('Belkin_combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-shadow",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Apple Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "constant-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apple['Text'] = df_apple['Text'].fillna(\"missing\")\n",
    "\n",
    "df_apple['TextClean'] = df_apple['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_apple['TextClean']:\n",
    "    df_apple.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_apple.at[count,\"TextClean\"] = remove_hashtags(df_apple.at[count,\"TextClean\"])\n",
    "    df_apple.at[count,\"TextClean\"] = remove_atMarks(df_apple.at[count,\"TextClean\"])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expired-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove number and special characters\n",
    "df_apple['TextClean'] = df_apple['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_apple['TextClean'] = df_apple['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_apple['TextClean'] = df_apple['TextClean'].str.strip()\n",
    "\n",
    "# fill empty review with na\n",
    "# df_apple['TextClean'] = df_apple['TextClean'].fillna('', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disabled-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# df_merge['reviewClean'] = df_merge['reviewClean'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df_apple['TextClean'] = df_apple['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supported-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apple['TextCleanLemm'] = df_apple['TextClean'].apply(lambda x: [lemmatizer_apple.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fallen-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_apple['tb_score'] = df_apple['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_apple['afinn_score'] = df_apple['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_apple['vader_com'] = df_apple['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_apple['vader_pos'] = df_apple['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_apple['vader_neg'] = df_apple['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_apple['vader_neu'] = df_apple['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "royal-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export apple data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_apple.to_csv(\"Apple_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-ukraine",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Pwr+ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "modified-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pwr['Text'] = df_pwr['Text'].fillna(\"missing\")\n",
    "\n",
    "df_pwr['TextClean'] = df_pwr['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_pwr['TextClean']:\n",
    "    df_pwr.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_pwr.at[count,\"TextClean\"] = remove_hashtags(df_pwr.at[count,\"TextClean\"])\n",
    "    df_pwr.at[count,\"TextClean\"] = remove_atMarks(df_pwr.at[count,\"TextClean\"])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "perceived-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove number and special characters\n",
    "df_pwr['TextClean'] = df_pwr['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_pwr['TextClean'] = df_pwr['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_pwr['TextClean'] = df_pwr['TextClean'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beautiful-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df_pwr['TextClean'] = df_pwr['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "employed-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_pwr = WordNetLemmatizer()\n",
    "\n",
    "df_pwr['TextCleanLemm'] = df_pwr['TextClean'].apply(lambda x: [lemmatizer_pwr.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "western-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_pwr['tb_score'] = df_pwr['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_pwr['afinn_score'] = df_pwr['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_pwr['vader_com'] = df_pwr['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_pwr['vader_pos'] = df_pwr['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_pwr['vader_neg'] = df_pwr['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_pwr['vader_neu'] = df_pwr['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "drawn-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export pwr+ data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_pwr.to_csv(\"Pwr+_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-typing",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Sony Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "protected-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sony['Text'] = df_sony['Text'].fillna(\"missing\")\n",
    "\n",
    "df_sony['TextClean'] = df_sony['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_sony['TextClean']:\n",
    "    df_sony.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_sony.at[count,\"TextClean\"] = remove_hashtags(df_sony.at[count,\"TextClean\"])\n",
    "    df_sony.at[count,\"TextClean\"] = remove_atMarks(df_sony.at[count,\"TextClean\"])\n",
    "    count += 1\n",
    "\n",
    "# Remove number and special characters\n",
    "df_sony['TextClean'] = df_sony['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_sony['TextClean'] = df_sony['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_sony['TextClean'] = df_sony['TextClean'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hungarian-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df_sony['TextClean'] = df_sony['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "photographic-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer_sony = WordNetLemmatizer()\n",
    "df_sony['TextCleanLemm'] = df_sony['TextClean'].apply(lambda x: [lemmatizer_sony.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "complete-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_sony['tb_score'] = df_sony['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_sony['afinn_score'] = df_sony['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_sony['vader_com'] = df_sony['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_sony['vader_pos'] = df_sony['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_sony['vader_neg'] = df_sony['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_sony['vader_neu'] = df_sony['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fifty-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export sony data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_sony.to_csv(\"Sony_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-transaction",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Garmin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "lucky-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_garmin['Text'] = df_garmin['Text'].fillna(\"missing\")\n",
    "\n",
    "df_garmin['TextClean'] = df_garmin['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_garmin['TextClean']:\n",
    "    df_garmin.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_garmin.at[count,\"TextClean\"] = remove_hashtags(df_garmin.at[count,\"TextClean\"])\n",
    "    df_garmin.at[count,\"TextClean\"] = remove_atMarks(df_garmin.at[count,\"TextClean\"])\n",
    "    count += 1\n",
    "\n",
    "# Remove number and special characters\n",
    "df_garmin['TextClean'] = df_garmin['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_garmin['TextClean'] = df_garmin['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_garmin['TextClean'] = df_garmin['TextClean'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "satisfactory-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df_garmin['TextClean'] = df_garmin['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "national-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer_garmin = WordNetLemmatizer()\n",
    "df_garmin['TextCleanLemm'] = df_garmin['TextClean'].apply(lambda x: [lemmatizer_garmin.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bizarre-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_garmin['tb_score'] = df_garmin['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_garmin['afinn_score'] = df_garmin['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_garmin['vader_com'] = df_garmin['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_garmin['vader_pos'] = df_garmin['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_garmin['vader_neg'] = df_garmin['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_garmin['vader_neu'] = df_garmin['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "competent-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export garmin data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_garmin.to_csv(\"Garmin_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-session",
   "metadata": {},
   "source": [
    "# Sentiment analysis for YamahaAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "neutral-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yamaha['Text'] = df_yamaha['Text'].fillna(\"missing\")\n",
    "\n",
    "df_yamaha['TextClean'] = df_yamaha['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_yamaha['TextClean']:\n",
    "    df_yamaha.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_yamaha.at[count,\"TextClean\"] = remove_hashtags(df_yamaha.at[count,\"TextClean\"])\n",
    "    df_yamaha.at[count,\"TextClean\"] = remove_atMarks(df_yamaha.at[count,\"TextClean\"])\n",
    "    count += 1\n",
    "\n",
    "# Remove number and special characters\n",
    "df_yamaha['TextClean'] = df_yamaha['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_yamaha['TextClean'] = df_yamaha['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_yamaha['TextClean'] = df_yamaha['TextClean'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "instrumental-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df_yamaha['TextClean'] = df_yamaha['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "taken-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer_yamaha = WordNetLemmatizer()\n",
    "df_yamaha['TextCleanLemm'] = df_yamaha['TextClean'].apply(lambda x: [lemmatizer_yamaha.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "historical-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_yamaha['tb_score'] = df_yamaha['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_yamaha['afinn_score'] = df_yamaha['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_yamaha['vader_com'] = df_yamaha['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_yamaha['vader_pos'] = df_yamaha['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_yamaha['vader_neg'] = df_yamaha['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_yamaha['vader_neu'] = df_yamaha['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "contrary-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export garmin data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_yamaha.to_csv(\"YamahaAudio_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-clinton",
   "metadata": {},
   "source": [
    "# Sentiment analysis for Belkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unique-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_belkin['Text'] = df_belkin['Text'].fillna(\"missing\")\n",
    "\n",
    "df_belkin['TextClean'] = df_belkin['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_belkin['TextClean']:\n",
    "    df_belkin.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_belkin.at[count,\"TextClean\"] = remove_hashtags(df_belkin.at[count,\"TextClean\"])\n",
    "    df_belkin.at[count,\"TextClean\"] = remove_atMarks(df_belkin.at[count,\"TextClean\"])\n",
    "    count += 1\n",
    "\n",
    "# Remove number and special characters\n",
    "df_belkin['TextClean'] = df_belkin['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_belkin['TextClean'] = df_belkin['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_belkin['TextClean'] = df_belkin['TextClean'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "spare-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df_belkin['TextClean'] = df_belkin['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "vulnerable-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer_belkin = WordNetLemmatizer()\n",
    "df_belkin['TextCleanLemm'] = df_belkin['TextClean'].apply(lambda x: [lemmatizer_belkin.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "banner-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_belkin['tb_score'] = df_belkin['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_belkin['afinn_score'] = df_belkin['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_belkin['vader_com'] = df_belkin['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_belkin['vader_pos'] = df_belkin['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_belkin['vader_neg'] = df_belkin['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_belkin['vader_neu'] = df_belkin['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "vertical-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export garmin data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_belkin.to_csv(\"Belkin_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-bread",
   "metadata": {},
   "source": [
    "# Sentiment analysis for BossAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "greatest-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/BossAudio/Combined')\n",
    "df_boss = pd.read_csv('BossAudio_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "homeless-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boss['Text'] = df_boss['Text'].fillna(\"missing\")\n",
    "\n",
    "df_boss['TextClean'] = df_boss['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_boss['TextClean']:\n",
    "    df_boss.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_boss.at[count,\"TextClean\"] = remove_hashtags(df_boss.at[count,\"TextClean\"])\n",
    "    df_boss.at[count,\"TextClean\"] = remove_atMarks(df_boss.at[count,\"TextClean\"])\n",
    "    count += 1\n",
    "\n",
    "# Remove number and special characters\n",
    "df_boss['TextClean'] = df_boss['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_boss['TextClean'] = df_boss['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_boss['TextClean'] = df_boss['TextClean'].str.strip()\n",
    "\n",
    "# remove stopwords\n",
    "df_boss['TextClean'] = df_boss['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer_boss = WordNetLemmatizer()\n",
    "df_boss['TextCleanLemm'] = df_boss['TextClean'].apply(lambda x: [lemmatizer_boss.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))\n",
    "\n",
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_boss['tb_score'] = df_boss['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_boss['afinn_score'] = df_boss['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_boss['vader_com'] = df_boss['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_boss['vader_pos'] = df_boss['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_boss['vader_neg'] = df_boss['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_boss['vader_neu'] = df_boss['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])\n",
    "\n",
    "#Export data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_boss.to_csv(\"BossAudio_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-valuation",
   "metadata": {},
   "source": [
    "# Sentiment analysis for TrippLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cordless-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/TrippLite/Combined')\n",
    "df_tl = pd.read_csv('TrippLite_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "suspended-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl['Text'] = df_tl['Text'].fillna(\"missing\")\n",
    "\n",
    "df_tl['TextClean'] = df_tl['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_tl['TextClean']:\n",
    "    df_tl.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_tl.at[count,\"TextClean\"] = remove_hashtags(df_tl.at[count,\"TextClean\"])\n",
    "    df_tl.at[count,\"TextClean\"] = remove_atMarks(df_tl.at[count,\"TextClean\"])\n",
    "    count += 1\n",
    "\n",
    "# Remove number and special characters\n",
    "df_tl['TextClean'] = df_tl['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_tl['TextClean'] = df_tl['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_tl['TextClean'] = df_tl['TextClean'].str.strip()\n",
    "\n",
    "# remove stopwords\n",
    "df_tl['TextClean'] = df_tl['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer_tl = WordNetLemmatizer()\n",
    "df_tl['TextCleanLemm'] = df_tl['TextClean'].apply(lambda x: [lemmatizer_tl.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))\n",
    "\n",
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_tl['tb_score'] = df_tl['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_tl['afinn_score'] = df_tl['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_tl['vader_com'] = df_tl['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_tl['vader_pos'] = df_tl['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_tl['vader_neg'] = df_tl['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_tl['vader_neu'] = df_tl['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])\n",
    "\n",
    "#Export data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_tl.to_csv(\"TrippLite_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-survival",
   "metadata": {},
   "source": [
    "# Sentiment analysis for PolkAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "toxic-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/PolkAudio/Combined')\n",
    "df_polk = pd.read_csv('PolkAudio_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "capable-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polk['Text'] = df_polk['Text'].fillna(\"missing\")\n",
    "\n",
    "df_polk['TextClean'] = df_polk['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_polk['TextClean']:\n",
    "    df_polk.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_polk.at[count,\"TextClean\"] = remove_hashtags(df_polk.at[count,\"TextClean\"])\n",
    "    df_polk.at[count,\"TextClean\"] = remove_atMarks(df_polk.at[count,\"TextClean\"])\n",
    "    count += 1\n",
    "\n",
    "# Remove number and special characters\n",
    "df_polk['TextClean'] = df_polk['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_polk['TextClean'] = df_polk['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_polk['TextClean'] = df_polk['TextClean'].str.strip()\n",
    "\n",
    "# remove stopwords\n",
    "df_polk['TextClean'] = df_polk['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer_polk = WordNetLemmatizer()\n",
    "df_polk['TextCleanLemm'] = df_polk['TextClean'].apply(lambda x: [lemmatizer_polk.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))\n",
    "\n",
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_polk['tb_score'] = df_polk['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_polk['afinn_score'] = df_polk['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_polk['vader_com'] = df_polk['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_polk['vader_pos'] = df_polk['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_polk['vader_neg'] = df_polk['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_polk['vader_neu'] = df_polk['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])\n",
    "\n",
    "#Export data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_polk.to_csv(\"PolkAudio_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-groove",
   "metadata": {},
   "source": [
    "# Sentiment analysis for Sangean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceramic-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/Sangean/Combined')\n",
    "df_san = pd.read_csv('Sangean_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "consolidated-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_san['Text'] = df_san['Text'].fillna(\"missing\")\n",
    "\n",
    "df_san['TextClean'] = df_san['Text']\n",
    "\n",
    "count = 0\n",
    "for i in df_san['TextClean']:\n",
    "    df_san.at[count,\"TextClean\"] = remove_URL(i)\n",
    "    df_san.at[count,\"TextClean\"] = remove_hashtags(df_san.at[count,\"TextClean\"])\n",
    "    df_san.at[count,\"TextClean\"] = remove_atMarks(df_san.at[count,\"TextClean\"])\n",
    "    count += 1\n",
    "\n",
    "# Remove number and special characters\n",
    "df_san['TextClean'] = df_san['TextClean'].replace(\"[^a-zA-Z]+\", \" \", regex = True)\n",
    "\n",
    "# lower case\n",
    "df_san['TextClean'] = df_san['TextClean'].str.lower()\n",
    "\n",
    "# remove white spaces\n",
    "df_san['TextClean'] = df_san['TextClean'].str.strip()\n",
    "\n",
    "# remove stopwords\n",
    "df_san['TextClean'] = df_san['TextClean'].apply(lambda x: \" \".join(x for x in x.split() if (x not in stop) or (x in excluded)))\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer_san = WordNetLemmatizer()\n",
    "df_san['TextCleanLemm'] = df_san['TextClean'].apply(lambda x: [lemmatizer_san.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]).apply(lambda x:\" \".join(x))\n",
    "\n",
    "## Sentiment scores\n",
    "\n",
    "# Textblob\n",
    "df_san['tb_score'] = df_san['TextCleanLemm'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Affin score\n",
    "af = Afinn()\n",
    "df_san['afinn_score'] = df_san['TextCleanLemm'].apply(lambda x: af.score(x))\n",
    "\n",
    "#vader analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df_san['vader_com'] = df_san['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_san['vader_pos'] = df_san['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "df_san['vader_neg'] = df_san['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neg']*(-1))\n",
    "df_san['vader_neu'] = df_san['TextCleanLemm'].apply(lambda x: sid.polarity_scores(x)['neu'])\n",
    "\n",
    "#Export data\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "df_san.to_csv(\"Sangean_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-removal",
   "metadata": {},
   "source": [
    "# Combine all brands twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hindu-generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TrippLite_sentiment_analysis.csv', 'YamahaAudio_sentiment_analysis.csv', 'Sangean_sentiment_analysis.csv', 'Apple_sentiment_analysis.csv', 'PolkAudio_sentiment_analysis.csv', 'Belkin_sentiment_analysis.csv', 'Pwr+_sentiment_analysis.csv', 'Garmin_sentiment_analysis.csv', 'Sony_sentiment_analysis.csv', 'BossAudio_sentiment_analysis.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "all_filenames = [i for i in glob.glob('*')]\n",
    "print(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_twitter_df = pd.DataFrame(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "guilty-reunion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31417, 15)\n",
      "(103107, 15)\n",
      "(135389, 15)\n",
      "(171926, 15)\n",
      "(204046, 15)\n",
      "(236915, 15)\n",
      "(269796, 16)\n",
      "(302669, 16)\n",
      "(325590, 16)\n",
      "(327982, 16)\n"
     ]
    }
   ],
   "source": [
    "combined_twitter_df = pd.DataFrame(list())\n",
    "\n",
    "for file in all_filenames:\n",
    "    brandname = file.split(\"_\")[0]\n",
    "    df = pd.read_csv(file, index_col=False, engine='python')\n",
    "    df['Brand'] = brandname\n",
    "    combined_twitter_df = pd.concat([combined_twitter_df,df])\n",
    "    print(combined_twitter_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cross-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnamed column\n",
    "combined_twitter_df = combined_twitter_df.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# Drop null values\n",
    "combined_twitter_df = combined_twitter_df.dropna(subset = ['Datetime', 'Tweet Id', 'Retweets', 'TextClean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "northern-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/joshuatam/Documents/NUS/AY_2020_21_SEM_2/BT4222/Group Project/Datasets/sentimentAnalysis')\n",
    "combined_twitter_df.to_csv(\"Combined_sentiment_analysis.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-portuguese",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
