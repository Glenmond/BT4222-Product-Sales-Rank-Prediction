{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Models with twitter and google news headlines sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models analysis for top 10 brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import ast\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None) # display all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/notebooks/Data')\n",
    "df_standby = pd.read_csv('aggregated_reviews_meta.csv')\n",
    "df_standby.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df = df_standby.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape is:  (7097, 17)\n",
      "Top 10 brand df's shape is:  (736, 17)\n"
     ]
    }
   ],
   "source": [
    "top10_brand_df = df[df['brand'].isin(['Sony','PWR+','Apple','Boss Audio','Polk Audio','Sangean', 'Tripp Lite', \n",
    "                    'Yamaha Audio','Belkin','Garmin'])]\n",
    "print(\"df shape is: \", df.shape)\n",
    "print(\"Top 10 brand df's shape is: \", top10_brand_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter_BERT_Sentiment data\n",
    "os.chdir('/notebooks/Data')\n",
    "df_BERT = pd.read_csv('Brand_BERT_sentiment.csv')\n",
    "#df_standby.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_bert = df_BERT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand\n",
      "Apple          0.606678\n",
      "Belkin         0.622052\n",
      "BossAudio      0.718182\n",
      "Garmin         0.595745\n",
      "PolkAudio      0.702373\n",
      "Pwr+           0.458546\n",
      "Sangean        0.540475\n",
      "Sony           0.609016\n",
      "TrippLite      0.412678\n",
      "YamahaAudio    0.695243\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Aggregate sentiment column of df_bert by brands create a new column for meta data\n",
    "bert_sentiment = pd.pivot_table(df_bert, index='Brand', values=\"Sentiment\", aggfunc=np.mean)\n",
    "print(bert_sentiment.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (top10_brand_df['brand'] == 'Apple'),\n",
    "    (top10_brand_df['brand'] == 'Belkin'),\n",
    "    (top10_brand_df['brand'] == 'Boss Audio'),\n",
    "    (top10_brand_df['brand'] == 'Garmin'),\n",
    "    (top10_brand_df['brand'] == 'Polk Audio'),\n",
    "    (top10_brand_df['brand'] == 'PWR+'),\n",
    "    (top10_brand_df['brand'] == 'Sangean'),\n",
    "    (top10_brand_df['brand'] == 'Sony'),\n",
    "    (top10_brand_df['brand'] == 'Tripp Lite'),\n",
    "    (top10_brand_df['brand'] == 'Yamaha Audio')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [0.606678, 0.622052, 0.718182, 0.595745, 0.702373, 0.458546, 0.540475, 0.609016, 0.412678, 0.695243]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "top10_brand_df['brand_sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load News_Sentiment data\n",
    "os.chdir('/notebooks/Data')\n",
    "df_news = pd.read_csv('Brand_News_Sentiment.csv')\n",
    "df_news.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (top10_brand_df['brand'] == 'Apple'),\n",
    "    (top10_brand_df['brand'] == 'Belkin'),\n",
    "    (top10_brand_df['brand'] == 'Boss Audio'),\n",
    "    (top10_brand_df['brand'] == 'Garmin'),\n",
    "    (top10_brand_df['brand'] == 'Polk Audio'),\n",
    "    (top10_brand_df['brand'] == 'PWR+'),\n",
    "    (top10_brand_df['brand'] == 'Sangean'),\n",
    "    (top10_brand_df['brand'] == 'Sony'),\n",
    "    (top10_brand_df['brand'] == 'Tripp Lite'),\n",
    "    (top10_brand_df['brand'] == 'Yamaha Audio')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [0.130569, 0.206817, 0.307327, 0.219252, 0.166181, 0.569831, 0.319515, 0.082589, 0.076493, 0.139865]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "top10_brand_df['brand_news_sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.19.5)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.5.4)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# do encoding for categorical variable\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "new_df = top10_brand_df\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "new_df['brand_encode'] = encoder.fit_transform(new_df['brand'], new_df['rankElectronics'])\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "new_df['main_cat_encode'] = encoder.fit_transform(new_df['main_cat'], new_df['rankElectronics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "We will be using four models for prediction of sales rank. The following are as shown:<br>\n",
    "1. XGBoost Regressor \n",
    "2. Neural Network\n",
    "3. Ridge / Lasso Regression\n",
    "4. Random Forest\n",
    "5. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df['normRankElectronics'] = np.array(sales_df.rankElectronics.rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split\n",
    "X = sales_df.drop(['rankElectronics', 'asin', 'title', 'brand', 'main_cat', 'normRankElectronics'], axis=1)\n",
    "y = sales_df.normRankElectronics\n",
    "\n",
    "X['vote'] = X['vote'].fillna(0)# to replace NaN with 0\n",
    "\n",
    "# Standard Scaler or Min Max Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4222)\n",
    "\n",
    "X_train[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_sentiment', 'brand_news_sentiment']] = scaler.fit_transform(X_train[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_sentiment', 'brand_news_sentiment']])\n",
    "X_test[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_sentiment', 'brand_news_sentiment']] = scaler.transform(X_test[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_sentiment', 'brand_news_sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "#Defining MAPE function\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "#models is a list of the fitted models, model_names is list of model names\n",
    "\n",
    "def model_performance(models, model_names):\n",
    "    #create empty df with col names\n",
    "    df = pd.DataFrame(columns = ['Model', 'Train: Rsquare', 'Test: Rsquare', 'Train: MAE', 'Test: MAE', 'Train: RMSE', 'Test: RMSE', 'Train: MAPE', 'Test: MAPE'])\n",
    "    \n",
    "    for n, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        #prepare values for model\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        name = model_names[n] \n",
    "        rsquare_train = r2_score(y_train, y_train_pred)\n",
    "        mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "        rmse_train = math.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        \n",
    "        rsquare_test = r2_score(y_test, y_pred)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred)\n",
    "        rmse_test = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        mape_train = MAPE(y_train, y_train_pred)\n",
    "        mape_test = MAPE(y_test, y_pred)\n",
    "\n",
    "        #append row to df\n",
    "        df = df.append({'Model' \n",
    "                        : name, 'Train: Rsquare' : rsquare_train, 'Test: Rsquare' : rsquare_test, 'Train: MAE': mae_train, 'Test: MAE' : mae_test, 'Train: RMSE': rmse_train,\n",
    "                         'Test: RMSE' : rmse_test, 'Train: MAPE': mape_train, 'Test: MAPE': mape_test}, \n",
    "                    ignore_index = True)\n",
    "            \n",
    "    return df.set_index('Model').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared on train dataset=0.19469439175619685\n",
      "R-Squared on test dataset=0.5218370630022122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>dtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.445184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.194694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>128.453769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>157.805276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>157.531492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>191.021574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>174.806516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>114.039335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  dtr\n",
       "Train: Rsquare    0.445184\n",
       "Test: Rsquare     0.194694\n",
       "Train: MAE      128.453769\n",
       "Test: MAE       157.805276\n",
       "Train: RMSE     157.531492\n",
       "Test: RMSE      191.021574\n",
       "Train: MAPE     174.806516\n",
       "Test: MAPE      114.039335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Without additional predictors\n",
    "dtr = DecisionTreeRegressor(max_depth=4,\n",
    "                           min_samples_split=5,\n",
    "                           max_leaf_nodes=10)\n",
    "\n",
    "dtr.fit(X_train,y_train)\n",
    "print(\"R-Squared on train dataset={}\".format(dtr.score(X_test,y_test)))\n",
    "\n",
    "dtr.fit(X_test,y_test)   \n",
    "print(\"R-Squared on test dataset={}\".format(dtr.score(X_test,y_test)))\n",
    "\n",
    "model_performance([dtr], [\"dtr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>dtr</th>\n",
       "      <th>dtr_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.445184</td>\n",
       "      <td>0.440927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.194694</td>\n",
       "      <td>0.238770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>128.453769</td>\n",
       "      <td>130.284235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>157.805276</td>\n",
       "      <td>153.808327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>157.531492</td>\n",
       "      <td>158.134691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>191.021574</td>\n",
       "      <td>185.720513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>174.806516</td>\n",
       "      <td>156.171651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>114.039335</td>\n",
       "      <td>101.800965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  dtr   dtr_tuned\n",
       "Train: Rsquare    0.445184    0.440927\n",
       "Test: Rsquare     0.194694    0.238770\n",
       "Train: MAE      128.453769  130.284235\n",
       "Test: MAE       157.805276  153.808327\n",
       "Train: RMSE     157.531492  158.134691\n",
       "Test: RMSE      191.021574  185.720513\n",
       "Train: MAPE     174.806516  156.171651\n",
       "Test: MAPE      114.039335  101.800965"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"criterion\": [\"mse\", \"mae\"],\n",
    "              \"min_samples_split\": [10, 20, 40],\n",
    "              \"max_depth\": [2, 6, 8],\n",
    "              \"min_samples_leaf\": [20, 40, 100],\n",
    "              \"max_leaf_nodes\": [5, 20, 100],\n",
    "              }\n",
    "\n",
    "## Comment in order to publish in kaggle.\n",
    "dtr_tuned = GridSearchCV(dtr, param_grid, cv=5)\n",
    "model_performance([dtr, dtr_tuned], [\"dtr\", \"dtr_tuned\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 18,817\n",
      "Trainable params: 18,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(128,\n",
    "                activation = 'relu',\n",
    "                input_shape = (16, ),\n",
    "                activity_regularizer = regularizers.l2(1e-5)))\n",
    "nn.add(Dropout(0.50))\n",
    "nn.add(Dense(128,\n",
    "                activation = 'relu', \n",
    "                activity_regularizer = regularizers.l2(1e-5)))\n",
    "nn.add(Dropout(0.50))\n",
    "nn.add(Dense(1, activation = 'relu'))\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "56/56 [==============================] - 1s 7ms/step - loss: 256391066.2456 - mse: 255709442.8070 - mae: 5717.6691 - val_loss: 778494.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 2/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 1292704.1469 - mse: 591589.8672 - mae: 374.3574 - val_loss: 753930.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 3/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 888057.5581 - mse: 244675.0436 - mae: 365.8246 - val_loss: 620262.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 4/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 745714.8980 - mse: 231153.9638 - mae: 366.2563 - val_loss: 581727.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 5/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 712534.3465 - mse: 210493.9572 - mae: 369.7688 - val_loss: 581124.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 6/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 823514.0976 - mse: 339359.1609 - mae: 407.8525 - val_loss: 571352.0625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 7/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 850287.2763 - mse: 365395.6009 - mae: 398.4520 - val_loss: 582770.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 8/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 664977.6667 - mse: 188885.9057 - mae: 357.3248 - val_loss: 582219.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 9/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 674624.7708 - mse: 187497.5387 - mae: 374.1650 - val_loss: 572474.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 10/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 695498.5417 - mse: 238485.6499 - mae: 368.8465 - val_loss: 566017.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 11/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 685025.0132 - mse: 218094.2023 - mae: 355.8458 - val_loss: 562079.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 12/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 721750.0680 - mse: 263755.0535 - mae: 383.1434 - val_loss: 558004.8750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 13/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 622300.7961 - mse: 174613.8624 - mae: 355.9820 - val_loss: 550124.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 14/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 668347.9825 - mse: 214324.0644 - mae: 372.3286 - val_loss: 546048.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 15/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 603764.9759 - mse: 167061.1793 - mae: 346.0659 - val_loss: 539021.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 16/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 846701.7325 - mse: 410614.3051 - mae: 383.1774 - val_loss: 534617.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 17/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 602258.7730 - mse: 187655.0872 - mae: 366.9071 - val_loss: 532586.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 18/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 599700.1842 - mse: 168066.2012 - mae: 349.5052 - val_loss: 529045.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 19/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 659755.7259 - mse: 250411.4030 - mae: 388.4278 - val_loss: 535269.0000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 20/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 782689.1420 - mse: 356613.8709 - mae: 373.8715 - val_loss: 540837.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 21/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 570328.3520 - mse: 162419.7566 - mae: 344.7251 - val_loss: 533714.0625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 22/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 571992.5466 - mse: 168713.1118 - mae: 355.0142 - val_loss: 527993.0000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 23/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 546248.2357 - mse: 163111.9731 - mae: 340.5348 - val_loss: 523399.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 24/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 617729.4375 - mse: 196667.9211 - mae: 360.9986 - val_loss: 525305.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 25/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 575810.8399 - mse: 177548.1280 - mae: 354.3213 - val_loss: 520370.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 26/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 610001.5066 - mse: 193449.1554 - mae: 371.2998 - val_loss: 515894.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 27/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 590003.4616 - mse: 191236.9320 - mae: 363.7928 - val_loss: 512653.9062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 28/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 537964.6985 - mse: 162507.9615 - mae: 344.5545 - val_loss: 507446.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 29/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 545790.8394 - mse: 167631.6823 - mae: 349.7190 - val_loss: 502943.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 30/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 623644.6541 - mse: 262909.8986 - mae: 346.3563 - val_loss: 513172.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 31/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 568428.7198 - mse: 172839.5663 - mae: 354.3039 - val_loss: 509505.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 32/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 565772.6743 - mse: 178243.3361 - mae: 362.9132 - val_loss: 504447.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 33/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 544469.5976 - mse: 176352.1916 - mae: 352.7095 - val_loss: 497984.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 34/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 547815.7346 - mse: 172610.4131 - mae: 354.4085 - val_loss: 492280.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 35/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 550437.8761 - mse: 179636.1982 - mae: 350.2294 - val_loss: 486784.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 36/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 561657.6590 - mse: 222113.0513 - mae: 382.3404 - val_loss: 489233.8750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 37/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 518115.1255 - mse: 168298.0200 - mae: 353.4376 - val_loss: 483091.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 38/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 540767.8394 - mse: 185450.4191 - mae: 368.9487 - val_loss: 480598.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 39/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 563399.7928 - mse: 217711.2692 - mae: 364.3163 - val_loss: 488795.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 40/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 511996.8734 - mse: 161312.4153 - mae: 344.8196 - val_loss: 482334.5312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 41/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 634714.0773 - mse: 275603.8991 - mae: 384.2667 - val_loss: 477129.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 42/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 545859.5795 - mse: 200652.3991 - mae: 362.3350 - val_loss: 472984.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 43/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 513670.1584 - mse: 176669.9649 - mae: 364.8105 - val_loss: 467303.5938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 44/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 497713.3399 - mse: 178114.2492 - mae: 359.6226 - val_loss: 461780.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 45/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 481342.7462 - mse: 167917.0027 - mae: 353.7992 - val_loss: 456701.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 46/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 607885.3558 - mse: 301571.2198 - mae: 374.0165 - val_loss: 456546.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 47/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 482333.3026 - mse: 182110.9123 - mae: 370.1882 - val_loss: 452124.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 48/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 472015.9254 - mse: 169130.9889 - mae: 352.4742 - val_loss: 449017.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 49/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 465734.0312 - mse: 160441.6039 - mae: 341.2258 - val_loss: 444122.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 50/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 467788.0740 - mse: 173974.4539 - mae: 352.1619 - val_loss: 442886.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 51/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 463797.4923 - mse: 175197.5208 - mae: 357.1392 - val_loss: 438599.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 52/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 456078.5274 - mse: 173796.5433 - mae: 359.6151 - val_loss: 434399.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 53/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 453228.4468 - mse: 175542.6735 - mae: 357.7447 - val_loss: 430132.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 54/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 512981.0247 - mse: 239660.3695 - mae: 365.2797 - val_loss: 430588.0000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 55/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 430773.6003 - mse: 152354.8224 - mae: 329.9633 - val_loss: 426517.9688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 56/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 447822.7182 - mse: 176981.1288 - mae: 357.8523 - val_loss: 421904.8750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 57/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 431319.9633 - mse: 169751.8002 - mae: 354.8644 - val_loss: 417799.3125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 58/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 434467.1453 - mse: 181165.2018 - mae: 371.2853 - val_loss: 413708.9688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 59/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 427936.8750 - mse: 172662.0894 - mae: 354.8781 - val_loss: 409981.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 60/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 418867.8070 - mse: 164750.8657 - mae: 344.6774 - val_loss: 406186.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 61/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 402966.1601 - mse: 166337.0562 - mae: 346.3245 - val_loss: 402399.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 62/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 416620.1360 - mse: 171841.1135 - mae: 354.0920 - val_loss: 398804.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 63/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 388352.1173 - mse: 162478.7763 - mae: 342.2959 - val_loss: 395280.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 64/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 385711.8306 - mse: 161868.7462 - mae: 342.9202 - val_loss: 391609.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 65/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 394609.8229 - mse: 170906.4342 - mae: 354.1628 - val_loss: 387850.5938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 66/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 401930.8564 - mse: 177888.5140 - mae: 364.9502 - val_loss: 384380.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 67/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 386561.3931 - mse: 177320.4073 - mae: 362.5052 - val_loss: 380677.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 68/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 386241.8191 - mse: 180933.0033 - mae: 351.5033 - val_loss: 377564.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 69/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 368075.4419 - mse: 165514.2700 - mae: 345.8296 - val_loss: 374427.3438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 70/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 486004.6661 - mse: 278387.2059 - mae: 376.9687 - val_loss: 373179.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 71/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 371678.6990 - mse: 173508.0707 - mae: 358.0740 - val_loss: 369907.2812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 72/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 387096.0866 - mse: 190811.5874 - mae: 369.7261 - val_loss: 367137.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 73/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 368531.8514 - mse: 175740.9104 - mae: 364.1686 - val_loss: 363985.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 74/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 375123.0471 - mse: 179027.6941 - mae: 363.3886 - val_loss: 360836.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 75/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 366836.8745 - mse: 175690.7357 - mae: 353.3026 - val_loss: 357267.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 76/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 341816.9792 - mse: 162079.6911 - mae: 343.5118 - val_loss: 353925.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 77/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 343759.3163 - mse: 164770.3676 - mae: 346.4512 - val_loss: 350549.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 78/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 339630.0850 - mse: 169408.2133 - mae: 352.4921 - val_loss: 346292.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 79/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 504416.4890 - mse: 336739.8232 - mae: 374.7496 - val_loss: 343539.0000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 80/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 320684.6645 - mse: 162336.8235 - mae: 341.3018 - val_loss: 340464.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 81/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 335266.9095 - mse: 174316.6151 - mae: 358.7713 - val_loss: 337156.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 82/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 333343.6880 - mse: 178355.1697 - mae: 365.7347 - val_loss: 334298.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 83/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 327440.8734 - mse: 171488.5236 - mae: 357.3757 - val_loss: 331094.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 84/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 316291.6055 - mse: 162869.2869 - mae: 342.2505 - val_loss: 328009.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 85/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 312190.5444 - mse: 167596.8772 - mae: 348.4828 - val_loss: 324865.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 86/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 312228.3640 - mse: 164421.1181 - mae: 345.3655 - val_loss: 321822.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 87/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 313392.2862 - mse: 166211.9825 - mae: 347.2820 - val_loss: 318632.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 88/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 328423.2029 - mse: 186599.8215 - mae: 365.4429 - val_loss: 315777.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 89/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 304846.9978 - mse: 170014.8476 - mae: 351.8191 - val_loss: 312902.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 90/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 312642.6765 - mse: 179756.7067 - mae: 370.0706 - val_loss: 309872.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 91/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 325177.1831 - mse: 191139.9180 - mae: 376.3600 - val_loss: 306661.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 92/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 292829.8851 - mse: 166007.1632 - mae: 345.3778 - val_loss: 303534.2812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 93/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 296478.8525 - mse: 172686.3024 - mae: 358.1902 - val_loss: 300407.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 94/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 286782.7108 - mse: 165553.8424 - mae: 347.3355 - val_loss: 297256.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 95/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 286123.7039 - mse: 171298.3643 - mae: 353.4618 - val_loss: 294379.9688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 96/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 269889.1721 - mse: 159954.3158 - mae: 338.2229 - val_loss: 291541.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 97/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 271589.1494 - mse: 160381.9123 - mae: 337.9203 - val_loss: 289211.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 98/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 277914.8539 - mse: 169248.2434 - mae: 351.2450 - val_loss: 286160.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 99/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 286536.7730 - mse: 179798.8309 - mae: 364.4353 - val_loss: 283229.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 100/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 273365.3117 - mse: 168283.8742 - mae: 348.9947 - val_loss: 280351.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 101/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 267838.1754 - mse: 169022.9975 - mae: 349.4470 - val_loss: 277577.8438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 102/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 268592.0027 - mse: 172681.9646 - mae: 352.1504 - val_loss: 274769.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 103/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 259113.7851 - mse: 166083.0585 - mae: 344.3436 - val_loss: 272007.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 104/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 267010.0403 - mse: 176395.4301 - mae: 356.7060 - val_loss: 269900.2188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 105/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 248305.5115 - mse: 161031.0825 - mae: 341.1665 - val_loss: 267421.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 106/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 259954.6798 - mse: 176656.0517 - mae: 352.6953 - val_loss: 266325.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 107/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 246873.0888 - mse: 162548.9700 - mae: 340.0072 - val_loss: 263604.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 108/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 252612.0488 - mse: 169891.7648 - mae: 351.1184 - val_loss: 261247.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 109/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 242482.9175 - mse: 164779.6990 - mae: 344.9945 - val_loss: 258675.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 110/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 233486.3791 - mse: 157924.3191 - mae: 339.1428 - val_loss: 256880.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 111/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 264998.3750 - mse: 189996.6116 - mae: 360.8322 - val_loss: 254876.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 112/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 256897.7050 - mse: 181968.8566 - mae: 369.6007 - val_loss: 252215.8594 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 113/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 259097.6316 - mse: 187068.9232 - mae: 371.1012 - val_loss: 250150.5938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 114/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 237949.7610 - mse: 166244.2081 - mae: 348.3561 - val_loss: 247690.4531 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 115/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 239402.6916 - mse: 172053.2961 - mae: 356.4294 - val_loss: 245330.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 116/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 239229.5740 - mse: 174130.7911 - mae: 360.0465 - val_loss: 243293.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 117/300\n",
      "56/56 [==============================] - ETA: 0s - loss: 229714.3795 - mse: 167981.8427 - mae: 351.715 - 0s 4ms/step - loss: 230590.0428 - mse: 168734.6417 - mae: 352.3273 - val_loss: 240942.2656 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 118/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 223414.6883 - mse: 164422.7135 - mae: 349.8150 - val_loss: 239460.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 119/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 218918.1584 - mse: 159986.6989 - mae: 334.6269 - val_loss: 237836.0781 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 120/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 229425.7703 - mse: 172040.6302 - mae: 358.3002 - val_loss: 235635.1719 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 121/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 228641.6812 - mse: 174681.1524 - mae: 358.8634 - val_loss: 233810.2656 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 122/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 231978.0806 - mse: 179315.8004 - mae: 365.4948 - val_loss: 231708.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 123/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 226026.5285 - mse: 177058.0694 - mae: 363.8601 - val_loss: 229818.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 124/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 226119.3928 - mse: 177000.9704 - mae: 358.2937 - val_loss: 228329.6094 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 125/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 207720.5702 - mse: 161787.5853 - mae: 344.7521 - val_loss: 226306.7031 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 126/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 215436.9463 - mse: 170815.2599 - mae: 359.0774 - val_loss: 224317.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 127/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 229653.5271 - mse: 186668.0948 - mae: 369.0068 - val_loss: 222945.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 128/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 220735.6414 - mse: 179432.5581 - mae: 363.0586 - val_loss: 221190.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 129/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 196814.7198 - mse: 158742.7301 - mae: 337.7471 - val_loss: 219781.8750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 130/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 209209.5913 - mse: 172235.8755 - mae: 359.0721 - val_loss: 218188.7344 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 131/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 213808.6332 - mse: 178481.9208 - mae: 363.2195 - val_loss: 217469.5156 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 132/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 206385.3183 - mse: 171459.4230 - mae: 359.4635 - val_loss: 216190.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 133/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 215090.8262 - mse: 182891.1963 - mae: 371.9196 - val_loss: 214826.4375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 134/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 193872.7141 - mse: 162352.1667 - mae: 344.9154 - val_loss: 213211.0156 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 135/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 203063.7382 - mse: 173824.9575 - mae: 357.4600 - val_loss: 211778.5469 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 136/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 206684.4507 - mse: 176785.6116 - mae: 359.1351 - val_loss: 210495.0781 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 137/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 204323.9545 - mse: 175977.0118 - mae: 358.2527 - val_loss: 209135.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 138/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 189827.8438 - mse: 163584.3986 - mae: 342.9149 - val_loss: 208750.5781 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 139/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 186565.3320 - mse: 161543.0615 - mae: 343.2569 - val_loss: 207940.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 140/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 185552.8215 - mse: 160900.6856 - mae: 338.5861 - val_loss: 206772.5156 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 141/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 193622.9912 - mse: 169874.9778 - mae: 356.1159 - val_loss: 205848.7969 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 142/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 185398.0480 - mse: 162665.1727 - mae: 344.5956 - val_loss: 181239.1406 - val_mse: 159769.3438 - val_mae: 345.9311\n",
      "Epoch 143/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 185758.0768 - mse: 164416.9539 - mae: 342.0023 - val_loss: 202470.2188 - val_mse: 181344.6562 - val_mae: 373.2367\n",
      "Epoch 144/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 184018.8396 - mse: 162312.7072 - mae: 342.9626 - val_loss: 204384.1875 - val_mse: 183182.4531 - val_mae: 375.6664\n",
      "Epoch 145/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 188759.6099 - mse: 167673.6414 - mae: 345.6824 - val_loss: 203884.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 146/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 173061.7262 - mse: 152781.9076 - mae: 328.4807 - val_loss: 130059.9062 - val_mse: 110900.2578 - val_mae: 277.7914\n",
      "Epoch 147/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 258820.8268 - mse: 239974.3835 - mae: 357.0694 - val_loss: 124927.0156 - val_mse: 107003.0781 - val_mae: 271.2337\n",
      "Epoch 148/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 158921.0803 - mse: 141228.7977 - mae: 308.8672 - val_loss: 109897.7500 - val_mse: 92775.5703 - val_mae: 253.1134\n",
      "Epoch 149/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 150567.5809 - mse: 133620.3062 - mae: 295.3121 - val_loss: 103348.1172 - val_mse: 86696.0234 - val_mae: 244.6927\n",
      "Epoch 150/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 173447.7815 - mse: 157106.3255 - mae: 320.8294 - val_loss: 127250.4922 - val_mse: 108420.5938 - val_mae: 275.1842\n",
      "Epoch 151/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 150154.3853 - mse: 131207.0171 - mae: 297.5571 - val_loss: 100971.8984 - val_mse: 82666.7109 - val_mae: 238.4529\n",
      "Epoch 152/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 132289.5742 - mse: 114842.5439 - mae: 279.0329 - val_loss: 112163.6719 - val_mse: 94202.5859 - val_mae: 255.4741\n",
      "Epoch 153/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 142216.3407 - mse: 124560.5299 - mae: 287.7853 - val_loss: 104405.7109 - val_mse: 87055.5938 - val_mae: 245.5603\n",
      "Epoch 154/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 142496.5484 - mse: 125948.7052 - mae: 292.1004 - val_loss: 110089.3438 - val_mse: 94188.3672 - val_mae: 255.9768\n",
      "Epoch 155/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 137965.7500 - mse: 122559.5280 - mae: 295.5779 - val_loss: 101284.0234 - val_mse: 86489.3672 - val_mae: 243.5294\n",
      "Epoch 156/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 138911.6165 - mse: 124679.0197 - mae: 290.2486 - val_loss: 93466.6250 - val_mse: 79583.7109 - val_mae: 233.0106\n",
      "Epoch 157/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 120915.6776 - mse: 107079.5666 - mae: 263.1741 - val_loss: 94935.1797 - val_mse: 81649.5938 - val_mae: 236.6710\n",
      "Epoch 158/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 108245.3861 - mse: 95401.1715 - mae: 250.6835 - val_loss: 75454.2891 - val_mse: 62730.3320 - val_mae: 207.3017\n",
      "Epoch 159/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 117445.7242 - mse: 104781.6449 - mae: 259.2010 - val_loss: 76515.9062 - val_mse: 64765.8828 - val_mae: 210.4903\n",
      "Epoch 160/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 113439.5704 - mse: 101808.7223 - mae: 255.7994 - val_loss: 88012.4453 - val_mse: 77200.0078 - val_mae: 231.0006\n",
      "Epoch 161/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 104461.7223 - mse: 93987.7854 - mae: 244.4348 - val_loss: 68475.3281 - val_mse: 58323.9883 - val_mae: 200.9791\n",
      "Epoch 162/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 98924.8688 - mse: 88812.5484 - mae: 237.9146 - val_loss: 70503.7812 - val_mse: 60458.1953 - val_mae: 204.8274\n",
      "Epoch 163/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 134199.8383 - mse: 124376.4544 - mae: 257.5937 - val_loss: 72244.4922 - val_mse: 62433.7891 - val_mae: 208.3139\n",
      "Epoch 164/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 108505.4171 - mse: 98962.8300 - mae: 255.2013 - val_loss: 85279.3359 - val_mse: 76107.6953 - val_mae: 229.9569\n",
      "Epoch 165/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 101656.1878 - mse: 92860.8194 - mae: 240.1672 - val_loss: 63365.8555 - val_mse: 54596.0898 - val_mae: 195.9930\n",
      "Epoch 166/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 102851.6113 - mse: 94911.5759 - mae: 248.1003 - val_loss: 72319.7188 - val_mse: 64338.6172 - val_mae: 210.4971\n",
      "Epoch 167/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 93742.3566 - mse: 86193.7588 - mae: 238.4798 - val_loss: 70706.4922 - val_mse: 63308.1445 - val_mae: 208.7529\n",
      "Epoch 168/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 98964.7015 - mse: 91891.3809 - mae: 245.1321 - val_loss: 75690.3125 - val_mse: 69026.3047 - val_mae: 217.2062\n",
      "Epoch 169/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 109006.8823 - mse: 102582.4138 - mae: 258.6623 - val_loss: 80211.4297 - val_mse: 74027.4688 - val_mae: 224.7805\n",
      "Epoch 170/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 98237.8654 - mse: 91889.9824 - mae: 241.9937 - val_loss: 79213.9766 - val_mse: 73568.2031 - val_mae: 223.0686\n",
      "Epoch 171/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 104367.3429 - mse: 98792.2326 - mae: 254.8812 - val_loss: 58720.4141 - val_mse: 52636.4062 - val_mae: 187.8451\n",
      "Epoch 172/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 111947.8006 - mse: 105652.3502 - mae: 257.2625 - val_loss: 77953.0000 - val_mse: 71628.2578 - val_mae: 221.6346\n",
      "Epoch 173/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 84271.0127 - mse: 78175.0049 - mae: 226.4568 - val_loss: 78186.0547 - val_mse: 72317.1953 - val_mae: 223.1783\n",
      "Epoch 174/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 81994.4034 - mse: 76234.2467 - mae: 222.6054 - val_loss: 62740.2031 - val_mse: 57145.9922 - val_mae: 197.3947\n",
      "Epoch 175/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 93839.8217 - mse: 88371.0288 - mae: 241.7967 - val_loss: 73846.2969 - val_mse: 68773.4375 - val_mae: 215.6347\n",
      "Epoch 176/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 90242.2777 - mse: 85120.8747 - mae: 231.4117 - val_loss: 65871.5781 - val_mse: 61105.9961 - val_mae: 202.2231\n",
      "Epoch 177/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 80951.1798 - mse: 76163.9237 - mae: 218.8408 - val_loss: 62375.1797 - val_mse: 57446.2227 - val_mae: 195.7533\n",
      "Epoch 178/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 88998.7810 - mse: 83991.1062 - mae: 239.8912 - val_loss: 58445.8633 - val_mse: 53746.3320 - val_mae: 191.0080\n",
      "Epoch 179/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 88553.8054 - mse: 83608.4449 - mae: 234.7752 - val_loss: 71802.7109 - val_mse: 67537.4766 - val_mae: 213.6762\n",
      "Epoch 180/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 88943.0650 - mse: 84963.7377 - mae: 238.4067 - val_loss: 77869.5234 - val_mse: 74099.6250 - val_mae: 225.8326\n",
      "Epoch 181/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 88633.4895 - mse: 84862.9666 - mae: 236.4651 - val_loss: 69918.1562 - val_mse: 66028.2891 - val_mae: 211.8322\n",
      "Epoch 182/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 91876.6524 - mse: 87952.8568 - mae: 236.6384 - val_loss: 79535.9844 - val_mse: 75882.8047 - val_mae: 229.1908\n",
      "Epoch 183/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 78177.7357 - mse: 74183.2560 - mae: 213.4481 - val_loss: 65544.0234 - val_mse: 61584.4727 - val_mae: 205.4689\n",
      "Epoch 184/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 88809.2755 - mse: 84729.5629 - mae: 233.5513 - val_loss: 58818.5508 - val_mse: 54715.6953 - val_mae: 193.4366\n",
      "Epoch 185/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 80576.0526 - mse: 76662.9703 - mae: 220.9836 - val_loss: 52524.0195 - val_mse: 48551.8789 - val_mae: 183.2717\n",
      "Epoch 186/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 89041.2282 - mse: 85552.8397 - mae: 234.8565 - val_loss: 73118.4375 - val_mse: 69685.7578 - val_mae: 219.2193\n",
      "Epoch 187/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 82994.4899 - mse: 79447.5016 - mae: 228.0516 - val_loss: 65428.6836 - val_mse: 61780.7031 - val_mae: 205.2391\n",
      "Epoch 188/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 86407.0448 - mse: 82769.9973 - mae: 231.9028 - val_loss: 75244.3906 - val_mse: 71680.6484 - val_mae: 222.2943\n",
      "Epoch 189/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 78469.1613 - mse: 74962.3636 - mae: 220.1947 - val_loss: 75469.8516 - val_mse: 72207.7891 - val_mae: 223.2045\n",
      "Epoch 190/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 85171.8007 - mse: 81743.1885 - mae: 232.0790 - val_loss: 67004.0078 - val_mse: 63755.3438 - val_mae: 208.6368\n",
      "Epoch 191/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 82925.8250 - mse: 79622.1712 - mae: 226.6078 - val_loss: 81079.7891 - val_mse: 78159.3516 - val_mae: 232.8043\n",
      "Epoch 192/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 92613.2174 - mse: 89840.1096 - mae: 243.9410 - val_loss: 67015.7266 - val_mse: 63816.2109 - val_mae: 210.3400\n",
      "Epoch 193/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 89794.3694 - mse: 86781.0234 - mae: 235.1737 - val_loss: 86752.5234 - val_mse: 84028.4453 - val_mae: 241.1585\n",
      "Epoch 194/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 87993.0416 - mse: 85069.4105 - mae: 231.1897 - val_loss: 77810.9922 - val_mse: 74403.4609 - val_mae: 226.9755\n",
      "Epoch 195/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 87508.3029 - mse: 84014.2058 - mae: 233.3838 - val_loss: 61835.9805 - val_mse: 58442.3477 - val_mae: 200.7978\n",
      "Epoch 196/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 90107.7532 - mse: 86988.5421 - mae: 232.2122 - val_loss: 68964.0234 - val_mse: 65957.0938 - val_mae: 211.6966\n",
      "Epoch 197/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 85573.4742 - mse: 82695.1598 - mae: 237.8393 - val_loss: 69192.2578 - val_mse: 66263.0703 - val_mae: 213.7339\n",
      "Epoch 198/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65291.1365 - mse: 62221.3134 - mae: 200.4605 - val_loss: 70941.6875 - val_mse: 67992.9531 - val_mae: 216.7205\n",
      "Epoch 199/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75548.2553 - mse: 72891.4616 - mae: 223.4047 - val_loss: 66946.1016 - val_mse: 64056.7734 - val_mae: 208.9292\n",
      "Epoch 200/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 89944.9343 - mse: 87246.3370 - mae: 236.3043 - val_loss: 60939.9219 - val_mse: 58192.5273 - val_mae: 199.1178\n",
      "Epoch 201/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 92117.3503 - mse: 89554.6035 - mae: 238.5222 - val_loss: 62637.5547 - val_mse: 59797.0859 - val_mae: 201.5377\n",
      "Epoch 202/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 80781.3558 - mse: 77907.9235 - mae: 230.2232 - val_loss: 68381.6953 - val_mse: 65514.9766 - val_mae: 211.0985\n",
      "Epoch 203/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 77974.5613 - mse: 75119.9141 - mae: 224.8688 - val_loss: 88245.6406 - val_mse: 85961.4766 - val_mae: 243.0152\n",
      "Epoch 204/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 85884.7018 - mse: 83646.0955 - mae: 235.4438 - val_loss: 74516.0391 - val_mse: 72182.3828 - val_mae: 221.1427\n",
      "Epoch 205/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 79965.0791 - mse: 77640.9963 - mae: 223.6399 - val_loss: 74611.5859 - val_mse: 72236.6719 - val_mae: 221.8895\n",
      "Epoch 206/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71766.7391 - mse: 69544.8597 - mae: 209.5811 - val_loss: 79634.4219 - val_mse: 77493.8281 - val_mae: 230.5164\n",
      "Epoch 207/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71673.8938 - mse: 69522.7362 - mae: 213.0606 - val_loss: 74083.0000 - val_mse: 72050.7344 - val_mae: 222.0259\n",
      "Epoch 208/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 84931.3764 - mse: 82823.7887 - mae: 232.6825 - val_loss: 72929.3203 - val_mse: 70756.1797 - val_mae: 219.6449\n",
      "Epoch 209/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 76373.0145 - mse: 74199.7147 - mae: 214.5752 - val_loss: 77623.5234 - val_mse: 75394.3984 - val_mae: 227.0807\n",
      "Epoch 210/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71405.5654 - mse: 69257.6939 - mae: 218.7721 - val_loss: 57064.3281 - val_mse: 54561.1133 - val_mae: 194.4268\n",
      "Epoch 211/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 79716.1621 - mse: 77400.3749 - mae: 225.3642 - val_loss: 62897.1523 - val_mse: 60512.1719 - val_mae: 203.7644\n",
      "Epoch 212/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 86545.4949 - mse: 84237.8686 - mae: 237.9627 - val_loss: 69186.9141 - val_mse: 66905.3438 - val_mae: 214.5443\n",
      "Epoch 213/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65608.3567 - mse: 63336.5418 - mae: 198.9149 - val_loss: 69981.1250 - val_mse: 67835.2188 - val_mae: 215.8474\n",
      "Epoch 214/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70590.5230 - mse: 68500.8293 - mae: 208.2029 - val_loss: 69979.2891 - val_mse: 67979.0156 - val_mae: 216.1857\n",
      "Epoch 215/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70062.0127 - mse: 68087.2623 - mae: 211.5020 - val_loss: 72542.4453 - val_mse: 70429.9141 - val_mae: 220.5434\n",
      "Epoch 216/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 76296.3813 - mse: 74176.7553 - mae: 217.6973 - val_loss: 75147.6016 - val_mse: 73151.3125 - val_mae: 224.2782\n",
      "Epoch 217/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 76502.2963 - mse: 74683.0040 - mae: 232.7463 - val_loss: 65179.0195 - val_mse: 63171.2422 - val_mae: 208.4362\n",
      "Epoch 218/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64967.7318 - mse: 63192.7729 - mae: 200.0618 - val_loss: 72817.6953 - val_mse: 71089.4531 - val_mae: 221.7160\n",
      "Epoch 219/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 86892.9775 - mse: 85197.6671 - mae: 234.6218 - val_loss: 76728.5859 - val_mse: 75109.7031 - val_mae: 227.7563\n",
      "Epoch 220/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72404.8664 - mse: 70639.4696 - mae: 213.5935 - val_loss: 95602.0625 - val_mse: 94201.6875 - val_mae: 256.6385\n",
      "Epoch 221/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67460.7561 - mse: 65899.4045 - mae: 206.4122 - val_loss: 101663.8672 - val_mse: 100176.0703 - val_mae: 265.2527\n",
      "Epoch 222/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 67689.9868 - mse: 66046.2347 - mae: 205.9026 - val_loss: 59259.2383 - val_mse: 57100.4766 - val_mae: 197.3529\n",
      "Epoch 223/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64213.0453 - mse: 62144.5022 - mae: 198.3427 - val_loss: 87183.6484 - val_mse: 85541.8984 - val_mae: 243.7953\n",
      "Epoch 224/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70818.2935 - mse: 69133.6238 - mae: 212.2703 - val_loss: 77505.2422 - val_mse: 75857.2109 - val_mae: 228.6070\n",
      "Epoch 225/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71048.2675 - mse: 69481.8330 - mae: 218.6420 - val_loss: 82231.0078 - val_mse: 80435.8359 - val_mae: 236.0428\n",
      "Epoch 226/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64327.9187 - mse: 62529.7548 - mae: 203.1642 - val_loss: 67455.4219 - val_mse: 65620.5156 - val_mae: 211.8044\n",
      "Epoch 227/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73359.5761 - mse: 71571.7735 - mae: 222.9290 - val_loss: 78980.5312 - val_mse: 77269.7422 - val_mae: 231.3503\n",
      "Epoch 228/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72159.2363 - mse: 70456.7255 - mae: 214.3823 - val_loss: 71520.0156 - val_mse: 69758.4297 - val_mae: 219.5612\n",
      "Epoch 229/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72434.3373 - mse: 70802.8520 - mae: 217.2053 - val_loss: 69604.4141 - val_mse: 68058.9688 - val_mae: 215.9586\n",
      "Epoch 230/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 76896.2078 - mse: 75367.9718 - mae: 219.2649 - val_loss: 93755.6953 - val_mse: 92597.1953 - val_mae: 254.3937\n",
      "Epoch 231/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70974.3074 - mse: 69688.1989 - mae: 215.0149 - val_loss: 72330.0391 - val_mse: 70658.2812 - val_mae: 220.3593\n",
      "Epoch 232/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66564.4333 - mse: 65025.8370 - mae: 209.2219 - val_loss: 89465.8125 - val_mse: 87955.5938 - val_mae: 247.6887\n",
      "Epoch 233/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63138.6406 - mse: 61564.1937 - mae: 203.4998 - val_loss: 72576.4609 - val_mse: 70987.8750 - val_mae: 220.8567\n",
      "Epoch 234/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65975.6432 - mse: 64439.5556 - mae: 202.6074 - val_loss: 78867.2422 - val_mse: 77468.5859 - val_mae: 231.6427\n",
      "Epoch 235/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67045.2970 - mse: 65496.6787 - mae: 208.9171 - val_loss: 71561.6094 - val_mse: 69957.3281 - val_mae: 219.0854\n",
      "Epoch 236/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60264.2597 - mse: 58655.1285 - mae: 195.5293 - val_loss: 97380.3359 - val_mse: 96133.5156 - val_mae: 259.7173\n",
      "Epoch 237/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69907.9707 - mse: 68586.2982 - mae: 212.1741 - val_loss: 88349.8906 - val_mse: 87162.6562 - val_mae: 246.2074\n",
      "Epoch 238/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 68238.6186 - mse: 66934.7823 - mae: 215.9680 - val_loss: 79083.2188 - val_mse: 77786.8281 - val_mae: 231.8879\n",
      "Epoch 239/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66590.4164 - mse: 65404.9942 - mae: 207.4762 - val_loss: 80875.2500 - val_mse: 79625.0547 - val_mae: 234.8256\n",
      "Epoch 240/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75355.5722 - mse: 74200.6201 - mae: 220.2718 - val_loss: 83593.0156 - val_mse: 82396.9375 - val_mae: 239.0941\n",
      "Epoch 241/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71064.7732 - mse: 69804.1756 - mae: 219.1641 - val_loss: 75223.1016 - val_mse: 73895.5781 - val_mae: 225.7232\n",
      "Epoch 242/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57562.9256 - mse: 56250.0856 - mae: 192.1970 - val_loss: 70760.6250 - val_mse: 69402.3672 - val_mae: 218.3173\n",
      "Epoch 243/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75277.0667 - mse: 73848.9785 - mae: 218.3866 - val_loss: 64529.6719 - val_mse: 63253.7305 - val_mae: 208.3307\n",
      "Epoch 244/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71505.1048 - mse: 70314.5588 - mae: 211.4855 - val_loss: 63692.7773 - val_mse: 62435.5312 - val_mae: 206.9622\n",
      "Epoch 245/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63759.8065 - mse: 62514.8374 - mae: 202.0812 - val_loss: 62010.2070 - val_mse: 60749.1719 - val_mae: 203.9189\n",
      "Epoch 246/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67470.1211 - mse: 66250.7477 - mae: 210.5007 - val_loss: 74081.3828 - val_mse: 72933.6016 - val_mae: 223.8597\n",
      "Epoch 247/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61589.4646 - mse: 60472.1488 - mae: 196.1963 - val_loss: 80689.7500 - val_mse: 79522.1797 - val_mae: 234.6862\n",
      "Epoch 248/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66889.1576 - mse: 65750.9907 - mae: 207.4849 - val_loss: 62477.9141 - val_mse: 61249.7344 - val_mae: 204.6693\n",
      "Epoch 249/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 74917.9951 - mse: 73726.6933 - mae: 224.5926 - val_loss: 77628.3984 - val_mse: 76425.4062 - val_mae: 229.1510\n",
      "Epoch 250/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 55807.5310 - mse: 54544.2844 - mae: 189.2923 - val_loss: 69012.2812 - val_mse: 67768.9141 - val_mae: 215.5093\n",
      "Epoch 251/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 60151.0497 - mse: 58887.8008 - mae: 191.1162 - val_loss: 71186.1719 - val_mse: 70057.4688 - val_mae: 219.3237\n",
      "Epoch 252/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59882.3161 - mse: 58718.4880 - mae: 194.5730 - val_loss: 70471.7422 - val_mse: 69219.3984 - val_mae: 217.9408\n",
      "Epoch 253/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71777.0271 - mse: 70633.1874 - mae: 213.0307 - val_loss: 89542.8281 - val_mse: 88491.0625 - val_mae: 246.9856\n",
      "Epoch 254/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69340.4031 - mse: 68257.3154 - mae: 211.7442 - val_loss: 72758.7109 - val_mse: 71659.0859 - val_mae: 221.6267\n",
      "Epoch 255/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69935.1789 - mse: 68829.0945 - mae: 216.1001 - val_loss: 65932.3047 - val_mse: 64888.5312 - val_mae: 210.6732\n",
      "Epoch 256/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71667.0663 - mse: 70728.8563 - mae: 221.2592 - val_loss: 63657.4062 - val_mse: 62424.2148 - val_mae: 206.6524\n",
      "Epoch 257/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66716.5188 - mse: 65586.0429 - mae: 204.6502 - val_loss: 82114.1797 - val_mse: 81199.1641 - val_mae: 237.1716\n",
      "Epoch 258/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71684.5232 - mse: 70759.7181 - mae: 218.6225 - val_loss: 81642.7656 - val_mse: 80657.4297 - val_mae: 236.4169\n",
      "Epoch 259/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65413.3055 - mse: 64378.4722 - mae: 212.4607 - val_loss: 71558.7188 - val_mse: 70465.9062 - val_mae: 220.0160\n",
      "Epoch 260/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69366.5344 - mse: 68238.3111 - mae: 213.4083 - val_loss: 74500.0703 - val_mse: 73475.9453 - val_mae: 224.9274\n",
      "Epoch 261/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 77160.7657 - mse: 76179.6897 - mae: 223.1811 - val_loss: 86844.8281 - val_mse: 85936.3438 - val_mae: 244.5467\n",
      "Epoch 262/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64189.8660 - mse: 63307.3973 - mae: 205.7527 - val_loss: 82500.0781 - val_mse: 81617.9922 - val_mae: 237.9468\n",
      "Epoch 263/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66335.3154 - mse: 65306.1082 - mae: 203.0000 - val_loss: 78665.8984 - val_mse: 77733.0938 - val_mae: 231.9116\n",
      "Epoch 264/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72024.4763 - mse: 71028.2926 - mae: 216.6068 - val_loss: 73460.6172 - val_mse: 72543.3906 - val_mae: 223.3666\n",
      "Epoch 265/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70933.1208 - mse: 70000.3837 - mae: 210.3505 - val_loss: 87371.4844 - val_mse: 86548.7109 - val_mae: 245.4489\n",
      "Epoch 266/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69107.2479 - mse: 68238.1402 - mae: 210.7777 - val_loss: 72647.3750 - val_mse: 71761.7188 - val_mae: 222.2986\n",
      "Epoch 267/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 75075.2201 - mse: 74233.1087 - mae: 221.4827 - val_loss: 78996.4219 - val_mse: 78178.4688 - val_mae: 232.6065\n",
      "Epoch 268/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59920.5105 - mse: 59048.0312 - mae: 194.8784 - val_loss: 76981.6484 - val_mse: 76188.8281 - val_mae: 229.3329\n",
      "Epoch 269/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70395.2584 - mse: 69461.7039 - mae: 212.5239 - val_loss: 68214.2344 - val_mse: 67205.0469 - val_mae: 214.5908\n",
      "Epoch 270/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66602.2124 - mse: 65672.5286 - mae: 204.8211 - val_loss: 64029.8750 - val_mse: 62960.1523 - val_mae: 207.4784\n",
      "Epoch 271/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63580.2746 - mse: 62575.4411 - mae: 198.3657 - val_loss: 81482.0547 - val_mse: 80441.0078 - val_mae: 236.2585\n",
      "Epoch 272/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73902.5356 - mse: 72791.2969 - mae: 221.0677 - val_loss: 80015.3516 - val_mse: 79114.6484 - val_mae: 234.0601\n",
      "Epoch 273/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63183.1428 - mse: 62254.1937 - mae: 201.5633 - val_loss: 68360.9766 - val_mse: 67368.8203 - val_mae: 214.7770\n",
      "Epoch 274/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63431.1008 - mse: 62533.9698 - mae: 197.2890 - val_loss: 76455.8125 - val_mse: 75551.9297 - val_mae: 228.2664\n",
      "Epoch 275/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73501.3490 - mse: 72538.3806 - mae: 212.2964 - val_loss: 72396.1797 - val_mse: 71370.6562 - val_mae: 221.4347\n",
      "Epoch 276/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71597.6852 - mse: 70613.2014 - mae: 214.7396 - val_loss: 74693.0859 - val_mse: 73763.9844 - val_mae: 225.2644\n",
      "Epoch 277/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71469.1275 - mse: 70611.7324 - mae: 216.2691 - val_loss: 72991.5781 - val_mse: 72035.8672 - val_mae: 222.6325\n",
      "Epoch 278/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73489.3646 - mse: 72509.0759 - mae: 215.5907 - val_loss: 79362.6406 - val_mse: 78517.5938 - val_mae: 233.0276\n",
      "Epoch 279/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 67522.6201 - mse: 66676.0815 - mae: 205.7876 - val_loss: 86828.0156 - val_mse: 86017.8594 - val_mae: 244.6325\n",
      "Epoch 280/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68615.5052 - mse: 67725.1509 - mae: 218.3768 - val_loss: 91640.8047 - val_mse: 90875.7734 - val_mae: 251.9560\n",
      "Epoch 281/300\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 61255.6240 - mse: 60341.4526 - mae: 199.8649 - val_loss: 74443.6250 - val_mse: 73520.0547 - val_mae: 225.0132\n",
      "Epoch 282/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65077.7649 - mse: 64198.4882 - mae: 208.4394 - val_loss: 76962.7656 - val_mse: 76083.1328 - val_mae: 229.2030\n",
      "Epoch 283/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57617.1348 - mse: 56699.6003 - mae: 190.2713 - val_loss: 80894.9453 - val_mse: 79940.4609 - val_mae: 235.2977\n",
      "Epoch 284/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64280.7789 - mse: 63365.5532 - mae: 207.0999 - val_loss: 69597.0078 - val_mse: 68662.1016 - val_mae: 216.7188\n",
      "Epoch 285/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 69468.1541 - mse: 68617.3031 - mae: 217.0706 - val_loss: 76338.5078 - val_mse: 75500.3438 - val_mae: 228.1288\n",
      "Epoch 286/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70142.1005 - mse: 69285.1305 - mae: 214.8956 - val_loss: 80937.1406 - val_mse: 80220.3594 - val_mae: 235.5015\n",
      "Epoch 287/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59548.8294 - mse: 58722.5866 - mae: 199.8080 - val_loss: 89023.5469 - val_mse: 88320.3125 - val_mae: 247.9070\n",
      "Epoch 288/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70332.7419 - mse: 69574.9317 - mae: 215.8506 - val_loss: 78572.3750 - val_mse: 77855.2734 - val_mae: 231.5536\n",
      "Epoch 289/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 67739.9966 - mse: 66922.1375 - mae: 209.0387 - val_loss: 84976.1172 - val_mse: 84214.9219 - val_mae: 241.6898\n",
      "Epoch 290/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60724.0827 - mse: 59853.7442 - mae: 196.6614 - val_loss: 84301.0625 - val_mse: 83601.2500 - val_mae: 240.8575\n",
      "Epoch 291/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60174.9384 - mse: 59412.8721 - mae: 195.9814 - val_loss: 70212.3672 - val_mse: 69384.4766 - val_mae: 217.9375\n",
      "Epoch 292/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61098.4977 - mse: 60290.2202 - mae: 196.6714 - val_loss: 81843.6562 - val_mse: 81021.2109 - val_mae: 236.8851\n",
      "Epoch 293/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 76369.6678 - mse: 75580.5543 - mae: 220.6899 - val_loss: 65461.2852 - val_mse: 64545.7461 - val_mae: 209.8896\n",
      "Epoch 294/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 77471.7354 - mse: 76622.2507 - mae: 224.8850 - val_loss: 71548.7188 - val_mse: 70627.9062 - val_mae: 220.1463\n",
      "Epoch 295/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62759.6484 - mse: 61780.1070 - mae: 208.5965 - val_loss: 91078.7578 - val_mse: 90395.8281 - val_mae: 251.1668\n",
      "Epoch 296/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 71023.5439 - mse: 70272.0524 - mae: 213.3093 - val_loss: 74352.9844 - val_mse: 73529.9688 - val_mae: 224.7657\n",
      "Epoch 297/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68133.1014 - mse: 67275.0861 - mae: 210.3898 - val_loss: 65983.5469 - val_mse: 65172.0469 - val_mae: 210.7899\n",
      "Epoch 298/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60667.2410 - mse: 59857.6796 - mae: 199.8203 - val_loss: 86926.3125 - val_mse: 86203.5391 - val_mae: 244.6779\n",
      "Epoch 299/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69778.9985 - mse: 69035.1825 - mae: 204.9984 - val_loss: 70507.6328 - val_mse: 69733.4609 - val_mae: 218.2370\n",
      "Epoch 300/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65396.0957 - mse: 64594.7940 - mae: 198.9257 - val_loss: 74347.5078 - val_mse: 73548.6484 - val_mae: 224.7325\n"
     ]
    }
   ],
   "source": [
    "history = nn.fit(X_train, y_train, epochs=300, batch_size=8, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(learning_rate = 0.01, activation = 'relu'):\n",
    "  \n",
    "    # Create an Adam optimizer with the given learning rate\n",
    "    opt = Adam(lr=learning_rate)\n",
    "  \n",
    "    # Create your binary classification model  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, \n",
    "                    activation = activation,\n",
    "                    input_shape = (16, ),\n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(128,\n",
    "                    activation = activation, \n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(1, activation = activation))\n",
    "# Compile the model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model,\n",
    "                       verbose=1)\n",
    "\n",
    "params = {'activation': [\"relu\"],\n",
    "          'batch_size': [16, 8], \n",
    "          'epochs': [200, 300, 500],\n",
    "          'learning_rate': [0.01, 0.05, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "35/35 [==============================] - 1s 3ms/step - loss: 118170499.3333 - mse: 117770849.0000 - mae: 4426.8821\n",
      "Epoch 2/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 608170.8160 - mse: 171743.2947 - mae: 356.8768\n",
      "Epoch 3/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 623767.0312 - mse: 184879.7847 - mae: 374.5880\n",
      "Epoch 4/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 648427.8594 - mse: 210208.5924 - mae: 370.6527\n",
      "Epoch 5/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 582489.9097 - mse: 170785.7031 - mae: 350.0122\n",
      "Epoch 6/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 544155.3038 - mse: 175045.6658 - mae: 360.4448\n",
      "Epoch 7/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 550819.5330 - mse: 182604.3116 - mae: 360.9632\n",
      "Epoch 8/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 535136.8785 - mse: 168940.4462 - mae: 352.9615\n",
      "Epoch 9/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 800194.9809 - mse: 427262.1944 - mae: 415.8399\n",
      "Epoch 10/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 598492.8715 - mse: 225264.9648 - mae: 373.5532\n",
      "Epoch 11/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 541795.9653 - mse: 160288.6471 - mae: 344.8545\n",
      "Epoch 12/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 552017.9184 - mse: 174371.9683 - mae: 358.4168\n",
      "Epoch 13/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 533016.9861 - mse: 169166.8963 - mae: 355.2807\n",
      "Epoch 14/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 525818.0616 - mse: 175257.9510 - mae: 362.9460\n",
      "Epoch 15/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 537541.7604 - mse: 181576.2096 - mae: 364.4749\n",
      "Epoch 16/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 537267.3212 - mse: 184767.8728 - mae: 370.3378\n",
      "Epoch 17/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 721707.0017 - mse: 385001.0599 - mae: 363.4014\n",
      "Epoch 18/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 511175.0000 - mse: 171447.9271 - mae: 360.1149\n",
      "Epoch 19/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 504702.2517 - mse: 160598.4284 - mae: 342.4595\n",
      "Epoch 20/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 513733.0503 - mse: 171357.6272 - mae: 356.3640\n",
      "Epoch 21/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 521031.7587 - mse: 173417.2218 - mae: 361.1565\n",
      "Epoch 22/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 530267.1606 - mse: 192264.2374 - mae: 364.0823\n",
      "Epoch 23/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 523308.6571 - mse: 182955.4957 - mae: 367.9885\n",
      "Epoch 24/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 516749.4167 - mse: 186123.9553 - mae: 371.1788\n",
      "Epoch 25/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 500212.3516 - mse: 171815.1615 - mae: 356.1075\n",
      "Epoch 26/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 593363.7309 - mse: 266903.6597 - mae: 372.5512\n",
      "Epoch 27/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 796435.4514 - mse: 458513.6697 - mae: 403.2574\n",
      "Epoch 28/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 495857.8620 - mse: 164314.7839 - mae: 349.0905\n",
      "Epoch 29/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 512130.0017 - mse: 179374.2374 - mae: 366.9998\n",
      "Epoch 30/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 495742.4688 - mse: 173470.9796 - mae: 355.8529\n",
      "Epoch 31/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 481682.3160 - mse: 172154.0469 - mae: 362.1875\n",
      "Epoch 32/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 470050.4141 - mse: 177850.7474 - mae: 357.3613\n",
      "Epoch 33/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 469253.2726 - mse: 176764.3863 - mae: 356.7491\n",
      "Epoch 34/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 445447.8481 - mse: 169483.8051 - mae: 356.2476\n",
      "Epoch 35/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 444887.0503 - mse: 177594.9848 - mae: 360.7181\n",
      "Epoch 36/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 435047.8082 - mse: 172167.0547 - mae: 358.5723\n",
      "Epoch 37/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 430597.0391 - mse: 173780.2309 - mae: 358.4412\n",
      "Epoch 38/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 447859.9757 - mse: 186191.4631 - mae: 369.5118\n",
      "Epoch 39/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 447864.9479 - mse: 183647.6398 - mae: 367.5357\n",
      "Epoch 40/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 435592.2422 - mse: 172150.9466 - mae: 354.8330\n",
      "Epoch 41/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 435728.2917 - mse: 174581.6923 - mae: 360.8645\n",
      "Epoch 42/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 425664.7092 - mse: 171095.4010 - mae: 355.7163\n",
      "Epoch 43/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 424230.2387 - mse: 170664.9553 - mae: 356.8711\n",
      "Epoch 44/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 413733.6988 - mse: 171223.9379 - mae: 358.3051\n",
      "Epoch 45/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 446544.0434 - mse: 199532.5855 - mae: 370.2220\n",
      "Epoch 46/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 420906.4141 - mse: 176340.2912 - mae: 359.5140\n",
      "Epoch 47/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 394922.7821 - mse: 162100.9970 - mae: 339.1564\n",
      "Epoch 48/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 427522.2049 - mse: 199757.8129 - mae: 370.8785\n",
      "Epoch 49/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 409323.1493 - mse: 180719.4405 - mae: 367.8475\n",
      "Epoch 50/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 411763.0729 - mse: 181094.1445 - mae: 365.5800\n",
      "Epoch 51/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 428766.0894 - mse: 198886.7648 - mae: 367.5730\n",
      "Epoch 52/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 417765.8906 - mse: 187691.7105 - mae: 370.6958\n",
      "Epoch 53/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 380029.3238 - mse: 165582.1372 - mae: 348.4831\n",
      "Epoch 54/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 408501.2335 - mse: 189564.5972 - mae: 366.1702\n",
      "Epoch 55/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 412119.2795 - mse: 193446.1536 - mae: 372.4399\n",
      "Epoch 56/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 397791.7066 - mse: 183024.0881 - mae: 360.6518\n",
      "Epoch 57/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 374569.5729 - mse: 169866.6788 - mae: 347.7997\n",
      "Epoch 58/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 374917.6736 - mse: 172209.5842 - mae: 358.9947\n",
      "Epoch 59/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 374778.0677 - mse: 172270.7878 - mae: 358.3655\n",
      "Epoch 60/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 362596.1328 - mse: 166724.1141 - mae: 351.7840\n",
      "Epoch 61/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 373833.4149 - mse: 172396.5868 - mae: 354.8869\n",
      "Epoch 62/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 372045.7196 - mse: 175529.0564 - mae: 360.5743\n",
      "Epoch 63/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 384407.8082 - mse: 183054.7908 - mae: 369.2765\n",
      "Epoch 64/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 375686.2222 - mse: 179605.2101 - mae: 365.1310\n",
      "Epoch 65/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 368458.3056 - mse: 173531.5760 - mae: 354.5343\n",
      "Epoch 66/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 366977.3168 - mse: 172950.4757 - mae: 354.8112\n",
      "Epoch 67/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 362523.8924 - mse: 172185.7118 - mae: 357.1014\n",
      "Epoch 68/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 382140.8333 - mse: 194839.7691 - mae: 371.8188\n",
      "Epoch 69/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 370833.6970 - mse: 190079.7947 - mae: 381.0349\n",
      "Epoch 70/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 348269.6571 - mse: 174602.8212 - mae: 357.1328\n",
      "Epoch 71/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 349131.0165 - mse: 171915.2101 - mae: 356.5106\n",
      "Epoch 72/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 349318.5130 - mse: 175228.5312 - mae: 363.4317\n",
      "Epoch 73/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 350553.2179 - mse: 178203.1797 - mae: 361.4296\n",
      "Epoch 74/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 383242.7552 - mse: 218071.7921 - mae: 382.7925\n",
      "Epoch 75/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 349068.0573 - mse: 172849.9418 - mae: 357.0877\n",
      "Epoch 76/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 333751.5356 - mse: 167735.7109 - mae: 347.2015\n",
      "Epoch 77/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 343128.5825 - mse: 179325.6120 - mae: 362.6955\n",
      "Epoch 78/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 348458.5139 - mse: 186583.3207 - mae: 364.7110\n",
      "Epoch 79/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 346140.2500 - mse: 179393.1610 - mae: 364.7940\n",
      "Epoch 80/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 340084.5425 - mse: 179905.3260 - mae: 365.0407\n",
      "Epoch 81/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 325450.4913 - mse: 168497.2266 - mae: 351.9939\n",
      "Epoch 82/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 319827.6406 - mse: 168473.7300 - mae: 347.0026\n",
      "Epoch 83/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 338414.2352 - mse: 180384.7765 - mae: 364.8203\n",
      "Epoch 84/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 317347.6111 - mse: 167989.4688 - mae: 353.8573\n",
      "Epoch 85/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 536501.5720 - mse: 383765.9488 - mae: 388.4616\n",
      "Epoch 86/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 320055.1155 - mse: 174735.4922 - mae: 357.9865\n",
      "Epoch 87/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 317287.4549 - mse: 174665.5026 - mae: 358.8744\n",
      "Epoch 88/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 336409.7491 - mse: 195091.8155 - mae: 365.1704\n",
      "Epoch 89/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 303846.3403 - mse: 164069.8112 - mae: 346.6326\n",
      "Epoch 90/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 306534.1814 - mse: 172458.3364 - mae: 359.0783\n",
      "Epoch 91/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 309425.7943 - mse: 176072.7292 - mae: 361.3189\n",
      "Epoch 92/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 306716.1953 - mse: 172232.0074 - mae: 351.7653\n",
      "Epoch 93/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 316079.0269 - mse: 184737.7695 - mae: 371.3355\n",
      "Epoch 94/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 305082.7886 - mse: 174974.8663 - mae: 362.1447\n",
      "Epoch 95/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 310637.3637 - mse: 181899.7088 - mae: 362.6925\n",
      "Epoch 96/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 289265.6814 - mse: 166392.0495 - mae: 346.7308\n",
      "Epoch 97/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 318983.2188 - mse: 194934.2556 - mae: 370.4684\n",
      "Epoch 98/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 286141.5677 - mse: 165662.5191 - mae: 346.8620\n",
      "Epoch 99/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 285988.9596 - mse: 172717.9592 - mae: 357.4906\n",
      "Epoch 100/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 293994.5382 - mse: 178812.4783 - mae: 362.7072\n",
      "Epoch 101/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 287805.4141 - mse: 176643.6445 - mae: 363.0491\n",
      "Epoch 102/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 289924.6745 - mse: 179241.9983 - mae: 364.7894\n",
      "Epoch 103/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 288189.4106 - mse: 175459.8442 - mae: 358.9456\n",
      "Epoch 104/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 278932.1215 - mse: 172265.2795 - mae: 355.2529\n",
      "Epoch 105/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 269160.0569 - mse: 166906.2999 - mae: 350.1240\n",
      "Epoch 106/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 293636.8464 - mse: 192821.1957 - mae: 378.6475\n",
      "Epoch 107/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 273155.3872 - mse: 169729.2018 - mae: 358.6210\n",
      "Epoch 108/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 284202.9089 - mse: 182206.5621 - mae: 372.9621\n",
      "Epoch 109/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 276391.2587 - mse: 178400.9800 - mae: 360.8724\n",
      "Epoch 110/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 281117.9761 - mse: 182017.1732 - mae: 367.0215\n",
      "Epoch 111/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 323492.7114 - mse: 225078.9609 - mae: 359.4126\n",
      "Epoch 112/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 264204.2192 - mse: 170503.0117 - mae: 352.6662\n",
      "Epoch 113/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 343390.9809 - mse: 249756.1155 - mae: 373.5404\n",
      "Epoch 114/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 268501.9323 - mse: 177528.0247 - mae: 368.5070\n",
      "Epoch 115/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 256757.1931 - mse: 169386.8689 - mae: 354.6956\n",
      "Epoch 116/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 252717.8069 - mse: 164823.6128 - mae: 349.5683\n",
      "Epoch 117/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 241308.0195 - mse: 158319.3898 - mae: 339.4960\n",
      "Epoch 118/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 256849.3490 - mse: 172193.1319 - mae: 352.0708\n",
      "Epoch 119/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 240065.9219 - mse: 159600.1137 - mae: 339.8003\n",
      "Epoch 120/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 241882.8498 - mse: 161638.4275 - mae: 342.4447\n",
      "Epoch 121/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 245850.1584 - mse: 165569.5482 - mae: 350.8778\n",
      "Epoch 122/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 236948.1584 - mse: 157906.1910 - mae: 340.1413\n",
      "Epoch 123/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 228732.6615 - mse: 152530.1400 - mae: 325.8314\n",
      "Epoch 124/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 241350.1832 - mse: 164962.9618 - mae: 345.6325\n",
      "Epoch 125/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 228798.8203 - mse: 154149.5330 - mae: 331.1516\n",
      "Epoch 126/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 224885.6979 - mse: 150811.8804 - mae: 331.8129\n",
      "Epoch 127/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 230227.3641 - mse: 155248.6385 - mae: 334.2396\n",
      "Epoch 128/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 237761.9067 - mse: 165441.9900 - mae: 343.6194\n",
      "Epoch 129/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 255707.6011 - mse: 188081.7574 - mae: 335.3446\n",
      "Epoch 130/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 207911.2422 - mse: 139728.4596 - mae: 310.4102\n",
      "Epoch 131/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 224904.0004 - mse: 155198.1758 - mae: 330.4021\n",
      "Epoch 132/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 234526.6515 - mse: 166169.8459 - mae: 346.0063\n",
      "Epoch 133/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 214006.5942 - mse: 148802.3628 - mae: 321.2171\n",
      "Epoch 134/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 217669.8095 - mse: 154067.9271 - mae: 332.4400\n",
      "Epoch 135/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 218717.9197 - mse: 154595.9549 - mae: 333.3483\n",
      "Epoch 136/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 227840.2856 - mse: 163804.0803 - mae: 340.4418\n",
      "Epoch 137/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 218268.9709 - mse: 154675.3641 - mae: 332.2143\n",
      "Epoch 138/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 214945.2339 - mse: 153170.0786 - mae: 335.2527\n",
      "Epoch 139/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 208257.9345 - mse: 149257.1523 - mae: 322.9136\n",
      "Epoch 140/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 196432.0799 - mse: 139851.9505 - mae: 314.4135\n",
      "Epoch 141/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 201503.1011 - mse: 143709.3220 - mae: 314.3987\n",
      "Epoch 142/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 216202.9062 - mse: 161197.1951 - mae: 339.1493\n",
      "Epoch 143/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 192711.9870 - mse: 140494.7094 - mae: 308.3910\n",
      "Epoch 144/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 195560.1606 - mse: 143308.9614 - mae: 316.6246\n",
      "Epoch 145/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 200295.8125 - mse: 148842.7548 - mae: 321.2339\n",
      "Epoch 146/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 194229.8073 - mse: 144946.4071 - mae: 323.4731\n",
      "Epoch 147/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 194557.0712 - mse: 146417.7387 - mae: 324.9020\n",
      "Epoch 148/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 194191.6480 - mse: 146863.0599 - mae: 319.9698\n",
      "Epoch 149/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 199124.5608 - mse: 152784.3129 - mae: 322.6200\n",
      "Epoch 150/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 185031.1944 - mse: 140847.5293 - mae: 309.8655\n",
      "Epoch 151/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 177006.3086 - mse: 134352.4019 - mae: 304.4941\n",
      "Epoch 152/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 194616.2365 - mse: 152864.6554 - mae: 324.4271\n",
      "Epoch 153/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 187826.1150 - mse: 146213.3136 - mae: 322.5267\n",
      "Epoch 154/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 166738.7044 - mse: 127563.0302 - mae: 297.4211\n",
      "Epoch 155/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 185163.3954 - mse: 146883.5959 - mae: 322.8106\n",
      "Epoch 156/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 180807.6901 - mse: 138379.4629 - mae: 312.6347\n",
      "Epoch 157/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 176031.3954 - mse: 135340.5590 - mae: 307.5628\n",
      "Epoch 158/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 191912.7426 - mse: 152560.0616 - mae: 327.4121\n",
      "Epoch 159/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 165296.8266 - mse: 127626.2470 - mae: 296.1280\n",
      "Epoch 160/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 183154.5091 - mse: 146247.5647 - mae: 319.3833\n",
      "Epoch 161/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 176170.0347 - mse: 140191.0215 - mae: 313.8600\n",
      "Epoch 162/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 175058.7569 - mse: 140143.2244 - mae: 312.5692\n",
      "Epoch 163/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 177469.5330 - mse: 144095.2752 - mae: 320.9820\n",
      "Epoch 164/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 187754.0334 - mse: 154806.3837 - mae: 335.1212\n",
      "Epoch 165/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 170712.5100 - mse: 138056.7684 - mae: 305.7025\n",
      "Epoch 166/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 159729.7387 - mse: 127703.3325 - mae: 294.0131\n",
      "Epoch 167/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 173315.2999 - mse: 142736.8607 - mae: 312.3306\n",
      "Epoch 168/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 167082.4683 - mse: 138341.6402 - mae: 310.4352\n",
      "Epoch 169/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 147378.9130 - mse: 120344.9759 - mae: 293.5879\n",
      "Epoch 170/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 168255.7478 - mse: 141387.3082 - mae: 315.2148\n",
      "Epoch 171/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 153489.9494 - mse: 127242.6235 - mae: 296.3801\n",
      "Epoch 172/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 151247.0447 - mse: 125412.6074 - mae: 292.3621\n",
      "Epoch 173/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 145662.9688 - mse: 120150.1452 - mae: 286.9373\n",
      "Epoch 174/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 155616.4588 - mse: 130567.0471 - mae: 299.4183\n",
      "Epoch 175/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 157957.8893 - mse: 134121.7496 - mae: 302.5214\n",
      "Epoch 176/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 148572.1150 - mse: 124826.5065 - mae: 288.1533\n",
      "Epoch 177/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 156229.7960 - mse: 133688.9579 - mae: 295.1626\n",
      "Epoch 178/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 144282.1786 - mse: 123144.2692 - mae: 285.5763\n",
      "Epoch 179/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 132426.8409 - mse: 111762.6402 - mae: 278.3869\n",
      "Epoch 180/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 149593.0881 - mse: 129115.3325 - mae: 289.3844\n",
      "Epoch 181/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 121879.1402 - mse: 101495.0026 - mae: 256.6397\n",
      "Epoch 182/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 124629.5707 - mse: 105019.0564 - mae: 259.5053\n",
      "Epoch 183/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 138941.9983 - mse: 119735.7776 - mae: 275.2248\n",
      "Epoch 184/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 128036.9271 - mse: 108605.2099 - mae: 273.0048\n",
      "Epoch 185/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 122923.7266 - mse: 104146.3739 - mae: 268.8496\n",
      "Epoch 186/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 131000.3270 - mse: 112729.5838 - mae: 272.1556\n",
      "Epoch 187/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 119279.2891 - mse: 102230.4733 - mae: 257.9590\n",
      "Epoch 188/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 121233.8479 - mse: 104786.5951 - mae: 267.0964\n",
      "Epoch 189/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 131159.4727 - mse: 115465.7105 - mae: 278.3391\n",
      "Epoch 190/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 132676.4316 - mse: 117216.8223 - mae: 281.6332\n",
      "Epoch 191/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 125775.3661 - mse: 110634.9464 - mae: 276.9038\n",
      "Epoch 192/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 114972.3034 - mse: 100494.9386 - mae: 254.1591\n",
      "Epoch 193/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 124712.7565 - mse: 110656.8574 - mae: 264.0317\n",
      "Epoch 194/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 123289.6545 - mse: 109638.9902 - mae: 269.9401\n",
      "Epoch 195/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 127567.6808 - mse: 114015.5987 - mae: 278.7119\n",
      "Epoch 196/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 114526.2324 - mse: 101709.2298 - mae: 257.7692\n",
      "Epoch 197/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 115918.3047 - mse: 103262.7166 - mae: 263.2539\n",
      "Epoch 198/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 118300.3770 - mse: 105927.9360 - mae: 257.3693\n",
      "Epoch 199/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 118461.5677 - mse: 106585.9599 - mae: 256.6938\n",
      "Epoch 200/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 124719.9073 - mse: 113109.9479 - mae: 277.0750\n",
      "Epoch 201/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 108272.6207 - mse: 97601.1194 - mae: 251.2567\n",
      "Epoch 202/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 101922.4965 - mse: 90766.9542 - mae: 242.4979\n",
      "Epoch 203/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 107512.2496 - mse: 97373.5306 - mae: 249.7952\n",
      "Epoch 204/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 108302.9868 - mse: 98652.4180 - mae: 253.9938\n",
      "Epoch 205/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 107804.0508 - mse: 98527.8509 - mae: 255.6373\n",
      "Epoch 206/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 94406.7116 - mse: 85394.4273 - mae: 233.8130\n",
      "Epoch 207/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 103977.5957 - mse: 95475.0367 - mae: 253.4622\n",
      "Epoch 208/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 99832.0888 - mse: 91866.8518 - mae: 248.4500\n",
      "Epoch 209/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 107338.7498 - mse: 100031.1656 - mae: 255.0269\n",
      "Epoch 210/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 99551.7057 - mse: 92660.2856 - mae: 247.0977\n",
      "Epoch 211/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 86638.8867 - mse: 79932.9785 - mae: 226.2232\n",
      "Epoch 212/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91790.5317 - mse: 84841.9169 - mae: 227.9910\n",
      "Epoch 213/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 97524.2776 - mse: 91134.2344 - mae: 242.6524\n",
      "Epoch 214/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 90488.0564 - mse: 84311.6697 - mae: 234.0424\n",
      "Epoch 215/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 88242.2765 - mse: 81853.1467 - mae: 228.0846\n",
      "Epoch 216/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 92002.0627 - mse: 86146.4559 - mae: 235.7833\n",
      "Epoch 217/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91182.0911 - mse: 85333.2635 - mae: 235.1762\n",
      "Epoch 218/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 92259.2289 - mse: 86642.0727 - mae: 244.9005\n",
      "Epoch 219/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 93930.9536 - mse: 88580.4338 - mae: 242.5728\n",
      "Epoch 220/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 88204.0106 - mse: 83309.3687 - mae: 228.2757\n",
      "Epoch 221/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 89988.9960 - mse: 85090.0289 - mae: 227.8893\n",
      "Epoch 222/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 91708.1233 - mse: 87166.0271 - mae: 236.4279\n",
      "Epoch 223/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 89735.1758 - mse: 85150.8617 - mae: 231.1523\n",
      "Epoch 224/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 84405.6270 - mse: 80170.8694 - mae: 224.3431\n",
      "Epoch 225/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 83178.1031 - mse: 78770.7993 - mae: 226.9964\n",
      "Epoch 226/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 83005.4920 - mse: 79035.2472 - mae: 223.4847\n",
      "Epoch 227/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 83697.3062 - mse: 79765.1714 - mae: 227.6711\n",
      "Epoch 228/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 76293.3751 - mse: 72657.8350 - mae: 215.8685\n",
      "Epoch 229/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 91989.8828 - mse: 88399.8770 - mae: 241.5641\n",
      "Epoch 230/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 83883.4371 - mse: 80631.4418 - mae: 226.9848\n",
      "Epoch 231/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 101872.4963 - mse: 98721.5849 - mae: 251.2834\n",
      "Epoch 232/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 83779.3839 - mse: 80482.0326 - mae: 220.5123\n",
      "Epoch 233/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 90902.7274 - mse: 87748.6213 - mae: 245.2509\n",
      "Epoch 234/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 84241.3475 - mse: 81227.5557 - mae: 233.2410\n",
      "Epoch 235/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 82156.7434 - mse: 79097.7397 - mae: 225.6368\n",
      "Epoch 236/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 87656.8240 - mse: 84700.9112 - mae: 230.6227\n",
      "Epoch 237/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 79850.9380 - mse: 77125.1608 - mae: 223.9625\n",
      "Epoch 238/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 86680.5493 - mse: 83970.0870 - mae: 231.8926\n",
      "Epoch 239/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 87395.9284 - mse: 84882.3969 - mae: 238.1472\n",
      "Epoch 240/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 75059.0211 - mse: 72382.5411 - mae: 218.2282\n",
      "Epoch 241/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 89097.3344 - mse: 86483.1482 - mae: 236.9428\n",
      "Epoch 242/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 86078.0694 - mse: 83667.2693 - mae: 237.8044\n",
      "Epoch 243/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 90190.5818 - mse: 87787.6984 - mae: 234.8493\n",
      "Epoch 244/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77690.5873 - mse: 75452.2201 - mae: 218.5058\n",
      "Epoch 245/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 81681.2007 - mse: 79404.3153 - mae: 223.5045\n",
      "Epoch 246/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 78821.0007 - mse: 76805.5588 - mae: 222.7573\n",
      "Epoch 247/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 76716.3118 - mse: 74733.7961 - mae: 217.5082\n",
      "Epoch 248/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 80505.8934 - mse: 78748.9701 - mae: 223.5290\n",
      "Epoch 249/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 91382.3546 - mse: 89253.7441 - mae: 241.6480\n",
      "Epoch 250/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73696.4905 - mse: 72010.0002 - mae: 218.9851\n",
      "Epoch 251/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70062.2752 - mse: 68204.3721 - mae: 213.0269\n",
      "Epoch 252/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73724.5720 - mse: 72112.6543 - mae: 212.9900\n",
      "Epoch 253/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 80797.1708 - mse: 79134.7375 - mae: 225.4815\n",
      "Epoch 254/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 76158.3819 - mse: 74389.2962 - mae: 217.5220\n",
      "Epoch 255/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73420.3859 - mse: 71748.1100 - mae: 217.2879\n",
      "Epoch 256/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73070.8867 - mse: 71336.2629 - mae: 213.2280\n",
      "Epoch 257/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 70539.2526 - mse: 68867.1266 - mae: 206.9411\n",
      "Epoch 258/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 78335.3547 - mse: 76818.8927 - mae: 219.2233\n",
      "Epoch 259/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 83535.4714 - mse: 82074.9461 - mae: 227.7619\n",
      "Epoch 260/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 76870.4034 - mse: 75430.0781 - mae: 223.1596\n",
      "Epoch 261/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 75156.2485 - mse: 73789.7656 - mae: 214.1762\n",
      "Epoch 262/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 69656.4316 - mse: 68366.0770 - mae: 210.0018\n",
      "Epoch 263/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73284.5875 - mse: 71953.7611 - mae: 216.2843\n",
      "Epoch 264/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77991.1101 - mse: 76506.1352 - mae: 218.6422\n",
      "Epoch 265/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73062.4865 - mse: 71716.4117 - mae: 212.2840\n",
      "Epoch 266/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 72206.3969 - mse: 70868.1732 - mae: 215.1724\n",
      "Epoch 267/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 68312.1845 - mse: 67070.8485 - mae: 211.2877\n",
      "Epoch 268/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 74697.6957 - mse: 73313.3767 - mae: 220.7910\n",
      "Epoch 269/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 89295.4271 - mse: 88040.5412 - mae: 239.1865\n",
      "Epoch 270/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77351.8292 - mse: 76140.4195 - mae: 225.5402\n",
      "Epoch 271/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 76603.9620 - mse: 75452.7023 - mae: 225.7222\n",
      "Epoch 272/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73903.7319 - mse: 72764.6699 - mae: 219.6006\n",
      "Epoch 273/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73117.4588 - mse: 71901.6692 - mae: 211.2730\n",
      "Epoch 274/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 80095.3191 - mse: 78997.2103 - mae: 224.4282\n",
      "Epoch 275/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73157.0548 - mse: 72063.5475 - mae: 218.1344\n",
      "Epoch 276/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 75408.0212 - mse: 74283.0995 - mae: 213.8416\n",
      "Epoch 277/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 74327.9696 - mse: 73401.1578 - mae: 222.4145\n",
      "Epoch 278/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70712.3318 - mse: 69580.6956 - mae: 207.6205\n",
      "Epoch 279/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70221.4478 - mse: 69161.9046 - mae: 212.6302\n",
      "Epoch 280/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71550.6806 - mse: 70559.5208 - mae: 210.2501\n",
      "Epoch 281/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66787.1253 - mse: 65686.0495 - mae: 209.5088\n",
      "Epoch 282/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 74711.4133 - mse: 73703.7308 - mae: 218.6783\n",
      "Epoch 283/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 69496.9508 - mse: 68393.9961 - mae: 213.9260\n",
      "Epoch 284/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66708.4173 - mse: 65741.9958 - mae: 213.5727\n",
      "Epoch 285/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73432.0560 - mse: 72571.6263 - mae: 220.1559\n",
      "Epoch 286/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59915.8645 - mse: 58898.9424 - mae: 196.7277\n",
      "Epoch 287/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 76892.7023 - mse: 75888.2617 - mae: 226.9570\n",
      "Epoch 288/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70467.7097 - mse: 69607.5485 - mae: 212.1975\n",
      "Epoch 289/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66638.3841 - mse: 65680.8659 - mae: 207.0931\n",
      "Epoch 290/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70642.6705 - mse: 69893.3751 - mae: 208.3111\n",
      "Epoch 291/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 71218.6822 - mse: 70294.2797 - mae: 215.6818\n",
      "Epoch 292/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 71046.9325 - mse: 70183.3283 - mae: 215.1981\n",
      "Epoch 293/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 66318.2485 - mse: 65385.9859 - mae: 206.5618\n",
      "Epoch 294/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 74837.3305 - mse: 73969.7526 - mae: 218.9242\n",
      "Epoch 295/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63881.2005 - mse: 63093.0780 - mae: 205.4827\n",
      "Epoch 296/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 68181.8126 - mse: 67313.0334 - mae: 209.5868\n",
      "Epoch 297/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61866.1232 - mse: 60947.0514 - mae: 198.4690\n",
      "Epoch 298/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66378.0596 - mse: 65477.9296 - mae: 208.0424\n",
      "Epoch 299/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65182.2831 - mse: 64409.7711 - mae: 207.9524\n",
      "Epoch 300/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 71360.5983 - mse: 70628.0451 - mae: 213.7236\n",
      "CPU times: user 41 s, sys: 6.39 s, total: 47.4 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(model,\n",
    "                                   param_distributions=params, n_jobs=-1)\n",
    "\n",
    "random_search_results = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05, 'epochs': 300, 'batch_size': 16, 'activation': 'relu'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tuned = random_search_results.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (1.3.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.5.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error test: 141.93875379147738\n",
      "Mean Absolute Error train: 1.6077424749947977\n"
     ]
    }
   ],
   "source": [
    "xgbr = XGBRegressor()\n",
    "\n",
    "xgbr.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_metric=\"mae\",  \n",
    "    verbose=True)\n",
    "\n",
    "predictions = xgbr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error test: \" + str(mean_absolute_error(predictions, y_test)))\n",
    "print(\"Mean Absolute Error train: \" + str(mean_absolute_error(xgbr.predict(X_train), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  190.715886\n",
      "Mean MAE: 148.921 (10.719)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8359390.840409388, tolerance: 2469.0218836503623\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "lasso_reg = Lasso(alpha=0.0)\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "pred = lasso_reg.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "scores = cross_val_score(lasso_reg, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "\n",
    "rmse = np.sqrt(MSE(y_test, pred))\n",
    "print(\"RMSE : % f\" %(rmse))\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "             estimator=Lasso(), n_jobs=-1,\n",
       "             param_grid={'alpha': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99])},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search hyperparameters for lasso regression\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# define model\n",
    "model = Lasso()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = arange(0, 1, 0.01)\n",
    "# define search\n",
    "lasso_tuned = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "lasso_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 152.866 (14.204)\n"
     ]
    }
   ],
   "source": [
    "# evaluate an ridge regression model on the dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define model\n",
    "ridge_reg = Ridge(alpha=0.00)\n",
    "# define model evaluation method\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(ridge_reg, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.48202e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.34069e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99]),\n",
       "        cv=RepeatedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "        scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import arange\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ridge_tune2 = RidgeCV(alphas=arange(0, 1, 0.01), cv=cv, scoring='neg_mean_absolute_error')\n",
    "# fit model\n",
    "ridge_tune2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = svm.SVR()\n",
    "sv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance with twitter BERT and news sentiments\n",
      "Epoch 1/300\n",
      "35/35 [==============================] - 1s 3ms/step - loss: 5536925.4097 - mse: 5066646.5278 - mae: 712.7170\n",
      "Epoch 2/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 598470.8073 - mse: 173884.4110 - mae: 359.6999\n",
      "Epoch 3/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 679723.2361 - mse: 296508.9249 - mae: 406.1582\n",
      "Epoch 4/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 871183.6163 - mse: 515488.8845 - mae: 383.3505\n",
      "Epoch 5/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 801063.2587 - mse: 444849.6181 - mae: 432.5209\n",
      "Epoch 6/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 553020.8785 - mse: 194041.2331 - mae: 367.5253\n",
      "Epoch 7/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 524188.9158 - mse: 185508.4701 - mae: 375.4263\n",
      "Epoch 8/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 503759.3194 - mse: 171223.0169 - mae: 355.7845\n",
      "Epoch 9/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 480228.3264 - mse: 174656.2192 - mae: 353.7265\n",
      "Epoch 10/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 469205.8837 - mse: 171769.5221 - mae: 353.4329\n",
      "Epoch 11/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 473665.3255 - mse: 177376.6623 - mae: 364.4743\n",
      "Epoch 12/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 542907.7908 - mse: 242696.9766 - mae: 387.7019\n",
      "Epoch 13/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 460855.2700 - mse: 185522.6740 - mae: 355.6628\n",
      "Epoch 14/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 479404.5356 - mse: 212278.4633 - mae: 357.4356\n",
      "Epoch 15/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 473743.6189 - mse: 210476.9293 - mae: 367.4499\n",
      "Epoch 16/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 505626.3151 - mse: 246429.9683 - mae: 386.4118\n",
      "Epoch 17/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 443468.3672 - mse: 202222.6254 - mae: 361.8260\n",
      "Epoch 18/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 532993.9002 - mse: 299565.0135 - mae: 373.7153\n",
      "Epoch 19/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 415561.5564 - mse: 180505.9648 - mae: 368.6671\n",
      "Epoch 20/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 733764.1389 - mse: 515021.7908 - mae: 413.5149\n",
      "Epoch 21/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 381155.1953 - mse: 161283.0065 - mae: 344.7599\n",
      "Epoch 22/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 393777.6050 - mse: 174536.3715 - mae: 360.3347\n",
      "Epoch 23/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 376349.7760 - mse: 164605.9583 - mae: 347.7452\n",
      "Epoch 24/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 393721.5668 - mse: 188943.6120 - mae: 364.1485\n",
      "Epoch 25/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 397791.2630 - mse: 199490.9822 - mae: 372.7275\n",
      "Epoch 26/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 388679.2439 - mse: 192428.9905 - mae: 365.7403\n",
      "Epoch 27/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 372865.6632 - mse: 181731.4753 - mae: 367.7768\n",
      "Epoch 28/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 472144.6372 - mse: 285301.5443 - mae: 368.3477\n",
      "Epoch 29/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 360852.8151 - mse: 172380.8997 - mae: 358.4692\n",
      "Epoch 30/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 353157.8212 - mse: 168512.4805 - mae: 348.6085\n",
      "Epoch 31/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 350635.5625 - mse: 168228.0642 - mae: 351.7122\n",
      "Epoch 32/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 346470.2161 - mse: 173430.1758 - mae: 355.6797\n",
      "Epoch 33/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 339086.4757 - mse: 172785.0052 - mae: 355.6012\n",
      "Epoch 34/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 340426.7231 - mse: 178278.8320 - mae: 367.4305\n",
      "Epoch 35/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 312402.2396 - mse: 162826.9857 - mae: 345.9437\n",
      "Epoch 36/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 317959.2326 - mse: 172836.7925 - mae: 356.5315\n",
      "Epoch 37/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 313206.1241 - mse: 168994.0469 - mae: 352.9931\n",
      "Epoch 38/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 314065.6111 - mse: 178596.8186 - mae: 359.8651\n",
      "Epoch 39/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 310829.3351 - mse: 175590.4905 - mae: 365.3646\n",
      "Epoch 40/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 298196.1675 - mse: 171967.7135 - mae: 359.4484\n",
      "Epoch 41/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 311294.1684 - mse: 185398.7122 - mae: 374.4818\n",
      "Epoch 42/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 299650.8733 - mse: 179513.3420 - mae: 366.4950\n",
      "Epoch 43/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 281864.3672 - mse: 165476.6536 - mae: 345.5722\n",
      "Epoch 44/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 307521.2005 - mse: 197369.2565 - mae: 371.3078\n",
      "Epoch 45/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 279911.3481 - mse: 168534.3850 - mae: 354.4225\n",
      "Epoch 46/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 282928.6337 - mse: 176655.6085 - mae: 362.4651\n",
      "Epoch 47/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 342830.1432 - mse: 240238.4718 - mae: 374.1702\n",
      "Epoch 48/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 261532.8572 - mse: 163355.6936 - mae: 345.4612\n",
      "Epoch 49/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 295989.5799 - mse: 202147.7904 - mae: 377.2641\n",
      "Epoch 50/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 269837.5577 - mse: 177000.3711 - mae: 364.5374\n",
      "Epoch 51/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 267896.6228 - mse: 177625.8047 - mae: 363.6268\n",
      "Epoch 52/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 249261.2374 - mse: 163554.1402 - mae: 343.9450\n",
      "Epoch 53/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 248059.0842 - mse: 167952.2548 - mae: 349.6318\n",
      "Epoch 54/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 253703.1519 - mse: 173667.6172 - mae: 357.7515\n",
      "Epoch 55/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 252528.8880 - mse: 179243.8468 - mae: 358.4760\n",
      "Epoch 56/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 253854.2457 - mse: 176505.6194 - mae: 361.2630\n",
      "Epoch 57/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 245161.8811 - mse: 174863.5516 - mae: 364.3320\n",
      "Epoch 58/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 246408.1380 - mse: 178778.9792 - mae: 362.2299\n",
      "Epoch 59/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 246919.6597 - mse: 180212.7648 - mae: 369.4274\n",
      "Epoch 60/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 230658.8780 - mse: 167358.1970 - mae: 347.8669\n",
      "Epoch 61/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 232821.9935 - mse: 173654.5590 - mae: 362.1075\n",
      "Epoch 62/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 262940.1324 - mse: 205958.8711 - mae: 358.5271\n",
      "Epoch 63/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 231707.1432 - mse: 176211.7192 - mae: 362.1676\n",
      "Epoch 64/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 226986.8498 - mse: 172907.1680 - mae: 362.0464\n",
      "Epoch 65/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 216628.5951 - mse: 164864.7005 - mae: 344.8427\n",
      "Epoch 66/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 211149.0859 - mse: 162734.2773 - mae: 341.9334\n",
      "Epoch 67/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 220461.5538 - mse: 172955.3346 - mae: 357.4435\n",
      "Epoch 68/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 210485.9062 - mse: 164419.6832 - mae: 346.5513\n",
      "Epoch 69/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 236877.2513 - mse: 194550.0556 - mae: 374.4106\n",
      "Epoch 70/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 241602.1319 - mse: 199381.7582 - mae: 372.2753\n",
      "Epoch 71/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 211649.4978 - mse: 170987.4922 - mae: 355.9994\n",
      "Epoch 72/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 208920.7270 - mse: 170366.3924 - mae: 352.8791\n",
      "Epoch 73/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 219095.8160 - mse: 181711.2969 - mae: 371.5101\n",
      "Epoch 74/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 202142.2027 - mse: 166747.1766 - mae: 350.0367\n",
      "Epoch 75/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 220070.1220 - mse: 185119.4371 - mae: 373.3128\n",
      "Epoch 76/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 207064.1632 - mse: 174426.9809 - mae: 360.1037\n",
      "Epoch 77/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 205497.1046 - mse: 174826.0395 - mae: 360.1813\n",
      "Epoch 78/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 204936.4683 - mse: 174426.9835 - mae: 359.9573\n",
      "Epoch 79/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 203675.6324 - mse: 175005.2166 - mae: 363.2833\n",
      "Epoch 80/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 202770.6189 - mse: 175221.0521 - mae: 363.4569\n",
      "Epoch 81/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 201079.4492 - mse: 174337.2205 - mae: 363.1912\n",
      "Epoch 82/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 204760.2847 - mse: 178455.0460 - mae: 364.1399\n",
      "Epoch 83/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 205434.6033 - mse: 181239.0977 - mae: 366.4972\n",
      "Epoch 84/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 219215.9323 - mse: 195368.7591 - mae: 370.0550\n",
      "Epoch 85/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 193840.5903 - mse: 171186.9800 - mae: 359.0157\n",
      "Epoch 86/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 197981.3620 - mse: 176940.1788 - mae: 364.9665\n",
      "Epoch 87/300\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 192323.0660 - mse: 172298.1931 - mae: 355.9012\n",
      "Epoch 88/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 221245.2517 - mse: 201390.2891 - mae: 372.9054\n",
      "Epoch 89/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 189832.9306 - mse: 171721.5803 - mae: 349.8966\n",
      "Epoch 90/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 203879.6098 - mse: 186719.7843 - mae: 375.7347\n",
      "Epoch 91/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 205127.7799 - mse: 188676.0599 - mae: 379.1835\n",
      "Epoch 92/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 204218.7352 - mse: 188211.9631 - mae: 377.4644\n",
      "Epoch 93/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 196071.7439 - mse: 181050.8333 - mae: 366.0738\n",
      "Epoch 94/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 183937.2140 - mse: 169822.5590 - mae: 352.0424\n",
      "Epoch 95/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 187342.7300 - mse: 173523.6081 - mae: 359.5095\n",
      "Epoch 96/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 180289.1487 - mse: 167306.1393 - mae: 348.7672\n",
      "Epoch 97/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 176657.9188 - mse: 163406.0690 - mae: 341.7775\n",
      "Epoch 98/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 176042.2248 - mse: 163981.6610 - mae: 343.0468\n",
      "Epoch 99/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 164592.9674 - mse: 151820.9089 - mae: 315.9257\n",
      "Epoch 100/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 164567.7166 - mse: 152009.5720 - mae: 331.7033\n",
      "Epoch 101/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 147172.1128 - mse: 135166.1304 - mae: 300.4019\n",
      "Epoch 102/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 135905.7964 - mse: 123794.3266 - mae: 285.0485\n",
      "Epoch 103/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 115048.2259 - mse: 103151.6458 - mae: 256.6941\n",
      "Epoch 104/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 112825.9661 - mse: 100724.4008 - mae: 258.3834\n",
      "Epoch 105/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 116817.3281 - mse: 106064.0295 - mae: 263.4748\n",
      "Epoch 106/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 109210.8524 - mse: 99016.6515 - mae: 254.6124\n",
      "Epoch 107/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 94447.5280 - mse: 84179.6803 - mae: 231.6419\n",
      "Epoch 108/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 103001.7786 - mse: 93174.6788 - mae: 250.7984\n",
      "Epoch 109/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 93749.3097 - mse: 84020.9974 - mae: 232.7737\n",
      "Epoch 110/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 112742.7960 - mse: 103447.9275 - mae: 262.4475\n",
      "Epoch 111/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 90397.0992 - mse: 81312.3980 - mae: 226.1582\n",
      "Epoch 112/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 99785.1765 - mse: 91195.8895 - mae: 242.7658\n",
      "Epoch 113/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 97722.7329 - mse: 89110.4891 - mae: 242.2803\n",
      "Epoch 114/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 98330.3655 - mse: 89824.1085 - mae: 240.0247\n",
      "Epoch 115/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 91923.3420 - mse: 83854.6343 - mae: 234.7533\n",
      "Epoch 116/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 83582.9462 - mse: 76244.6570 - mae: 224.1046\n",
      "Epoch 117/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 95692.6773 - mse: 88555.6914 - mae: 242.5362\n",
      "Epoch 118/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 92191.1751 - mse: 85288.0143 - mae: 238.4848\n",
      "Epoch 119/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 94873.3861 - mse: 87613.1738 - mae: 242.3627\n",
      "Epoch 120/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 90773.5124 - mse: 84144.3587 - mae: 236.0725\n",
      "Epoch 121/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 75748.2999 - mse: 69471.8570 - mae: 208.2717\n",
      "Epoch 122/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 87503.5982 - mse: 81397.5477 - mae: 230.5294\n",
      "Epoch 123/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 83751.7799 - mse: 78021.7257 - mae: 219.6778\n",
      "Epoch 124/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 88429.2589 - mse: 83568.5230 - mae: 231.0661\n",
      "Epoch 125/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 95580.8852 - mse: 90324.8301 - mae: 250.3933\n",
      "Epoch 126/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 81587.0490 - mse: 76808.7878 - mae: 221.0359\n",
      "Epoch 127/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 78459.1964 - mse: 74187.3817 - mae: 219.4080\n",
      "Epoch 128/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 81235.7333 - mse: 76568.7993 - mae: 222.9256\n",
      "Epoch 129/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 84427.5237 - mse: 80070.4013 - mae: 229.8995\n",
      "Epoch 130/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 79051.9989 - mse: 74886.1618 - mae: 219.6898\n",
      "Epoch 131/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 85035.3151 - mse: 80869.3526 - mae: 234.0418\n",
      "Epoch 132/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77388.6938 - mse: 73419.7454 - mae: 220.1314\n",
      "Epoch 133/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 82527.2376 - mse: 78711.8748 - mae: 227.9922\n",
      "Epoch 134/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73071.7779 - mse: 69088.1417 - mae: 210.9956\n",
      "Epoch 135/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 87531.6519 - mse: 83953.2556 - mae: 235.0655\n",
      "Epoch 136/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 76883.5841 - mse: 73621.8400 - mae: 220.6430\n",
      "Epoch 137/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 71613.3879 - mse: 68446.8089 - mae: 211.0746\n",
      "Epoch 138/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 74559.7278 - mse: 71329.1441 - mae: 217.6251\n",
      "Epoch 139/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77802.7242 - mse: 74730.0928 - mae: 222.3144\n",
      "Epoch 140/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 81510.3464 - mse: 78216.2399 - mae: 230.7009\n",
      "Epoch 141/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77541.5000 - mse: 74466.5888 - mae: 222.4705\n",
      "Epoch 142/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 77791.8555 - mse: 74990.3570 - mae: 224.2420\n",
      "Epoch 143/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73939.0997 - mse: 71041.9992 - mae: 212.3675\n",
      "Epoch 144/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 68514.4066 - mse: 65994.6643 - mae: 210.2245\n",
      "Epoch 145/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 76260.2441 - mse: 73439.5959 - mae: 216.1771\n",
      "Epoch 146/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73975.3072 - mse: 71224.8753 - mae: 210.4347\n",
      "Epoch 147/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 68813.0780 - mse: 66332.2905 - mae: 208.9023\n",
      "Epoch 148/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 80260.8286 - mse: 77951.5189 - mae: 233.1458\n",
      "Epoch 149/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 73864.1851 - mse: 70988.0547 - mae: 219.4775\n",
      "Epoch 150/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73863.0723 - mse: 71117.7195 - mae: 218.5270\n",
      "Epoch 151/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 66677.4974 - mse: 64188.3464 - mae: 205.9638\n",
      "Epoch 152/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 74384.5913 - mse: 71875.8696 - mae: 211.8071\n",
      "Epoch 153/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71818.9775 - mse: 69137.6240 - mae: 219.1114\n",
      "Epoch 154/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67366.5460 - mse: 65066.5674 - mae: 211.2602\n",
      "Epoch 155/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67732.4446 - mse: 65399.8946 - mae: 205.4144\n",
      "Epoch 156/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 69688.3415 - mse: 67445.7280 - mae: 209.9805\n",
      "Epoch 157/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 67654.8380 - mse: 65366.7644 - mae: 202.0291\n",
      "Epoch 158/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67692.1070 - mse: 65511.0825 - mae: 211.4167\n",
      "Epoch 159/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63601.9618 - mse: 61251.0088 - mae: 196.4369\n",
      "Epoch 160/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 68797.5139 - mse: 66451.7997 - mae: 207.9099\n",
      "Epoch 161/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70214.4748 - mse: 67928.8940 - mae: 208.4759\n",
      "Epoch 162/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 64572.8344 - mse: 62559.5664 - mae: 203.5345\n",
      "Epoch 163/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 71936.0573 - mse: 69981.4056 - mae: 214.4791\n",
      "Epoch 164/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 69248.0470 - mse: 67219.2103 - mae: 211.0087\n",
      "Epoch 165/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67671.6647 - mse: 65726.2174 - mae: 204.3663\n",
      "Epoch 166/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65622.3813 - mse: 63702.4211 - mae: 208.1062\n",
      "Epoch 167/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 56813.4524 - mse: 54907.1984 - mae: 191.6908\n",
      "Epoch 168/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61199.8764 - mse: 59231.7747 - mae: 195.4731\n",
      "Epoch 169/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65961.7801 - mse: 63993.5384 - mae: 204.2630\n",
      "Epoch 170/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63695.6410 - mse: 61852.0403 - mae: 207.4775\n",
      "Epoch 171/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60437.4530 - mse: 58888.2371 - mae: 198.6192\n",
      "Epoch 172/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 73691.3368 - mse: 72059.6528 - mae: 220.1299\n",
      "Epoch 173/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61717.1631 - mse: 60143.6013 - mae: 199.9060\n",
      "Epoch 174/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 71845.6289 - mse: 70313.8115 - mae: 218.0588\n",
      "Epoch 175/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61360.5807 - mse: 59666.6798 - mae: 193.6106\n",
      "Epoch 176/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63354.5228 - mse: 61768.5042 - mae: 206.7186\n",
      "Epoch 177/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60807.5943 - mse: 59068.6646 - mae: 197.9769\n",
      "Epoch 178/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 57576.2390 - mse: 55943.6118 - mae: 191.5323\n",
      "Epoch 179/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 66758.1543 - mse: 65275.4107 - mae: 202.2611\n",
      "Epoch 180/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 64401.8318 - mse: 62744.1261 - mae: 203.3680\n",
      "Epoch 181/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 64755.5958 - mse: 63197.3014 - mae: 204.6097\n",
      "Epoch 182/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 65051.1071 - mse: 63569.1201 - mae: 204.8082\n",
      "Epoch 183/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 62663.7531 - mse: 61031.9039 - mae: 200.4653\n",
      "Epoch 184/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 67854.8737 - mse: 66340.1649 - mae: 209.9954\n",
      "Epoch 185/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60984.0824 - mse: 59498.7116 - mae: 198.7123\n",
      "Epoch 186/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 70660.4331 - mse: 69281.2133 - mae: 207.4819\n",
      "Epoch 187/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59760.5432 - mse: 58365.4047 - mae: 197.0156\n",
      "Epoch 188/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60005.3966 - mse: 58507.8385 - mae: 194.5137\n",
      "Epoch 189/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61849.7091 - mse: 60446.6365 - mae: 203.4251\n",
      "Epoch 190/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58979.0549 - mse: 57533.7552 - mae: 195.9818\n",
      "Epoch 191/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63952.2492 - mse: 62451.5040 - mae: 205.4337\n",
      "Epoch 192/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62144.5706 - mse: 60618.4640 - mae: 201.6225\n",
      "Epoch 193/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61085.9655 - mse: 59649.3786 - mae: 199.0771\n",
      "Epoch 194/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56369.8271 - mse: 54893.6478 - mae: 192.8602\n",
      "Epoch 195/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 57390.2274 - mse: 55896.3682 - mae: 192.4733\n",
      "Epoch 196/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63597.8931 - mse: 62079.6788 - mae: 202.9884\n",
      "Epoch 197/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61105.4422 - mse: 59649.1926 - mae: 201.7383\n",
      "Epoch 198/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 63065.6489 - mse: 61786.9187 - mae: 201.2776\n",
      "Epoch 199/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 63950.1440 - mse: 62535.6182 - mae: 198.7685\n",
      "Epoch 200/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 57316.8687 - mse: 56022.2184 - mae: 189.7701\n",
      "Epoch 201/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60541.1717 - mse: 59195.6559 - mae: 195.6102\n",
      "Epoch 202/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58925.2550 - mse: 57707.4592 - mae: 195.7924\n",
      "Epoch 203/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56697.5590 - mse: 55437.1381 - mae: 188.7330\n",
      "Epoch 204/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 55881.8689 - mse: 54716.2929 - mae: 187.9621\n",
      "Epoch 205/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 55896.0461 - mse: 54779.9711 - mae: 190.4611\n",
      "Epoch 206/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65255.6967 - mse: 64130.2487 - mae: 204.6358\n",
      "Epoch 207/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60446.0234 - mse: 59365.6071 - mae: 196.0918\n",
      "Epoch 208/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 61931.4427 - mse: 60885.2459 - mae: 194.8184\n",
      "Epoch 209/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 52275.0408 - mse: 51113.8028 - mae: 180.1262\n",
      "Epoch 210/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 50906.8587 - mse: 49809.7346 - mae: 182.2449\n",
      "Epoch 211/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59534.2854 - mse: 58415.4659 - mae: 196.9533\n",
      "Epoch 212/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 55451.8651 - mse: 54363.6627 - mae: 189.7079\n",
      "Epoch 213/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56926.4462 - mse: 55647.7027 - mae: 188.7946\n",
      "Epoch 214/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 54895.2657 - mse: 53748.4842 - mae: 186.7906\n",
      "Epoch 215/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 62373.6057 - mse: 61329.6967 - mae: 202.3590\n",
      "Epoch 216/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 62772.6759 - mse: 61631.2934 - mae: 202.2243\n",
      "Epoch 217/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 57332.4980 - mse: 56086.9667 - mae: 195.3103\n",
      "Epoch 218/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58544.9495 - mse: 57319.8032 - mae: 198.9043\n",
      "Epoch 219/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 57676.8518 - mse: 56615.1835 - mae: 193.7992\n",
      "Epoch 220/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 55427.2793 - mse: 54332.9076 - mae: 190.7738\n",
      "Epoch 221/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58273.8290 - mse: 57092.9582 - mae: 197.9900\n",
      "Epoch 222/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60932.9128 - mse: 59864.8429 - mae: 194.9857\n",
      "Epoch 223/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 63058.7150 - mse: 62044.1236 - mae: 201.5175\n",
      "Epoch 224/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58896.8620 - mse: 57817.3537 - mae: 192.2508\n",
      "Epoch 225/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61889.9472 - mse: 60992.9196 - mae: 203.4350\n",
      "Epoch 226/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60409.1874 - mse: 59465.2161 - mae: 200.0563\n",
      "Epoch 227/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56318.8962 - mse: 55426.1438 - mae: 187.9718\n",
      "Epoch 228/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 64412.5836 - mse: 63467.5275 - mae: 202.6818\n",
      "Epoch 229/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 59575.1528 - mse: 58535.1740 - mae: 196.3887\n",
      "Epoch 230/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59155.4760 - mse: 58286.2548 - mae: 200.3024\n",
      "Epoch 231/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65125.1617 - mse: 64134.5387 - mae: 207.4243\n",
      "Epoch 232/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 55189.5126 - mse: 54175.5433 - mae: 194.0704\n",
      "Epoch 233/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60659.5522 - mse: 59749.9724 - mae: 198.2858\n",
      "Epoch 234/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58890.5408 - mse: 57965.7954 - mae: 197.7200\n",
      "Epoch 235/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 60859.3793 - mse: 59832.7560 - mae: 194.9376\n",
      "Epoch 236/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 49553.6683 - mse: 48560.6874 - mae: 177.1016\n",
      "Epoch 237/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 53471.7113 - mse: 52543.4822 - mae: 188.1029\n",
      "Epoch 238/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58522.6407 - mse: 57603.5208 - mae: 190.3679\n",
      "Epoch 239/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 56860.7873 - mse: 55992.4118 - mae: 191.6468\n",
      "Epoch 240/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61245.7810 - mse: 60395.1216 - mae: 198.9097\n",
      "Epoch 241/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 61527.8064 - mse: 60674.5015 - mae: 201.8800\n",
      "Epoch 242/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 56248.7357 - mse: 55481.6048 - mae: 194.9838\n",
      "Epoch 243/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 57536.3356 - mse: 56804.0378 - mae: 199.1244\n",
      "Epoch 244/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56211.9278 - mse: 55537.7306 - mae: 191.8776\n",
      "Epoch 245/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 51815.1096 - mse: 50941.2875 - mae: 184.3356\n",
      "Epoch 246/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60636.5689 - mse: 59919.2108 - mae: 198.9660\n",
      "Epoch 247/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58675.6444 - mse: 57870.0137 - mae: 199.1985\n",
      "Epoch 248/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 53589.2642 - mse: 52836.9668 - mae: 188.1083\n",
      "Epoch 249/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60976.9935 - mse: 60265.3895 - mae: 201.1688\n",
      "Epoch 250/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 50828.6211 - mse: 50080.5672 - mae: 181.5355\n",
      "Epoch 251/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59634.1544 - mse: 58880.5358 - mae: 200.3627\n",
      "Epoch 252/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 55673.9376 - mse: 54884.4876 - mae: 190.3527\n",
      "Epoch 253/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 57191.2484 - mse: 56505.5645 - mae: 195.2737\n",
      "Epoch 254/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 55606.4718 - mse: 54978.5165 - mae: 188.0084\n",
      "Epoch 255/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60812.6589 - mse: 60140.8513 - mae: 201.1498\n",
      "Epoch 256/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56548.9389 - mse: 55917.6763 - mae: 193.7970\n",
      "Epoch 257/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 56437.0310 - mse: 55825.0272 - mae: 194.2805\n",
      "Epoch 258/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58800.8344 - mse: 58252.0469 - mae: 199.4789\n",
      "Epoch 259/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 52663.9295 - mse: 52109.0034 - mae: 188.6025\n",
      "Epoch 260/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 53396.4969 - mse: 52765.4595 - mae: 186.8221\n",
      "Epoch 261/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 56968.1197 - mse: 56350.1475 - mae: 192.2189\n",
      "Epoch 262/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58744.1035 - mse: 58133.1264 - mae: 194.3918\n",
      "Epoch 263/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58061.6241 - mse: 57400.5518 - mae: 195.2727\n",
      "Epoch 264/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 59533.2492 - mse: 58903.5737 - mae: 199.6203\n",
      "Epoch 265/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 57774.0421 - mse: 57085.0102 - mae: 199.5244\n",
      "Epoch 266/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 57159.0991 - mse: 56566.8441 - mae: 194.2744\n",
      "Epoch 267/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 59692.0826 - mse: 59104.4731 - mae: 197.2482\n",
      "Epoch 268/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 53872.6799 - mse: 53268.0932 - mae: 189.8777\n",
      "Epoch 269/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 60578.8497 - mse: 59970.8372 - mae: 200.8521\n",
      "Epoch 270/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 56797.5085 - mse: 56239.0127 - mae: 190.4703\n",
      "Epoch 271/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 59470.4443 - mse: 58935.6123 - mae: 197.4449\n",
      "Epoch 272/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 53102.0245 - mse: 52563.3328 - mae: 187.8242\n",
      "Epoch 273/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 51747.1690 - mse: 51157.8804 - mae: 184.7554\n",
      "Epoch 274/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58011.0343 - mse: 57372.1876 - mae: 198.6274\n",
      "Epoch 275/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 50107.7770 - mse: 49477.2421 - mae: 176.4530\n",
      "Epoch 276/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 65758.5508 - mse: 65217.7228 - mae: 204.4478\n",
      "Epoch 277/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 53602.4655 - mse: 52983.7580 - mae: 188.0107\n",
      "Epoch 278/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 52310.0490 - mse: 51727.8761 - mae: 186.3302\n",
      "Epoch 279/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58792.3979 - mse: 58305.4206 - mae: 197.6382\n",
      "Epoch 280/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58780.3006 - mse: 58226.2676 - mae: 195.4039\n",
      "Epoch 281/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 51682.5170 - mse: 51162.9711 - mae: 184.5259\n",
      "Epoch 282/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 57840.8617 - mse: 57344.7867 - mae: 190.7516\n",
      "Epoch 283/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 53703.8739 - mse: 53137.4222 - mae: 186.5719\n",
      "Epoch 284/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 48546.3669 - mse: 47986.1793 - mae: 170.7215\n",
      "Epoch 285/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 57899.4909 - mse: 57345.5789 - mae: 193.4371\n",
      "Epoch 286/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58488.1225 - mse: 57975.1161 - mae: 194.6218\n",
      "Epoch 287/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 48820.3524 - mse: 48229.9322 - mae: 178.8870\n",
      "Epoch 288/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 51580.9450 - mse: 51056.8404 - mae: 181.4188\n",
      "Epoch 289/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 53005.4121 - mse: 52488.5486 - mae: 185.5432\n",
      "Epoch 290/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 52565.2919 - mse: 52066.7474 - mae: 184.2858\n",
      "Epoch 291/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 58354.6130 - mse: 57896.6776 - mae: 196.4374\n",
      "Epoch 292/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 67737.2490 - mse: 67238.2273 - mae: 213.9360\n",
      "Epoch 293/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 51674.5431 - mse: 51105.6239 - mae: 186.6681\n",
      "Epoch 294/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 62138.6991 - mse: 61590.2635 - mae: 197.9376\n",
      "Epoch 295/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 55104.7072 - mse: 54548.7703 - mae: 191.4012\n",
      "Epoch 296/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 52583.0295 - mse: 52065.2451 - mae: 182.1586\n",
      "Epoch 297/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 58333.3020 - mse: 57792.6842 - mae: 194.5955\n",
      "Epoch 298/300\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 52209.4867 - mse: 51679.9138 - mae: 187.1483\n",
      "Epoch 299/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 51775.2299 - mse: 51245.5847 - mae: 183.6646\n",
      "Epoch 300/300\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 54850.9915 - mse: 54366.8583 - mae: 188.7223\n",
      "35/35 [==============================] - 0s 886us/step\n",
      "12/12 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8359390.840409388, tolerance: 2469.0218836503623\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.48202e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.34069e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>xgb</th>\n",
       "      <th>knn</th>\n",
       "      <th>lasso</th>\n",
       "      <th>lasso_tuned</th>\n",
       "      <th>ridge</th>\n",
       "      <th>ridge_tuned</th>\n",
       "      <th>rf</th>\n",
       "      <th>sv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.999876</td>\n",
       "      <td>-0.614823</td>\n",
       "      <td>0.322858</td>\n",
       "      <td>0.322351</td>\n",
       "      <td>0.119215</td>\n",
       "      <td>0.322857</td>\n",
       "      <td>0.916841</td>\n",
       "      <td>0.031379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.281444</td>\n",
       "      <td>-0.973612</td>\n",
       "      <td>0.197270</td>\n",
       "      <td>0.200060</td>\n",
       "      <td>-0.026026</td>\n",
       "      <td>0.197424</td>\n",
       "      <td>0.365699</td>\n",
       "      <td>-0.007966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>1.607742</td>\n",
       "      <td>220.580739</td>\n",
       "      <td>144.686675</td>\n",
       "      <td>144.747499</td>\n",
       "      <td>159.034822</td>\n",
       "      <td>144.692474</td>\n",
       "      <td>49.178007</td>\n",
       "      <td>179.724546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>141.938754</td>\n",
       "      <td>250.976184</td>\n",
       "      <td>159.524052</td>\n",
       "      <td>159.303509</td>\n",
       "      <td>177.047829</td>\n",
       "      <td>159.509235</td>\n",
       "      <td>138.029918</td>\n",
       "      <td>185.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>2.358672</td>\n",
       "      <td>268.754258</td>\n",
       "      <td>174.033468</td>\n",
       "      <td>174.098592</td>\n",
       "      <td>198.485080</td>\n",
       "      <td>174.033551</td>\n",
       "      <td>60.988530</td>\n",
       "      <td>208.146836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>180.439863</td>\n",
       "      <td>299.042501</td>\n",
       "      <td>190.715886</td>\n",
       "      <td>190.384080</td>\n",
       "      <td>215.616161</td>\n",
       "      <td>190.697509</td>\n",
       "      <td>169.531188</td>\n",
       "      <td>213.710113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>1.280965</td>\n",
       "      <td>185.415135</td>\n",
       "      <td>188.378630</td>\n",
       "      <td>187.267513</td>\n",
       "      <td>244.329426</td>\n",
       "      <td>188.264488</td>\n",
       "      <td>52.135240</td>\n",
       "      <td>315.961874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>83.236118</td>\n",
       "      <td>115.411093</td>\n",
       "      <td>129.030494</td>\n",
       "      <td>129.598248</td>\n",
       "      <td>126.762572</td>\n",
       "      <td>129.064488</td>\n",
       "      <td>83.221574</td>\n",
       "      <td>190.504844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  xgb         knn       lasso  lasso_tuned       ridge  \\\n",
       "Train: Rsquare    0.999876   -0.614823    0.322858     0.322351    0.119215   \n",
       "Test: Rsquare     0.281444   -0.973612    0.197270     0.200060   -0.026026   \n",
       "Train: MAE        1.607742  220.580739  144.686675   144.747499  159.034822   \n",
       "Test: MAE       141.938754  250.976184  159.524052   159.303509  177.047829   \n",
       "Train: RMSE       2.358672  268.754258  174.033468   174.098592  198.485080   \n",
       "Test: RMSE      180.439863  299.042501  190.715886   190.384080  215.616161   \n",
       "Train: MAPE       1.280965  185.415135  188.378630   187.267513  244.329426   \n",
       "Test: MAPE       83.236118  115.411093  129.030494   129.598248  126.762572   \n",
       "\n",
       "Model           ridge_tuned          rf          sv  \n",
       "Train: Rsquare     0.322857    0.916841    0.031379  \n",
       "Test: Rsquare      0.197424    0.365699   -0.007966  \n",
       "Train: MAE       144.692474   49.178007  179.724546  \n",
       "Test: MAE        159.509235  138.029918  185.051200  \n",
       "Train: RMSE      174.033551   60.988530  208.146836  \n",
       "Test: RMSE       190.697509  169.531188  213.710113  \n",
       "Train: MAPE      188.264488   52.135240  315.961874  \n",
       "Test: MAPE       129.064488   83.221574  190.504844  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model performance with twitter BERT and news sentiments\")\n",
    "model_performance([xgbr, nn_tuned, lasso_reg, lasso_tuned, ridge_reg, ridge_tune2, rf, sv], [\"xgb\", 'knn', 'lasso', 'lasso_tuned', 'ridge', 'ridge_tuned', 'rf', 'sv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important_gain = xgbr.get_booster().get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_important_gain = xgbr.get_booster().get_score(importance_type='gain')\n",
    "keys = list(feature_important_gain.keys())\n",
    "values = list(feature_important_gain.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=['score']).sort_values(by='score', ascending=False)\n",
    "data.plot(kind='barh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
