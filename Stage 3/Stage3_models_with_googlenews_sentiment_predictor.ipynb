{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Models with google news headlines sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models analysis for top 10 brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import ast\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None) # display all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/notebooks/Data')\n",
    "df_standby = pd.read_csv('aggregated_reviews_meta.csv')\n",
    "df_standby.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df = df_standby.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape is:  (7097, 17)\n",
      "Top 10 brand df's shape is:  (736, 17)\n"
     ]
    }
   ],
   "source": [
    "top10_brand_df = df[df['brand'].isin(['Sony','PWR+','Apple','Boss Audio','Polk Audio','Sangean', 'Tripp Lite', \n",
    "                    'Yamaha Audio','Belkin','Garmin'])]\n",
    "print(\"df shape is: \", df.shape)\n",
    "print(\"Top 10 brand df's shape is: \", top10_brand_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Twitter_BERT_Sentiment data\n",
    "os.chdir('/notebooks/Data')\n",
    "df_BERT = pd.read_csv('Brand_BERT_sentiment.csv')\n",
    "#df_standby.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_bert = df_BERT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand\n",
      "Apple          0.606678\n",
      "Belkin         0.622052\n",
      "BossAudio      0.718182\n",
      "Garmin         0.595745\n",
      "PolkAudio      0.702373\n",
      "Pwr+           0.458546\n",
      "Sangean        0.540475\n",
      "Sony           0.609016\n",
      "TrippLite      0.412678\n",
      "YamahaAudio    0.695243\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Aggregate sentiment column of df_bert by brands create a new column for meta data\n",
    "bert_sentiment = pd.pivot_table(df_bert, index='Brand', values=\"Sentiment\", aggfunc=np.mean)\n",
    "print(bert_sentiment.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (top10_brand_df['brand'] == 'Apple'),\n",
    "    (top10_brand_df['brand'] == 'Belkin'),\n",
    "    (top10_brand_df['brand'] == 'Boss Audio'),\n",
    "    (top10_brand_df['brand'] == 'Garmin'),\n",
    "    (top10_brand_df['brand'] == 'Polk Audio'),\n",
    "    (top10_brand_df['brand'] == 'PWR+'),\n",
    "    (top10_brand_df['brand'] == 'Sangean'),\n",
    "    (top10_brand_df['brand'] == 'Sony'),\n",
    "    (top10_brand_df['brand'] == 'Tripp Lite'),\n",
    "    (top10_brand_df['brand'] == 'Yamaha Audio')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [0.606678, 0.622052, 0.718182, 0.595745, 0.702373, 0.458546, 0.540475, 0.609016, 0.412678, 0.695243]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "top10_brand_df['brand_sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load News_Sentiment data\n",
    "os.chdir('/notebooks/Data')\n",
    "df_news = pd.read_csv('Brand_News_Sentiment.csv')\n",
    "df_news.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (top10_brand_df['brand'] == 'Apple'),\n",
    "    (top10_brand_df['brand'] == 'Belkin'),\n",
    "    (top10_brand_df['brand'] == 'Boss Audio'),\n",
    "    (top10_brand_df['brand'] == 'Garmin'),\n",
    "    (top10_brand_df['brand'] == 'Polk Audio'),\n",
    "    (top10_brand_df['brand'] == 'PWR+'),\n",
    "    (top10_brand_df['brand'] == 'Sangean'),\n",
    "    (top10_brand_df['brand'] == 'Sony'),\n",
    "    (top10_brand_df['brand'] == 'Tripp Lite'),\n",
    "    (top10_brand_df['brand'] == 'Yamaha Audio')\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [0.130569, 0.206817, 0.307327, 0.219252, 0.166181, 0.569831, 0.319515, 0.082589, 0.076493, 0.139865]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "top10_brand_df['brand_news_sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.12.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.1.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# do encoding for categorical variable\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "new_df = top10_brand_df\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "new_df['brand_encode'] = encoder.fit_transform(new_df['brand'], new_df['rankElectronics'])\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "new_df['main_cat_encode'] = encoder.fit_transform(new_df['main_cat'], new_df['rankElectronics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "We will be using four models for prediction of sales rank. The following are as shown:<br>\n",
    "1. XGBoost Regressor \n",
    "2. Neural Network\n",
    "3. Ridge / Lasso Regression\n",
    "4. Random Forest\n",
    "5. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales_df = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales_df['normRankElectronics'] = np.array(sales_df.rankElectronics.rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split\n",
    "X = sales_df.drop(['rankElectronics', 'asin', 'title', 'brand', 'main_cat', 'normRankElectronics', 'brand_sentiment'], axis=1)\n",
    "y = sales_df.normRankElectronics\n",
    "\n",
    "X['vote'] = X['vote'].fillna(0)# to replace NaN with 0\n",
    "\n",
    "# Standard Scaler or Min Max Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4222)\n",
    "\n",
    "X_train[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_news_sentiment']] = scaler.fit_transform(X_train[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_news_sentiment']])\n",
    "X_test[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_news_sentiment']] = scaler.transform(X_test[[\"rating\", \"title_len\", \"vote\", \"summary_len\", \"category_count\",\"review_count\",\"verified_true_ratio\", \"also_buy_count\", \"also_view_count\", \"price\", \"review_text_len\", \"percentage_positive\", 'brand_news_sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "#Defining MAPE function\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "#models is a list of the fitted models, model_names is list of model names\n",
    "\n",
    "def model_performance(models, model_names):\n",
    "    #create empty df with col names\n",
    "    df = pd.DataFrame(columns = ['Model', 'Train: Rsquare', 'Test: Rsquare', 'Train: MAE', 'Test: MAE', 'Train: RMSE', 'Test: RMSE', 'Train: MAPE', 'Test: MAPE'])\n",
    "    \n",
    "    for n, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        #prepare values for model\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        name = model_names[n] \n",
    "        rsquare_train = r2_score(y_train, y_train_pred)\n",
    "        mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "        rmse_train = math.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        \n",
    "        rsquare_test = r2_score(y_test, y_pred)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred)\n",
    "        rmse_test = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        mape_train = MAPE(y_train, y_train_pred)\n",
    "        mape_test = MAPE(y_test, y_pred)\n",
    "\n",
    "        #append row to df\n",
    "        df = df.append({'Model' \n",
    "                        : name, 'Train: Rsquare' : rsquare_train, 'Test: Rsquare' : rsquare_test, 'Train: MAE': mae_train, 'Test: MAE' : mae_test, 'Train: RMSE': rmse_train,\n",
    "                         'Test: RMSE' : rmse_test, 'Train: MAPE': mape_train, 'Test: MAPE': mape_test}, \n",
    "                    ignore_index = True)\n",
    "            \n",
    "    return df.set_index('Model').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared on train dataset=0.19469439175619685\n",
      "R-Squared on test dataset=0.5218370630022122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>dtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.445184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.194694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>128.453769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>157.805276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>157.531492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>191.021574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>174.806516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>114.039335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  dtr\n",
       "Train: Rsquare    0.445184\n",
       "Test: Rsquare     0.194694\n",
       "Train: MAE      128.453769\n",
       "Test: MAE       157.805276\n",
       "Train: RMSE     157.531492\n",
       "Test: RMSE      191.021574\n",
       "Train: MAPE     174.806516\n",
       "Test: MAPE      114.039335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Without additional predictors\n",
    "dtr = DecisionTreeRegressor(max_depth=4,\n",
    "                           min_samples_split=5,\n",
    "                           max_leaf_nodes=10)\n",
    "\n",
    "dtr.fit(X_train,y_train)\n",
    "print(\"R-Squared on train dataset={}\".format(dtr.score(X_test,y_test)))\n",
    "\n",
    "dtr.fit(X_test,y_test)   \n",
    "print(\"R-Squared on test dataset={}\".format(dtr.score(X_test,y_test)))\n",
    "\n",
    "model_performance([dtr], [\"dtr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>dtr</th>\n",
       "      <th>dtr_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.445184</td>\n",
       "      <td>0.503350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.194694</td>\n",
       "      <td>0.204002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>128.453769</td>\n",
       "      <td>119.484159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>157.805276</td>\n",
       "      <td>153.235066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>157.531492</td>\n",
       "      <td>149.045256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>191.021574</td>\n",
       "      <td>189.914516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>174.806516</td>\n",
       "      <td>142.688494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>114.039335</td>\n",
       "      <td>97.181057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  dtr   dtr_tuned\n",
       "Train: Rsquare    0.445184    0.503350\n",
       "Test: Rsquare     0.194694    0.204002\n",
       "Train: MAE      128.453769  119.484159\n",
       "Test: MAE       157.805276  153.235066\n",
       "Train: RMSE     157.531492  149.045256\n",
       "Test: RMSE      191.021574  189.914516\n",
       "Train: MAPE     174.806516  142.688494\n",
       "Test: MAPE      114.039335   97.181057"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"criterion\": [\"mse\", \"mae\"],\n",
    "              \"min_samples_split\": [10, 20, 40],\n",
    "              \"max_depth\": [2, 6, 8],\n",
    "              \"min_samples_leaf\": [20, 40, 100],\n",
    "              \"max_leaf_nodes\": [5, 20, 100],\n",
    "              }\n",
    "\n",
    "## Comment in order to publish in kaggle.\n",
    "dtr_tuned = GridSearchCV(dtr, param_grid, cv=5)\n",
    "model_performance([dtr, dtr_tuned], [\"dtr\", \"dtr_tuned\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 18,689\n",
      "Trainable params: 18,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(128,\n",
    "                activation = 'relu',\n",
    "                input_shape = (15, ),\n",
    "                activity_regularizer = regularizers.l2(1e-5)))\n",
    "nn.add(Dropout(0.50))\n",
    "nn.add(Dense(128,\n",
    "                activation = 'relu', \n",
    "                activity_regularizer = regularizers.l2(1e-5)))\n",
    "nn.add(Dropout(0.50))\n",
    "nn.add(Dense(1, activation = 'relu'))\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "56/56 [==============================] - 2s 14ms/step - loss: 17023097.7895 - mse: 16726846.0439 - mae: 1116.9288 - val_loss: 407095.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 2/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 494269.5515 - mse: 234104.4298 - mae: 358.3984 - val_loss: 387076.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 3/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 426598.9737 - mse: 191203.0159 - mae: 360.2332 - val_loss: 371121.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 4/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 539409.0181 - mse: 330959.3413 - mae: 400.9445 - val_loss: 365729.9375 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 5/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 435612.9073 - mse: 231726.1173 - mae: 376.8378 - val_loss: 365245.5938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 6/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 405686.9737 - mse: 202787.0236 - mae: 357.7803 - val_loss: 365111.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 7/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 649803.7012 - mse: 454577.1310 - mae: 409.3079 - val_loss: 363297.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 8/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 495356.1327 - mse: 298793.4230 - mae: 377.5295 - val_loss: 364555.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 9/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 373391.0833 - mse: 174209.3300 - mae: 347.4827 - val_loss: 361171.9062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 10/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 361130.6261 - mse: 167267.2264 - mae: 337.1592 - val_loss: 356932.4688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 11/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 416838.7401 - mse: 238770.1557 - mae: 388.0606 - val_loss: 355891.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 12/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 346475.1376 - mse: 161562.9463 - mae: 341.4680 - val_loss: 353725.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 13/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 361328.5559 - mse: 183037.3972 - mae: 361.4892 - val_loss: 350570.0625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 14/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 444776.0126 - mse: 266989.0754 - mae: 393.1401 - val_loss: 348512.0000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 15/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 333890.2749 - mse: 161428.8777 - mae: 342.7292 - val_loss: 345715.5312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 16/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 332033.4293 - mse: 163112.9359 - mae: 342.8960 - val_loss: 342367.2812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 17/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 353124.5296 - mse: 181491.7673 - mae: 371.3634 - val_loss: 338928.7500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 18/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 348450.7072 - mse: 177337.3037 - mae: 362.1909 - val_loss: 336371.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 19/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 346204.4852 - mse: 185890.9545 - mae: 357.9649 - val_loss: 333388.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 20/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 348110.8284 - mse: 190539.2231 - mae: 356.3152 - val_loss: 329615.0000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 21/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 336168.7796 - mse: 181019.2357 - mae: 365.2346 - val_loss: 326863.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 22/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 316445.5938 - mse: 164860.1039 - mae: 330.6432 - val_loss: 323533.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 23/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 325521.6469 - mse: 177177.8651 - mae: 359.4465 - val_loss: 320183.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 24/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 341867.9276 - mse: 196760.0677 - mae: 369.0106 - val_loss: 316910.2812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 25/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 313744.2484 - mse: 173731.3846 - mae: 358.8763 - val_loss: 313822.6562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 26/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 318200.8333 - mse: 181504.1859 - mae: 363.9530 - val_loss: 310804.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 27/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 320545.4967 - mse: 188332.0510 - mae: 371.2907 - val_loss: 307795.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 28/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 292557.2686 - mse: 166078.5661 - mae: 349.7472 - val_loss: 305007.9688 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 29/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 303044.0828 - mse: 174391.9619 - mae: 357.8929 - val_loss: 301187.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 30/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 297721.7201 - mse: 176286.0861 - mae: 358.7008 - val_loss: 298137.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 31/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 297860.2604 - mse: 179298.9940 - mae: 367.5481 - val_loss: 295567.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 32/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 359756.4786 - mse: 241337.9460 - mae: 383.7542 - val_loss: 292149.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 33/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 298603.2604 - mse: 182487.7349 - mae: 368.9531 - val_loss: 289288.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 34/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 285214.1812 - mse: 177497.5658 - mae: 365.4180 - val_loss: 286562.4062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 35/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 275125.6272 - mse: 167366.6598 - mae: 351.0935 - val_loss: 283632.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 36/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 276728.9781 - mse: 173192.9779 - mae: 358.7583 - val_loss: 281371.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 37/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 276784.1771 - mse: 178829.5016 - mae: 368.4544 - val_loss: 278325.1250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 38/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 272020.5255 - mse: 175867.2050 - mae: 363.8639 - val_loss: 275408.9062 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 39/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 264317.4353 - mse: 172000.5894 - mae: 356.3275 - val_loss: 272672.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 40/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 260695.6220 - mse: 169109.0166 - mae: 348.2999 - val_loss: 270154.1875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 41/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 257274.1521 - mse: 168534.1738 - mae: 353.2930 - val_loss: 268360.3438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 42/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 245961.3141 - mse: 158832.8990 - mae: 338.8480 - val_loss: 266168.1562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 43/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 231701.9844 - mse: 149358.6430 - mae: 329.6128 - val_loss: 263565.0938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 44/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 246799.6593 - mse: 164664.9827 - mae: 346.9526 - val_loss: 260842.6094 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 45/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 254293.8621 - mse: 176649.6157 - mae: 366.2458 - val_loss: 258100.7188 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 46/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 247058.5452 - mse: 170335.4578 - mae: 354.0106 - val_loss: 255472.0312 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 47/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 241955.2684 - mse: 169656.1406 - mae: 351.2836 - val_loss: 253125.2500 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 48/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 239740.9734 - mse: 169192.3298 - mae: 351.7803 - val_loss: 251161.5625 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 49/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 249589.6269 - mse: 181683.6310 - mae: 369.5086 - val_loss: 249157.6562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 50/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 242199.2341 - mse: 176744.1088 - mae: 365.6005 - val_loss: 246753.6875 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 51/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 238685.8438 - mse: 176207.9677 - mae: 357.9274 - val_loss: 245668.2812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 52/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 228502.9350 - mse: 167810.3303 - mae: 348.4255 - val_loss: 243738.5156 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 53/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 238018.7495 - mse: 178675.3703 - mae: 353.6017 - val_loss: 160633.7188 - val_mse: 103769.3281 - val_mae: 269.2747\n",
      "Epoch 54/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 222742.5118 - mse: 167640.7928 - mae: 345.1540 - val_loss: 239198.3438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 55/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 217494.8956 - mse: 162923.5066 - mae: 348.5174 - val_loss: 236724.8906 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 56/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 226675.3786 - mse: 173530.7717 - mae: 360.0451 - val_loss: 234493.5469 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 57/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 215406.3525 - mse: 165019.3654 - mae: 342.5256 - val_loss: 232328.5156 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 58/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 210227.9250 - mse: 161075.4522 - mae: 345.6198 - val_loss: 230255.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 59/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 206639.0965 - mse: 161201.6941 - mae: 338.8016 - val_loss: 228266.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 60/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 222010.3972 - mse: 177880.0299 - mae: 362.8754 - val_loss: 226316.7344 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 61/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 214663.3824 - mse: 172885.8794 - mae: 357.1849 - val_loss: 224488.5938 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 62/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 214536.4918 - mse: 173240.9027 - mae: 358.2546 - val_loss: 222682.8125 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 63/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 209537.8300 - mse: 169870.7615 - mae: 350.6737 - val_loss: 220992.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 64/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 217135.2352 - mse: 178752.2664 - mae: 366.0096 - val_loss: 219091.6562 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 65/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 204135.7056 - mse: 168218.4370 - mae: 354.1057 - val_loss: 217344.2812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 66/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 210980.7344 - mse: 177661.3835 - mae: 364.5906 - val_loss: 215703.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 67/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 199550.8939 - mse: 167204.6181 - mae: 342.4149 - val_loss: 215616.3750 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 68/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 203808.7346 - mse: 170904.4082 - mae: 352.2108 - val_loss: 214888.5000 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 69/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 204559.0587 - mse: 173151.3314 - mae: 353.4715 - val_loss: 213295.5469 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 70/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 197500.8846 - mse: 167461.1483 - mae: 351.2538 - val_loss: 211967.7812 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 71/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 190028.1647 - mse: 162019.6371 - mae: 343.9238 - val_loss: 210547.8906 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 72/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 192055.2410 - mse: 165918.2917 - mae: 353.0085 - val_loss: 209327.6250 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 73/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 185109.0757 - mse: 158931.1983 - mae: 339.3548 - val_loss: 207827.3906 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 74/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 195506.9630 - mse: 171801.9104 - mae: 354.3235 - val_loss: 207278.1094 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 75/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 197901.3470 - mse: 174893.4745 - mae: 356.2213 - val_loss: 206001.9844 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 76/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 186789.4789 - mse: 164520.3740 - mae: 344.7060 - val_loss: 204656.3438 - val_mse: 183669.9531 - val_mae: 376.0901\n",
      "Epoch 77/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 181662.0428 - mse: 160522.1606 - mae: 341.6058 - val_loss: 169912.7188 - val_mse: 150078.1406 - val_mae: 332.4084\n",
      "Epoch 78/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 205823.0233 - mse: 186027.5304 - mae: 336.5578 - val_loss: 138001.9219 - val_mse: 117441.6484 - val_mae: 289.2848\n",
      "Epoch 79/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 160362.2064 - mse: 138570.6589 - mae: 314.8866 - val_loss: 146456.3281 - val_mse: 125535.8828 - val_mae: 299.2999\n",
      "Epoch 80/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 168956.3935 - mse: 148232.5891 - mae: 326.7511 - val_loss: 123808.2500 - val_mse: 104613.0469 - val_mae: 270.4078\n",
      "Epoch 81/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 148414.5269 - mse: 129397.9875 - mae: 292.1291 - val_loss: 102118.4844 - val_mse: 83712.6250 - val_mae: 240.7101\n",
      "Epoch 82/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 161187.5140 - mse: 143563.4557 - mae: 317.0108 - val_loss: 91250.6250 - val_mse: 74226.3750 - val_mae: 226.2694\n",
      "Epoch 83/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 147017.3081 - mse: 128896.2109 - mae: 296.5150 - val_loss: 91517.1875 - val_mse: 74658.8906 - val_mae: 226.6180\n",
      "Epoch 84/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 137655.6095 - mse: 120323.2426 - mae: 277.0035 - val_loss: 77669.6250 - val_mse: 61438.2266 - val_mae: 204.0920\n",
      "Epoch 85/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 128207.2023 - mse: 111514.8725 - mae: 272.5535 - val_loss: 84094.6094 - val_mse: 68305.6250 - val_mae: 215.6376\n",
      "Epoch 86/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 141060.6028 - mse: 125172.6619 - mae: 290.3102 - val_loss: 79749.7188 - val_mse: 64501.3320 - val_mae: 209.0014\n",
      "Epoch 87/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 130672.1175 - mse: 115092.2686 - mae: 274.6704 - val_loss: 79126.5703 - val_mse: 63800.4219 - val_mae: 208.5727\n",
      "Epoch 88/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 124468.6027 - mse: 109138.7992 - mae: 270.1263 - val_loss: 103189.8906 - val_mse: 88880.4844 - val_mae: 248.4134\n",
      "Epoch 89/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 127951.0465 - mse: 113865.2611 - mae: 271.3408 - val_loss: 73828.6719 - val_mse: 60547.3047 - val_mae: 202.2930\n",
      "Epoch 90/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 112666.6817 - mse: 99828.7113 - mae: 254.7766 - val_loss: 88381.6953 - val_mse: 75779.2266 - val_mae: 228.3721\n",
      "Epoch 91/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 110560.3066 - mse: 98368.8370 - mae: 254.3392 - val_loss: 72233.1094 - val_mse: 60724.5078 - val_mae: 204.0835\n",
      "Epoch 92/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 113649.0056 - mse: 102233.0540 - mae: 251.1161 - val_loss: 76861.8281 - val_mse: 65792.1016 - val_mae: 210.4198\n",
      "Epoch 93/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 110581.2511 - mse: 99635.5995 - mae: 255.7311 - val_loss: 71685.2578 - val_mse: 61246.9922 - val_mae: 203.4179\n",
      "Epoch 94/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 106783.7878 - mse: 96461.2896 - mae: 250.8875 - val_loss: 70503.3594 - val_mse: 61145.4766 - val_mae: 204.2047\n",
      "Epoch 95/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 112522.6593 - mse: 102727.0203 - mae: 259.4489 - val_loss: 63662.7305 - val_mse: 53631.8008 - val_mae: 186.6494\n",
      "Epoch 96/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 106556.8708 - mse: 96612.6397 - mae: 251.3276 - val_loss: 76776.1797 - val_mse: 67807.8906 - val_mae: 211.8806\n",
      "Epoch 97/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 104443.4382 - mse: 95550.0488 - mae: 249.7246 - val_loss: 73395.3203 - val_mse: 64883.0312 - val_mae: 209.9765\n",
      "Epoch 98/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 100598.5659 - mse: 92105.4153 - mae: 248.6058 - val_loss: 68261.2188 - val_mse: 60106.7422 - val_mae: 202.6415\n",
      "Epoch 99/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 116726.1816 - mse: 108436.1785 - mae: 266.5962 - val_loss: 71109.6875 - val_mse: 63580.3789 - val_mae: 208.8524\n",
      "Epoch 100/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 100992.3827 - mse: 93567.0513 - mae: 244.7125 - val_loss: 69372.6719 - val_mse: 62092.2422 - val_mae: 207.2922\n",
      "Epoch 101/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 91221.4760 - mse: 83715.2797 - mae: 237.1873 - val_loss: 66706.0781 - val_mse: 58616.9805 - val_mae: 199.5352\n",
      "Epoch 102/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 99477.0466 - mse: 91570.4002 - mae: 244.9348 - val_loss: 64493.4297 - val_mse: 56845.5391 - val_mae: 194.2965\n",
      "Epoch 103/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 94609.6327 - mse: 86906.6987 - mae: 240.3855 - val_loss: 68007.6641 - val_mse: 61087.6406 - val_mae: 203.3824\n",
      "Epoch 104/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 85060.8125 - mse: 78530.1916 - mae: 227.8272 - val_loss: 65629.9922 - val_mse: 59347.1484 - val_mae: 200.8627\n",
      "Epoch 105/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 87452.5119 - mse: 81051.0191 - mae: 231.7296 - val_loss: 68742.6406 - val_mse: 62166.4141 - val_mae: 204.7824\n",
      "Epoch 106/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 88103.9283 - mse: 81778.6177 - mae: 226.4587 - val_loss: 64358.4336 - val_mse: 58764.9336 - val_mae: 200.7773\n",
      "Epoch 107/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 80541.7351 - mse: 75311.4805 - mae: 219.5412 - val_loss: 60152.0898 - val_mse: 54483.5195 - val_mae: 192.9556\n",
      "Epoch 108/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 97708.8176 - mse: 91823.6808 - mae: 244.7370 - val_loss: 63360.1367 - val_mse: 58289.0977 - val_mae: 198.6550\n",
      "Epoch 109/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 92548.6473 - mse: 87195.2278 - mae: 248.6934 - val_loss: 86597.4531 - val_mse: 81937.9453 - val_mae: 238.3297\n",
      "Epoch 110/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 81209.1181 - mse: 76440.3888 - mae: 220.3169 - val_loss: 56747.6289 - val_mse: 51731.7930 - val_mae: 187.3370\n",
      "Epoch 111/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 90545.5472 - mse: 85336.6392 - mae: 236.4612 - val_loss: 66594.1328 - val_mse: 62165.9180 - val_mae: 206.1696\n",
      "Epoch 112/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 85850.9276 - mse: 81273.0600 - mae: 228.4331 - val_loss: 52869.0625 - val_mse: 48254.2109 - val_mae: 184.4247\n",
      "Epoch 113/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 81321.3289 - mse: 76782.3896 - mae: 220.1772 - val_loss: 65511.4297 - val_mse: 61194.7773 - val_mae: 204.4386\n",
      "Epoch 114/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 77768.1696 - mse: 73273.1465 - mae: 217.0183 - val_loss: 76189.6641 - val_mse: 72090.4219 - val_mae: 222.3651\n",
      "Epoch 115/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 85095.6401 - mse: 80914.2205 - mae: 225.2515 - val_loss: 80035.6016 - val_mse: 76146.4141 - val_mae: 228.6997\n",
      "Epoch 116/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 78441.7526 - mse: 74485.1523 - mae: 218.9386 - val_loss: 54976.3242 - val_mse: 50858.5781 - val_mae: 188.6664\n",
      "Epoch 117/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 85709.2274 - mse: 81278.8712 - mae: 232.1292 - val_loss: 74856.5234 - val_mse: 71138.3125 - val_mae: 220.7939\n",
      "Epoch 118/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 79314.1730 - mse: 75154.1593 - mae: 220.8996 - val_loss: 79714.6016 - val_mse: 76105.8984 - val_mae: 230.1325\n",
      "Epoch 119/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73628.9505 - mse: 69782.1756 - mae: 216.2795 - val_loss: 85821.4688 - val_mse: 82545.1172 - val_mae: 238.2859\n",
      "Epoch 120/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 79387.9611 - mse: 75875.1326 - mae: 214.0314 - val_loss: 83047.5156 - val_mse: 79983.7109 - val_mae: 233.0778\n",
      "Epoch 121/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 75128.5953 - mse: 71805.2159 - mae: 217.0191 - val_loss: 64288.0000 - val_mse: 61035.1992 - val_mae: 204.7233\n",
      "Epoch 122/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70285.7869 - mse: 66509.0548 - mae: 206.7882 - val_loss: 68087.0391 - val_mse: 64425.2305 - val_mae: 210.8056\n",
      "Epoch 123/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 94470.2307 - mse: 90782.4278 - mae: 245.8416 - val_loss: 77560.5547 - val_mse: 74048.6641 - val_mae: 226.2230\n",
      "Epoch 124/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73001.1963 - mse: 69141.7767 - mae: 212.5976 - val_loss: 69487.3906 - val_mse: 65685.3906 - val_mae: 211.9483\n",
      "Epoch 125/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66215.7070 - mse: 62326.3056 - mae: 206.4012 - val_loss: 58433.2344 - val_mse: 54944.5781 - val_mae: 193.7501\n",
      "Epoch 126/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 81650.0093 - mse: 78253.3204 - mae: 228.5905 - val_loss: 76264.4844 - val_mse: 73269.7344 - val_mae: 223.5882\n",
      "Epoch 127/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 66800.5150 - mse: 63593.5381 - mae: 198.6867 - val_loss: 69750.7422 - val_mse: 66956.6250 - val_mae: 213.1427\n",
      "Epoch 128/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65902.1846 - mse: 63134.8561 - mae: 200.4856 - val_loss: 71289.4844 - val_mse: 69033.7422 - val_mae: 217.9541\n",
      "Epoch 129/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72756.9005 - mse: 70507.6721 - mae: 220.7227 - val_loss: 74623.0391 - val_mse: 72410.0234 - val_mae: 223.0766\n",
      "Epoch 130/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 78164.7899 - mse: 75771.6088 - mae: 227.7598 - val_loss: 77285.6484 - val_mse: 75095.9062 - val_mae: 226.6294\n",
      "Epoch 131/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66213.7329 - mse: 63914.2354 - mae: 204.6218 - val_loss: 80211.9766 - val_mse: 78040.4688 - val_mae: 229.4473\n",
      "Epoch 132/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73075.2890 - mse: 70761.3387 - mae: 214.8812 - val_loss: 64238.5938 - val_mse: 61748.0078 - val_mae: 201.3639\n",
      "Epoch 133/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69925.3910 - mse: 67574.5241 - mae: 210.3349 - val_loss: 78143.8203 - val_mse: 75815.2266 - val_mae: 227.6257\n",
      "Epoch 134/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66226.7781 - mse: 63829.1621 - mae: 204.7431 - val_loss: 82968.4922 - val_mse: 80699.2266 - val_mae: 233.4658\n",
      "Epoch 135/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72286.5937 - mse: 69907.2540 - mae: 216.1921 - val_loss: 91783.4062 - val_mse: 89792.7734 - val_mae: 247.8656\n",
      "Epoch 136/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 76505.6160 - mse: 74334.3845 - mae: 219.5312 - val_loss: 88839.7734 - val_mse: 86868.0781 - val_mae: 244.1081\n",
      "Epoch 137/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67208.9492 - mse: 65026.5092 - mae: 208.9495 - val_loss: 82404.8359 - val_mse: 80367.1719 - val_mae: 232.3721\n",
      "Epoch 138/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65907.0272 - mse: 63855.6773 - mae: 204.6500 - val_loss: 71557.0156 - val_mse: 69375.4844 - val_mae: 215.4048\n",
      "Epoch 139/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64429.9302 - mse: 62123.0187 - mae: 203.4144 - val_loss: 68416.2188 - val_mse: 66235.9531 - val_mae: 209.7130\n",
      "Epoch 140/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70551.0660 - mse: 68384.0556 - mae: 212.6869 - val_loss: 73294.1719 - val_mse: 71230.5391 - val_mae: 220.7415\n",
      "Epoch 141/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57875.7378 - mse: 55818.0808 - mae: 193.3850 - val_loss: 70513.0469 - val_mse: 68650.2500 - val_mae: 215.7038\n",
      "Epoch 142/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63038.7438 - mse: 60945.7982 - mae: 204.1793 - val_loss: 77267.9062 - val_mse: 75317.7734 - val_mae: 227.3225\n",
      "Epoch 143/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 80051.4552 - mse: 78043.7574 - mae: 223.8103 - val_loss: 89049.8359 - val_mse: 87230.8281 - val_mae: 244.5046\n",
      "Epoch 144/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 73142.1562 - mse: 71084.7113 - mae: 220.2358 - val_loss: 76992.2109 - val_mse: 75176.5078 - val_mae: 226.5282\n",
      "Epoch 145/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57707.8485 - mse: 55689.4386 - mae: 194.1171 - val_loss: 78356.0703 - val_mse: 76500.1641 - val_mae: 229.4581\n",
      "Epoch 146/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 72275.8442 - mse: 70439.1412 - mae: 213.4521 - val_loss: 66111.0703 - val_mse: 64322.5859 - val_mae: 209.5344\n",
      "Epoch 147/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67294.6927 - mse: 65437.7036 - mae: 206.1153 - val_loss: 73252.7266 - val_mse: 71509.8203 - val_mae: 221.0173\n",
      "Epoch 148/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63304.1522 - mse: 61410.3072 - mae: 203.5331 - val_loss: 62609.7578 - val_mse: 60853.0117 - val_mae: 203.2331\n",
      "Epoch 149/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59439.4550 - mse: 57708.1847 - mae: 193.7666 - val_loss: 71374.1406 - val_mse: 69608.8438 - val_mae: 217.1274\n",
      "Epoch 150/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56582.1502 - mse: 54845.9260 - mae: 192.7983 - val_loss: 61227.3945 - val_mse: 59363.4297 - val_mae: 199.9546\n",
      "Epoch 151/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63473.9372 - mse: 61623.5269 - mae: 200.6089 - val_loss: 74640.2188 - val_mse: 72907.8438 - val_mae: 222.3883\n",
      "Epoch 152/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 67625.4248 - mse: 65715.1782 - mae: 207.9035 - val_loss: 73073.9297 - val_mse: 71170.5938 - val_mae: 219.7788\n",
      "Epoch 153/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59886.0503 - mse: 57918.1871 - mae: 194.7376 - val_loss: 92271.1328 - val_mse: 90734.3047 - val_mae: 250.9496\n",
      "Epoch 154/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64217.7153 - mse: 62450.2897 - mae: 207.3149 - val_loss: 77570.5703 - val_mse: 76094.2266 - val_mae: 228.4855\n",
      "Epoch 155/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56635.2098 - mse: 54990.5192 - mae: 190.6855 - val_loss: 77353.1328 - val_mse: 75955.7500 - val_mae: 228.2659\n",
      "Epoch 156/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59148.2144 - mse: 57779.9524 - mae: 192.9149 - val_loss: 72240.2422 - val_mse: 70698.3906 - val_mae: 219.4149\n",
      "Epoch 157/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59508.9372 - mse: 58142.4568 - mae: 195.3371 - val_loss: 86011.2734 - val_mse: 84677.8828 - val_mae: 242.0077\n",
      "Epoch 158/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65011.5458 - mse: 63423.8753 - mae: 202.0787 - val_loss: 82363.7266 - val_mse: 80985.5859 - val_mae: 236.1485\n",
      "Epoch 159/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57800.3044 - mse: 56288.0298 - mae: 187.0670 - val_loss: 87439.0078 - val_mse: 86016.8594 - val_mae: 243.5595\n",
      "Epoch 160/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65026.6402 - mse: 63536.7590 - mae: 202.1949 - val_loss: 75902.4453 - val_mse: 74435.5391 - val_mae: 225.4625\n",
      "Epoch 161/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64201.3723 - mse: 62685.4960 - mae: 205.6832 - val_loss: 73319.9375 - val_mse: 71650.5391 - val_mae: 221.6776\n",
      "Epoch 162/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64783.4399 - mse: 62967.2422 - mae: 206.9785 - val_loss: 82126.7422 - val_mse: 80512.4922 - val_mae: 235.4525\n",
      "Epoch 163/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68457.9718 - mse: 66820.9238 - mae: 210.9469 - val_loss: 88204.2578 - val_mse: 86617.0078 - val_mae: 244.9217\n",
      "Epoch 164/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65805.2219 - mse: 64073.1316 - mae: 200.0171 - val_loss: 78292.4922 - val_mse: 76623.4609 - val_mae: 229.1868\n",
      "Epoch 165/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61537.1751 - mse: 59859.0050 - mae: 198.8635 - val_loss: 76826.4297 - val_mse: 75163.7266 - val_mae: 227.1793\n",
      "Epoch 166/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61575.9536 - mse: 59995.6016 - mae: 198.2194 - val_loss: 80804.0625 - val_mse: 79532.6875 - val_mae: 234.3381\n",
      "Epoch 167/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60222.7472 - mse: 58896.9451 - mae: 194.5140 - val_loss: 72875.6875 - val_mse: 71490.8281 - val_mae: 220.9762\n",
      "Epoch 168/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63341.8681 - mse: 61909.1532 - mae: 198.2893 - val_loss: 82109.4062 - val_mse: 80843.0391 - val_mae: 236.5714\n",
      "Epoch 169/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 68398.1081 - mse: 67124.0696 - mae: 212.5031 - val_loss: 94736.3516 - val_mse: 93485.4453 - val_mae: 255.1013\n",
      "Epoch 170/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 60986.8363 - mse: 59658.2929 - mae: 198.4271 - val_loss: 75835.6094 - val_mse: 74360.8594 - val_mae: 225.4536\n",
      "Epoch 171/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57224.3363 - mse: 55766.6068 - mae: 193.1797 - val_loss: 85726.8203 - val_mse: 84435.9531 - val_mae: 242.2305\n",
      "Epoch 172/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68980.3950 - mse: 67644.2201 - mae: 211.2038 - val_loss: 79422.7109 - val_mse: 78089.9297 - val_mae: 232.2637\n",
      "Epoch 173/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66154.8046 - mse: 64855.6996 - mae: 210.1232 - val_loss: 79199.1719 - val_mse: 78021.4922 - val_mae: 230.9771\n",
      "Epoch 174/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 62509.5928 - mse: 61214.0938 - mae: 199.0030 - val_loss: 73494.7188 - val_mse: 72326.0547 - val_mae: 221.9916\n",
      "Epoch 175/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61736.9520 - mse: 60482.3018 - mae: 200.0155 - val_loss: 78935.3984 - val_mse: 77749.8438 - val_mae: 230.6939\n",
      "Epoch 176/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57846.4302 - mse: 56678.0278 - mae: 192.5325 - val_loss: 72098.7734 - val_mse: 70874.3906 - val_mae: 219.4773\n",
      "Epoch 177/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65045.3611 - mse: 63807.2692 - mae: 207.5997 - val_loss: 99335.1562 - val_mse: 98281.1641 - val_mae: 262.4429\n",
      "Epoch 178/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58310.6372 - mse: 57105.6568 - mae: 196.1359 - val_loss: 81411.7812 - val_mse: 80185.4453 - val_mae: 235.1860\n",
      "Epoch 179/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57370.9587 - mse: 56046.9745 - mae: 193.5875 - val_loss: 69328.7422 - val_mse: 68051.7812 - val_mae: 216.0982\n",
      "Epoch 180/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59523.4437 - mse: 58307.2624 - mae: 193.4787 - val_loss: 83938.7266 - val_mse: 82905.2344 - val_mae: 239.7182\n",
      "Epoch 181/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62663.3106 - mse: 61513.3222 - mae: 199.7951 - val_loss: 88930.9766 - val_mse: 87867.6719 - val_mae: 247.2207\n",
      "Epoch 182/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62123.4972 - mse: 60964.8975 - mae: 197.7503 - val_loss: 79649.8984 - val_mse: 78530.2109 - val_mae: 232.9934\n",
      "Epoch 183/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64436.1894 - mse: 63230.8539 - mae: 207.3781 - val_loss: 72391.1719 - val_mse: 71252.6406 - val_mae: 221.4120\n",
      "Epoch 184/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 54734.5866 - mse: 53471.5125 - mae: 187.4250 - val_loss: 73383.0703 - val_mse: 72214.8047 - val_mae: 222.9067\n",
      "Epoch 185/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61474.9396 - mse: 60303.5127 - mae: 198.5219 - val_loss: 78164.9609 - val_mse: 77072.3906 - val_mae: 230.5892\n",
      "Epoch 186/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70642.2806 - mse: 69557.1063 - mae: 217.3722 - val_loss: 70449.4609 - val_mse: 69321.2422 - val_mae: 217.7047\n",
      "Epoch 187/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65620.7680 - mse: 64425.9913 - mae: 205.1882 - val_loss: 66428.0859 - val_mse: 65316.3906 - val_mae: 211.6858\n",
      "Epoch 188/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 52935.0340 - mse: 51807.8502 - mae: 186.6093 - val_loss: 79039.7266 - val_mse: 77896.6953 - val_mae: 231.9118\n",
      "Epoch 189/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57556.1005 - mse: 56340.4710 - mae: 194.8330 - val_loss: 78618.8438 - val_mse: 77290.0703 - val_mae: 230.9332\n",
      "Epoch 190/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 54225.5620 - mse: 52906.5386 - mae: 185.7910 - val_loss: 69433.9922 - val_mse: 68111.1172 - val_mae: 216.2653\n",
      "Epoch 191/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 54698.3644 - mse: 53297.0049 - mae: 188.1583 - val_loss: 84230.9062 - val_mse: 83191.0000 - val_mae: 240.3229\n",
      "Epoch 192/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62352.0357 - mse: 61200.0352 - mae: 204.9049 - val_loss: 84479.6484 - val_mse: 83399.4062 - val_mae: 240.8142\n",
      "Epoch 193/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57587.7562 - mse: 56387.8193 - mae: 195.4169 - val_loss: 75795.2812 - val_mse: 74583.8594 - val_mae: 226.4858\n",
      "Epoch 194/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 52309.8015 - mse: 51027.9731 - mae: 184.3906 - val_loss: 81837.0078 - val_mse: 80744.9844 - val_mae: 236.3500\n",
      "Epoch 195/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60278.4165 - mse: 59149.0180 - mae: 198.0067 - val_loss: 80390.1641 - val_mse: 79326.4531 - val_mae: 233.9502\n",
      "Epoch 196/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60115.4119 - mse: 59031.7738 - mae: 193.6747 - val_loss: 83045.8281 - val_mse: 82030.0938 - val_mae: 238.3718\n",
      "Epoch 197/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63478.0793 - mse: 62444.1842 - mae: 192.9185 - val_loss: 70399.1250 - val_mse: 69342.0938 - val_mae: 218.0816\n",
      "Epoch 198/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 51380.8636 - mse: 50294.4911 - mae: 183.3494 - val_loss: 86518.6016 - val_mse: 85541.2109 - val_mae: 243.5351\n",
      "Epoch 199/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56030.8525 - mse: 54963.8329 - mae: 192.2530 - val_loss: 74835.0156 - val_mse: 73736.1016 - val_mae: 225.0023\n",
      "Epoch 200/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65560.7337 - mse: 64494.1476 - mae: 211.0769 - val_loss: 78324.0547 - val_mse: 77305.7422 - val_mae: 230.6761\n",
      "Epoch 201/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58742.5691 - mse: 57762.6163 - mae: 196.5300 - val_loss: 59103.8906 - val_mse: 57875.0977 - val_mae: 198.4044\n",
      "Epoch 202/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61423.0734 - mse: 60314.9979 - mae: 198.9102 - val_loss: 77618.3984 - val_mse: 76523.7734 - val_mae: 228.9892\n",
      "Epoch 203/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67749.2032 - mse: 66667.4472 - mae: 210.9751 - val_loss: 95249.2734 - val_mse: 94444.1016 - val_mae: 256.8922\n",
      "Epoch 204/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57112.5646 - mse: 56120.7089 - mae: 189.1269 - val_loss: 74089.5391 - val_mse: 73103.9375 - val_mae: 223.7726\n",
      "Epoch 205/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55110.8284 - mse: 54189.3062 - mae: 190.9779 - val_loss: 80877.3594 - val_mse: 80019.5859 - val_mae: 235.3235\n",
      "Epoch 206/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57312.7636 - mse: 56447.3940 - mae: 189.2061 - val_loss: 70926.2109 - val_mse: 69826.2188 - val_mae: 218.6769\n",
      "Epoch 207/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58997.9438 - mse: 57962.7137 - mae: 190.2300 - val_loss: 83279.0703 - val_mse: 82538.9297 - val_mae: 239.0821\n",
      "Epoch 208/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66089.5606 - mse: 65257.2954 - mae: 211.3911 - val_loss: 75651.3359 - val_mse: 74810.5547 - val_mae: 226.8473\n",
      "Epoch 209/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56901.9485 - mse: 56018.2113 - mae: 191.7976 - val_loss: 64825.0664 - val_mse: 63910.2344 - val_mae: 208.5815\n",
      "Epoch 210/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56562.1656 - mse: 55628.4109 - mae: 194.5835 - val_loss: 78338.7422 - val_mse: 77512.1797 - val_mae: 231.4603\n",
      "Epoch 211/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62467.3874 - mse: 61577.0208 - mae: 205.9181 - val_loss: 70916.0859 - val_mse: 70022.7969 - val_mae: 219.6001\n",
      "Epoch 212/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62040.1128 - mse: 61204.6916 - mae: 203.4609 - val_loss: 77112.3516 - val_mse: 76282.5781 - val_mae: 228.9328\n",
      "Epoch 213/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 59855.4628 - mse: 59007.0475 - mae: 201.1099 - val_loss: 75317.7266 - val_mse: 74369.3281 - val_mae: 225.6346\n",
      "Epoch 214/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62792.8512 - mse: 61824.9463 - mae: 206.1623 - val_loss: 68238.9141 - val_mse: 67257.0391 - val_mae: 214.5818\n",
      "Epoch 215/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62551.2947 - mse: 61597.1297 - mae: 195.3916 - val_loss: 92082.3281 - val_mse: 91342.0703 - val_mae: 252.2778\n",
      "Epoch 216/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66077.1680 - mse: 65185.0360 - mae: 208.0483 - val_loss: 86287.5938 - val_mse: 85419.1250 - val_mae: 243.3091\n",
      "Epoch 217/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63143.5532 - mse: 62260.5020 - mae: 203.2672 - val_loss: 77539.2500 - val_mse: 76652.1797 - val_mae: 229.5105\n",
      "Epoch 218/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61656.5073 - mse: 60727.4595 - mae: 205.4337 - val_loss: 82494.8828 - val_mse: 81579.2422 - val_mae: 237.4467\n",
      "Epoch 219/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 55646.1610 - mse: 54683.9678 - mae: 193.7338 - val_loss: 88842.4141 - val_mse: 88004.4141 - val_mae: 246.7093\n",
      "Epoch 220/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55172.6625 - mse: 54298.0040 - mae: 183.6603 - val_loss: 85745.6641 - val_mse: 84846.7656 - val_mae: 242.1254\n",
      "Epoch 221/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65115.9126 - mse: 64197.0079 - mae: 202.4788 - val_loss: 62100.3359 - val_mse: 61067.1367 - val_mae: 203.9050\n",
      "Epoch 222/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 68209.6325 - mse: 67320.9764 - mae: 209.1726 - val_loss: 84112.0625 - val_mse: 83337.2812 - val_mae: 239.1497\n",
      "Epoch 223/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58405.6059 - mse: 57574.6003 - mae: 197.3465 - val_loss: 78994.1875 - val_mse: 78233.8281 - val_mae: 231.1054\n",
      "Epoch 224/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62717.0132 - mse: 61860.7519 - mae: 199.6102 - val_loss: 70508.7422 - val_mse: 69676.4297 - val_mae: 217.7074\n",
      "Epoch 225/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61782.4213 - mse: 60931.2209 - mae: 202.4543 - val_loss: 69576.6250 - val_mse: 68736.8750 - val_mae: 216.4767\n",
      "Epoch 226/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 64651.8353 - mse: 63829.3971 - mae: 202.6476 - val_loss: 73676.5391 - val_mse: 72812.3516 - val_mae: 222.3741\n",
      "Epoch 227/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59383.5601 - mse: 58560.6035 - mae: 194.3266 - val_loss: 70396.8750 - val_mse: 69497.6719 - val_mae: 217.6456\n",
      "Epoch 228/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 67219.6084 - mse: 66289.1147 - mae: 209.4697 - val_loss: 65798.4297 - val_mse: 64985.7617 - val_mae: 210.4500\n",
      "Epoch 229/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 53285.8085 - mse: 52362.5335 - mae: 185.4976 - val_loss: 75759.6562 - val_mse: 75043.8828 - val_mae: 226.8000\n",
      "Epoch 230/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57096.0246 - mse: 56344.9104 - mae: 194.5678 - val_loss: 62841.7148 - val_mse: 62009.1094 - val_mae: 205.2829\n",
      "Epoch 231/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59158.5392 - mse: 58390.4522 - mae: 196.1959 - val_loss: 68770.1641 - val_mse: 67938.7578 - val_mae: 215.5912\n",
      "Epoch 232/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61993.9503 - mse: 61172.3240 - mae: 202.6496 - val_loss: 65949.5469 - val_mse: 65245.8750 - val_mae: 211.0633\n",
      "Epoch 233/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57621.3659 - mse: 56858.4861 - mae: 192.7751 - val_loss: 94681.7188 - val_mse: 94119.3516 - val_mae: 256.4128\n",
      "Epoch 234/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 52360.1682 - mse: 51581.1034 - mae: 177.8446 - val_loss: 85708.0938 - val_mse: 85025.4531 - val_mae: 242.9201\n",
      "Epoch 235/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61194.1469 - mse: 60482.8802 - mae: 202.3118 - val_loss: 88908.1719 - val_mse: 88238.5781 - val_mae: 247.7915\n",
      "Epoch 236/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 65765.2632 - mse: 65095.2958 - mae: 207.4234 - val_loss: 81999.5234 - val_mse: 81391.6953 - val_mae: 237.5790\n",
      "Epoch 237/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56760.7324 - mse: 56101.3175 - mae: 192.5285 - val_loss: 66962.8672 - val_mse: 66190.1719 - val_mae: 212.8888\n",
      "Epoch 238/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59852.9105 - mse: 59044.6778 - mae: 192.5660 - val_loss: 80737.6641 - val_mse: 80077.6953 - val_mae: 235.4530\n",
      "Epoch 239/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58667.1073 - mse: 57923.6848 - mae: 198.0320 - val_loss: 78029.0156 - val_mse: 77375.0234 - val_mae: 231.3103\n",
      "Epoch 240/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55807.4775 - mse: 55071.0538 - mae: 194.8262 - val_loss: 88692.9219 - val_mse: 88015.3125 - val_mae: 247.6269\n",
      "Epoch 241/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61009.1952 - mse: 60264.1756 - mae: 202.2245 - val_loss: 97681.7578 - val_mse: 97065.9453 - val_mae: 260.9425\n",
      "Epoch 242/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56888.2955 - mse: 56158.1336 - mae: 194.8585 - val_loss: 74071.3438 - val_mse: 73397.2266 - val_mae: 224.9823\n",
      "Epoch 243/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55284.1148 - mse: 54574.0460 - mae: 189.4609 - val_loss: 65383.4727 - val_mse: 64635.8203 - val_mae: 210.2774\n",
      "Epoch 244/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 53739.2123 - mse: 52970.3968 - mae: 182.6392 - val_loss: 73573.0234 - val_mse: 72853.8672 - val_mae: 224.0125\n",
      "Epoch 245/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56897.4938 - mse: 56190.4648 - mae: 188.3741 - val_loss: 64474.7539 - val_mse: 63633.7539 - val_mae: 208.7661\n",
      "Epoch 246/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63460.6663 - mse: 62766.8581 - mae: 206.1103 - val_loss: 66935.2266 - val_mse: 66262.5469 - val_mae: 213.0537\n",
      "Epoch 247/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 54561.3948 - mse: 53903.6422 - mae: 191.4741 - val_loss: 83033.5547 - val_mse: 82409.9062 - val_mae: 239.1606\n",
      "Epoch 248/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55493.4115 - mse: 54863.9170 - mae: 185.4199 - val_loss: 76461.9375 - val_mse: 75932.2188 - val_mae: 228.8652\n",
      "Epoch 249/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59114.3410 - mse: 58538.5407 - mae: 199.1243 - val_loss: 75247.5000 - val_mse: 74644.7344 - val_mae: 226.8468\n",
      "Epoch 250/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 51962.0196 - mse: 51317.9210 - mae: 186.2192 - val_loss: 72207.7734 - val_mse: 71593.4609 - val_mae: 221.8910\n",
      "Epoch 251/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56372.2689 - mse: 55760.1413 - mae: 190.2014 - val_loss: 63844.5273 - val_mse: 63243.3711 - val_mae: 208.1291\n",
      "Epoch 252/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61395.5694 - mse: 60748.7506 - mae: 200.9436 - val_loss: 79515.4453 - val_mse: 78919.7188 - val_mae: 233.8467\n",
      "Epoch 253/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57959.8220 - mse: 57253.6106 - mae: 195.4389 - val_loss: 81054.7109 - val_mse: 80482.6016 - val_mae: 236.2032\n",
      "Epoch 254/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56195.1647 - mse: 55601.9435 - mae: 190.8209 - val_loss: 66685.2422 - val_mse: 66005.3750 - val_mae: 212.5711\n",
      "Epoch 255/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63744.3296 - mse: 63062.1902 - mae: 207.0585 - val_loss: 63727.2383 - val_mse: 63131.8125 - val_mae: 207.6515\n",
      "Epoch 256/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 59734.1573 - mse: 59171.5890 - mae: 199.0706 - val_loss: 65129.0586 - val_mse: 64478.2930 - val_mae: 210.0433\n",
      "Epoch 257/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 54177.1333 - mse: 53488.9833 - mae: 194.2520 - val_loss: 68918.4609 - val_mse: 68283.3984 - val_mae: 216.3272\n",
      "Epoch 258/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 54923.2650 - mse: 54361.4511 - mae: 192.6014 - val_loss: 73126.4375 - val_mse: 72459.7734 - val_mae: 223.1785\n",
      "Epoch 259/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59711.7084 - mse: 59038.9888 - mae: 194.2363 - val_loss: 66342.7578 - val_mse: 65655.6797 - val_mae: 211.9818\n",
      "Epoch 260/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57420.2802 - mse: 56777.2598 - mae: 193.3144 - val_loss: 74235.3047 - val_mse: 73507.9062 - val_mae: 225.0119\n",
      "Epoch 261/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 66745.1961 - mse: 66064.5242 - mae: 211.0626 - val_loss: 66118.1875 - val_mse: 65532.6758 - val_mae: 211.9033\n",
      "Epoch 262/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58682.3056 - mse: 58102.1745 - mae: 195.9102 - val_loss: 84919.7266 - val_mse: 84428.3672 - val_mae: 242.3550\n",
      "Epoch 263/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58517.5906 - mse: 57885.9837 - mae: 196.3904 - val_loss: 80826.4141 - val_mse: 80284.7891 - val_mae: 235.8790\n",
      "Epoch 264/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 63800.2908 - mse: 63190.5567 - mae: 204.3240 - val_loss: 90768.6406 - val_mse: 90290.6328 - val_mae: 251.0825\n",
      "Epoch 265/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56227.6876 - mse: 55632.3293 - mae: 194.8058 - val_loss: 69896.7500 - val_mse: 69202.5156 - val_mae: 217.7845\n",
      "Epoch 266/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 55739.9844 - mse: 55101.0357 - mae: 187.9983 - val_loss: 80125.5781 - val_mse: 79609.8359 - val_mae: 234.7712\n",
      "Epoch 267/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 53369.7052 - mse: 52791.3385 - mae: 183.1320 - val_loss: 70066.5703 - val_mse: 69483.0156 - val_mae: 218.5164\n",
      "Epoch 268/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58033.2585 - mse: 57440.2974 - mae: 189.1836 - val_loss: 71803.6172 - val_mse: 71264.9219 - val_mae: 221.4780\n",
      "Epoch 269/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57621.0638 - mse: 57050.7643 - mae: 196.5163 - val_loss: 66368.6875 - val_mse: 65688.7734 - val_mae: 212.2695\n",
      "Epoch 270/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 53882.0684 - mse: 53227.1587 - mae: 191.6204 - val_loss: 73542.9922 - val_mse: 72962.8203 - val_mae: 224.0612\n",
      "Epoch 271/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56600.0942 - mse: 55992.6622 - mae: 188.6791 - val_loss: 69644.9219 - val_mse: 69013.7578 - val_mae: 217.4259\n",
      "Epoch 272/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 54897.1124 - mse: 54265.9360 - mae: 189.9580 - val_loss: 71286.3984 - val_mse: 70730.4922 - val_mae: 220.3099\n",
      "Epoch 273/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59757.4403 - mse: 59187.5333 - mae: 195.8805 - val_loss: 69248.8281 - val_mse: 68701.0703 - val_mae: 216.9842\n",
      "Epoch 274/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59957.5466 - mse: 59332.0138 - mae: 197.7444 - val_loss: 71498.1250 - val_mse: 70830.1016 - val_mae: 220.4510\n",
      "Epoch 275/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59060.5114 - mse: 58393.0086 - mae: 200.0118 - val_loss: 82982.2891 - val_mse: 82434.5078 - val_mae: 239.0072\n",
      "Epoch 276/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 63170.1606 - mse: 62620.1123 - mae: 202.6675 - val_loss: 63717.5938 - val_mse: 63058.1172 - val_mae: 207.3876\n",
      "Epoch 277/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65477.2740 - mse: 64806.5863 - mae: 205.5509 - val_loss: 97366.1562 - val_mse: 96860.9375 - val_mae: 260.5075\n",
      "Epoch 278/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 59125.3418 - mse: 58545.1515 - mae: 198.5152 - val_loss: 66002.9219 - val_mse: 65317.4062 - val_mae: 211.2483\n",
      "Epoch 279/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57617.1489 - mse: 56931.4381 - mae: 200.3605 - val_loss: 65119.3438 - val_mse: 64389.3008 - val_mae: 209.7070\n",
      "Epoch 280/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 57313.5497 - mse: 56639.4412 - mae: 195.9837 - val_loss: 72822.2188 - val_mse: 72207.1328 - val_mae: 222.6659\n",
      "Epoch 281/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 55706.0833 - mse: 55077.8057 - mae: 189.7590 - val_loss: 86759.3047 - val_mse: 86196.4219 - val_mae: 244.8526\n",
      "Epoch 282/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62673.4144 - mse: 62104.9050 - mae: 200.5305 - val_loss: 84825.5703 - val_mse: 84305.2812 - val_mae: 241.9930\n",
      "Epoch 283/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59627.4287 - mse: 59026.9583 - mae: 195.0174 - val_loss: 70242.6016 - val_mse: 69717.7188 - val_mae: 218.7184\n",
      "Epoch 284/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58166.2983 - mse: 57626.2167 - mae: 196.0110 - val_loss: 79129.7031 - val_mse: 78597.3984 - val_mae: 233.1369\n",
      "Epoch 285/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 54754.4539 - mse: 54159.1553 - mae: 191.3405 - val_loss: 72639.3672 - val_mse: 72156.0703 - val_mae: 222.6456\n",
      "Epoch 286/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 50546.7575 - mse: 49970.1884 - mae: 185.6788 - val_loss: 78431.8828 - val_mse: 77864.3750 - val_mae: 231.9754\n",
      "Epoch 287/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 52090.3074 - mse: 51499.6223 - mae: 186.8419 - val_loss: 68925.8672 - val_mse: 68386.7891 - val_mae: 216.3834\n",
      "Epoch 288/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58000.2220 - mse: 57408.5064 - mae: 198.2109 - val_loss: 82665.9219 - val_mse: 82159.8281 - val_mae: 238.6828\n",
      "Epoch 289/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58445.5490 - mse: 57841.4666 - mae: 196.6001 - val_loss: 70376.4062 - val_mse: 69709.5391 - val_mae: 218.6005\n",
      "Epoch 290/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65206.8407 - mse: 64650.0630 - mae: 203.2296 - val_loss: 71240.8359 - val_mse: 70648.0391 - val_mae: 220.0704\n",
      "Epoch 291/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 55443.7464 - mse: 54862.4267 - mae: 191.1696 - val_loss: 74431.4453 - val_mse: 73897.5703 - val_mae: 225.3814\n",
      "Epoch 292/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56301.9302 - mse: 55778.8871 - mae: 194.1889 - val_loss: 73544.9219 - val_mse: 72985.2734 - val_mae: 223.8897\n",
      "Epoch 293/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 56411.7612 - mse: 55834.5855 - mae: 192.8253 - val_loss: 82148.1797 - val_mse: 81614.5859 - val_mae: 237.7246\n",
      "Epoch 294/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 53513.1494 - mse: 52967.7744 - mae: 188.0699 - val_loss: 68187.3906 - val_mse: 67612.3125 - val_mae: 214.9507\n",
      "Epoch 295/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 60500.7529 - mse: 59932.3420 - mae: 200.2415 - val_loss: 60818.0781 - val_mse: 60216.1445 - val_mae: 202.5874\n",
      "Epoch 296/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 51233.7179 - mse: 50690.7877 - mae: 185.2892 - val_loss: 72486.0234 - val_mse: 71953.4062 - val_mae: 222.2091\n",
      "Epoch 297/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 61125.0711 - mse: 60555.9950 - mae: 202.0349 - val_loss: 76399.5078 - val_mse: 75899.1953 - val_mae: 228.7062\n",
      "Epoch 298/300\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 51747.9009 - mse: 51200.6719 - mae: 183.5882 - val_loss: 61976.3633 - val_mse: 61402.5391 - val_mae: 204.7292\n",
      "Epoch 299/300\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 55356.7120 - mse: 54807.3318 - mae: 192.9737 - val_loss: 66999.2891 - val_mse: 66450.8672 - val_mae: 213.0832\n",
      "Epoch 300/300\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 58524.6715 - mse: 58010.5164 - mae: 200.5540 - val_loss: 83395.5078 - val_mse: 82872.5938 - val_mae: 239.7144\n"
     ]
    }
   ],
   "source": [
    "history = nn.fit(X_train, y_train, epochs=300, batch_size=8, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(learning_rate = 0.01, activation = 'relu'):\n",
    "  \n",
    "    # Create an Adam optimizer with the given learning rate\n",
    "    opt = Adam(lr=learning_rate)\n",
    "  \n",
    "    # Create your binary classification model  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, \n",
    "                    activation = activation,\n",
    "                    input_shape = (15, ),\n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(128,\n",
    "                    activation = activation, \n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(1, activation = activation))\n",
    "# Compile the model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model,\n",
    "                       verbose=1)\n",
    "\n",
    "params = {'activation': [\"relu\"],\n",
    "          'batch_size': [16, 8], \n",
    "          'epochs': [200, 300, 500],\n",
    "          'learning_rate': [0.01, 0.05, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "35/35 [==============================] - 1s 2ms/step - loss: 208623227.3333 - mse: 208093594.0000 - mae: 6327.4467\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 995505.0069 - mse: 425191.0625 - mae: 391.6884\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 781119.9062 - mse: 187077.8077 - mae: 369.7106\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 713826.6267 - mse: 175710.6332 - mae: 361.5946\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 669018.0156 - mse: 165357.2270 - mae: 343.6575\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 651698.2014 - mse: 174179.0165 - mae: 352.7403\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 654374.1215 - mse: 189402.7227 - mae: 375.1910\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 688502.4514 - mse: 226709.7305 - mae: 366.8479\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 662316.5191 - mse: 181035.4401 - mae: 365.1249\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 657991.6997 - mse: 181097.4249 - mae: 354.4992\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 698643.1701 - mse: 243661.9431 - mae: 365.5364\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 616295.0095 - mse: 170778.0805 - mae: 346.8395\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 645966.4375 - mse: 184947.2791 - mae: 365.5773\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 679710.2899 - mse: 217881.3442 - mae: 367.1648\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 629958.2309 - mse: 171502.5061 - mae: 352.0133\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 623305.9462 - mse: 178284.0365 - mae: 363.1534\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 629419.4045 - mse: 182388.2109 - mae: 364.9490\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 631527.1771 - mse: 181689.7339 - mae: 365.1706\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 717891.6927 - mse: 293169.9171 - mae: 381.4110\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 606060.8333 - mse: 187226.4293 - mae: 356.3717\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 609555.9983 - mse: 186589.9905 - mae: 363.6657\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 643415.8663 - mse: 220463.7904 - mae: 368.0468\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 588959.3559 - mse: 171110.8320 - mae: 355.1316\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 589017.3403 - mse: 179578.9479 - mae: 355.8599\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 607553.3038 - mse: 195353.2726 - mae: 385.2965\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 590760.1024 - mse: 171681.8707 - mae: 356.5920\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1154432.3698 - mse: 748057.6276 - mae: 422.1937\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 583555.5052 - mse: 177162.9045 - mae: 359.1999\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 556881.1467 - mse: 172930.7661 - mae: 356.7747\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 627273.2309 - mse: 233603.1819 - mae: 374.5240\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 551932.4957 - mse: 171924.1189 - mae: 356.4914\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 604935.5312 - mse: 211943.8077 - mae: 379.2390\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 655532.0191 - mse: 288203.3568 - mae: 368.4393\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 544444.0634 - mse: 164656.6558 - mae: 343.8648\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 542597.3802 - mse: 173978.6827 - mae: 357.4301\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 623521.1042 - mse: 254755.2036 - mae: 377.4286\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 539778.1458 - mse: 174194.2331 - mae: 357.6097\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 524119.6033 - mse: 167401.4384 - mae: 349.5759\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 924586.4002 - mse: 564224.1098 - mae: 389.4006\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 589444.2153 - mse: 200592.2209 - mae: 367.2635\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 542993.9844 - mse: 168440.7465 - mae: 350.9748\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 541974.2839 - mse: 181488.7170 - mae: 371.8424\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 599436.9444 - mse: 221177.6667 - mae: 380.7745\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 532234.4800 - mse: 178355.9983 - mae: 365.0080\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 514174.8090 - mse: 166343.7678 - mae: 351.7294\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 547906.4149 - mse: 181773.9536 - mae: 365.7888\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 528873.6753 - mse: 179868.7517 - mae: 359.8734\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 514107.2786 - mse: 178217.2977 - mae: 367.0220\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 509274.5998 - mse: 173935.5942 - mae: 359.7580\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 517527.2448 - mse: 192273.3698 - mae: 368.7849\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 487597.6892 - mse: 171371.5430 - mae: 356.8352\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 492636.0564 - mse: 177192.6940 - mae: 366.4210\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 463674.3316 - mse: 167190.8989 - mae: 352.9816\n",
      "Epoch 54/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 473615.2075 - mse: 165008.3338 - mae: 349.6641\n",
      "Epoch 55/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 486255.1519 - mse: 180919.6176 - mae: 364.2956\n",
      "Epoch 56/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 475746.0998 - mse: 176717.5273 - mae: 360.9903\n",
      "Epoch 57/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 487557.4062 - mse: 186742.6254 - mae: 373.0652\n",
      "Epoch 58/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 467778.8889 - mse: 177238.9622 - mae: 365.9210\n",
      "Epoch 59/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 465242.7387 - mse: 172686.7014 - mae: 353.8922\n",
      "Epoch 60/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 530744.2986 - mse: 254881.8594 - mae: 391.8251\n",
      "Epoch 61/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 439663.9653 - mse: 170129.5035 - mae: 356.6755\n",
      "Epoch 62/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 448283.1128 - mse: 178173.2096 - mae: 361.7778\n",
      "Epoch 63/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 464257.0443 - mse: 187829.4488 - mae: 367.8977\n",
      "Epoch 64/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 494715.9644 - mse: 217340.8329 - mae: 375.6858\n",
      "Epoch 65/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 420357.3411 - mse: 168396.0122 - mae: 354.3979\n",
      "Epoch 66/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 434967.2995 - mse: 173127.7739 - mae: 354.4798\n",
      "Epoch 67/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 428263.1250 - mse: 174036.6875 - mae: 358.2449\n",
      "Epoch 68/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 411962.8594 - mse: 161028.0786 - mae: 346.0956\n",
      "Epoch 69/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 434106.9436 - mse: 185987.8529 - mae: 362.8604\n",
      "Epoch 70/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 413017.1406 - mse: 165341.9475 - mae: 345.9552\n",
      "Epoch 71/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 404582.4028 - mse: 168344.0273 - mae: 350.9931\n",
      "Epoch 72/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 400255.1927 - mse: 171416.7131 - mae: 357.6021\n",
      "Epoch 73/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 408827.0842 - mse: 184826.1237 - mae: 368.9610\n",
      "Epoch 74/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 397043.2569 - mse: 168658.9536 - mae: 353.1795\n",
      "Epoch 75/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 378021.3472 - mse: 160950.8351 - mae: 340.1608\n",
      "Epoch 76/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 392930.8359 - mse: 173151.9392 - mae: 358.7460\n",
      "Epoch 77/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 381667.5286 - mse: 169355.6341 - mae: 350.6696\n",
      "Epoch 78/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 438865.6146 - mse: 222615.3025 - mae: 366.2019\n",
      "Epoch 79/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 393141.2943 - mse: 180440.9123 - mae: 358.5867\n",
      "Epoch 80/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 992150.7161 - mse: 778314.9644 - mae: 413.4454\n",
      "Epoch 81/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 430001.7266 - mse: 224058.5343 - mae: 371.0296\n",
      "Epoch 82/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 429367.5503 - mse: 221634.4709 - mae: 363.1617\n",
      "Epoch 83/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 384038.6085 - mse: 171136.3329 - mae: 352.8689\n",
      "Epoch 84/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 381539.0625 - mse: 176333.1819 - mae: 363.4045\n",
      "Epoch 85/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 378922.6823 - mse: 180277.4544 - mae: 365.5375\n",
      "Epoch 86/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 353244.4883 - mse: 160169.0321 - mae: 343.2453\n",
      "Epoch 87/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 376043.6224 - mse: 182373.1875 - mae: 374.0501\n",
      "Epoch 88/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 370239.9983 - mse: 181340.6545 - mae: 364.2067\n",
      "Epoch 89/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 362223.4271 - mse: 176441.0742 - mae: 363.6886\n",
      "Epoch 90/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 356318.9062 - mse: 169957.1389 - mae: 351.1143\n",
      "Epoch 91/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 350422.9974 - mse: 172946.1458 - mae: 358.1507\n",
      "Epoch 92/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 352747.0200 - mse: 172194.7248 - mae: 356.0275\n",
      "Epoch 93/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 350112.0208 - mse: 184560.0074 - mae: 375.3923\n",
      "Epoch 94/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 335557.2005 - mse: 168978.1089 - mae: 353.8196\n",
      "Epoch 95/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 340148.4497 - mse: 175956.9557 - mae: 360.7571\n",
      "Epoch 96/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 339172.4896 - mse: 181211.9835 - mae: 368.0508\n",
      "Epoch 97/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 333930.2196 - mse: 174684.4536 - mae: 359.2866\n",
      "Epoch 98/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 325155.7587 - mse: 172127.1541 - mae: 356.2693\n",
      "Epoch 99/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 335556.4575 - mse: 181949.2986 - mae: 364.8469\n",
      "Epoch 100/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 317186.8108 - mse: 170155.6228 - mae: 356.1001\n",
      "Epoch 101/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 322387.4774 - mse: 177329.8277 - mae: 364.1676\n",
      "Epoch 102/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 319545.4323 - mse: 179185.1571 - mae: 363.7707\n",
      "Epoch 103/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 318964.6345 - mse: 176473.3737 - mae: 362.7883\n",
      "Epoch 104/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 330086.1224 - mse: 190955.2938 - mae: 377.5491\n",
      "Epoch 105/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 349486.1198 - mse: 215629.8333 - mae: 376.9455\n",
      "Epoch 106/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 311197.8950 - mse: 179490.4410 - mae: 366.1481\n",
      "Epoch 107/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 317200.0009 - mse: 181652.1654 - mae: 354.5774\n",
      "Epoch 108/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 307419.0556 - mse: 179593.3546 - mae: 363.2779\n",
      "Epoch 109/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 300156.1849 - mse: 175185.2682 - mae: 363.5704\n",
      "Epoch 110/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 300416.0087 - mse: 175819.5929 - mae: 358.5489\n",
      "Epoch 111/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 303067.4601 - mse: 182684.1840 - mae: 367.5460\n",
      "Epoch 112/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 287250.6628 - mse: 168414.1970 - mae: 353.8827\n",
      "Epoch 113/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 283415.0334 - mse: 167273.2810 - mae: 345.2461\n",
      "Epoch 114/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 294092.3160 - mse: 180385.8880 - mae: 367.1902\n",
      "Epoch 115/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 289055.2569 - mse: 175696.5547 - mae: 361.5776\n",
      "Epoch 116/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 281696.6840 - mse: 171698.1202 - mae: 356.0466\n",
      "Epoch 117/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 284402.7656 - mse: 179658.1241 - mae: 367.4336\n",
      "Epoch 118/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 283034.1554 - mse: 179651.8798 - mae: 360.9793\n",
      "Epoch 119/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 284741.9392 - mse: 178952.6528 - mae: 370.6184\n",
      "Epoch 120/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 279153.8685 - mse: 177682.7313 - mae: 362.6543\n",
      "Epoch 121/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 286361.5816 - mse: 185746.6662 - mae: 358.3687\n",
      "Epoch 122/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 266731.9275 - mse: 168470.0356 - mae: 353.4636\n",
      "Epoch 123/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 267701.3659 - mse: 172289.8220 - mae: 354.9522\n",
      "Epoch 124/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 267310.4657 - mse: 169795.4948 - mae: 351.3432\n",
      "Epoch 125/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 267125.8021 - mse: 178484.8390 - mae: 364.1904\n",
      "Epoch 126/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 263599.2040 - mse: 176948.9119 - mae: 361.6231\n",
      "Epoch 127/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 250919.5825 - mse: 162960.7804 - mae: 344.6339\n",
      "Epoch 128/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 277615.4136 - mse: 189610.3854 - mae: 379.1486\n",
      "Epoch 129/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 262304.0148 - mse: 177400.0703 - mae: 366.9041\n",
      "Epoch 130/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 257600.3503 - mse: 173057.1832 - mae: 357.1699\n",
      "Epoch 131/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 251408.2005 - mse: 168494.4002 - mae: 353.3210\n",
      "Epoch 132/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 249404.6050 - mse: 171871.0178 - mae: 353.2887\n",
      "Epoch 133/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 252709.2452 - mse: 175814.4384 - mae: 358.1427\n",
      "Epoch 134/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 250818.5851 - mse: 173698.3186 - mae: 357.1515\n",
      "Epoch 135/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 253368.0898 - mse: 179471.6480 - mae: 363.4001\n",
      "Epoch 136/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 250312.3837 - mse: 177695.4414 - mae: 363.0735\n",
      "Epoch 137/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 251118.1910 - mse: 182966.7387 - mae: 370.1959\n",
      "Epoch 138/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 258212.6836 - mse: 188636.9062 - mae: 370.0888\n",
      "Epoch 139/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 243489.1285 - mse: 175398.4657 - mae: 360.6063\n",
      "Epoch 140/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 244555.7739 - mse: 177315.5538 - mae: 359.4707\n",
      "Epoch 141/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 245259.1198 - mse: 180388.3750 - mae: 369.6784\n",
      "Epoch 142/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 236846.4362 - mse: 173522.0660 - mae: 361.1081\n",
      "Epoch 143/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 224954.9540 - mse: 163413.8620 - mae: 345.2273\n",
      "Epoch 144/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 223659.5208 - mse: 159594.2873 - mae: 338.9669\n",
      "Epoch 145/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 232134.1424 - mse: 172816.0169 - mae: 359.9958\n",
      "Epoch 146/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 236797.2474 - mse: 177844.6654 - mae: 360.0145\n",
      "Epoch 147/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 241882.1489 - mse: 185651.1519 - mae: 370.5837\n",
      "Epoch 148/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 220526.2552 - mse: 165278.9727 - mae: 343.8573\n",
      "Epoch 149/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 226318.6155 - mse: 172357.5443 - mae: 354.3154\n",
      "Epoch 150/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 242786.2018 - mse: 190516.6671 - mae: 363.8946\n",
      "Epoch 151/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 220693.1584 - mse: 167086.7413 - mae: 346.2859\n",
      "Epoch 152/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 211012.7305 - mse: 159076.2700 - mae: 334.0899\n",
      "Epoch 153/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 204491.5161 - mse: 155084.1094 - mae: 333.1520\n",
      "Epoch 154/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 211057.2370 - mse: 160699.2057 - mae: 327.1556\n",
      "Epoch 155/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 186973.2543 - mse: 137734.7873 - mae: 313.9601\n",
      "Epoch 156/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 200186.6250 - mse: 151944.2721 - mae: 327.9962\n",
      "Epoch 157/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 198796.6007 - mse: 152002.7072 - mae: 328.9996\n",
      "Epoch 158/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 191826.2083 - mse: 147068.4188 - mae: 320.5311\n",
      "Epoch 159/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 190240.3303 - mse: 146197.8190 - mae: 321.0532\n",
      "Epoch 160/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 184274.4510 - mse: 140050.2079 - mae: 304.2827\n",
      "Epoch 161/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 192735.8676 - mse: 150789.1918 - mae: 328.7382\n",
      "Epoch 162/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 183436.5226 - mse: 142024.4151 - mae: 316.7025\n",
      "Epoch 163/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 187472.5464 - mse: 146951.1163 - mae: 323.4150\n",
      "Epoch 164/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 181242.3720 - mse: 142598.6135 - mae: 304.8932\n",
      "Epoch 165/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 170549.7635 - mse: 132103.2385 - mae: 299.1206\n",
      "Epoch 166/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 172239.7632 - mse: 135809.9653 - mae: 311.6684\n",
      "Epoch 167/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 188282.5747 - mse: 152309.9332 - mae: 330.5741\n",
      "Epoch 168/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 177409.1784 - mse: 143094.4301 - mae: 314.9399\n",
      "Epoch 169/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 170224.6293 - mse: 136543.3229 - mae: 308.0348\n",
      "Epoch 170/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 172977.9453 - mse: 140246.1424 - mae: 310.7546\n",
      "Epoch 171/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 182650.3898 - mse: 151827.4761 - mae: 314.5309\n",
      "Epoch 172/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 173440.8702 - mse: 142342.5043 - mae: 317.8179\n",
      "Epoch 173/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 163607.6046 - mse: 133466.7739 - mae: 302.4054\n",
      "Epoch 174/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 159467.9674 - mse: 131055.2166 - mae: 297.5877\n",
      "Epoch 175/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 165846.2287 - mse: 136924.8432 - mae: 308.0858\n",
      "Epoch 176/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 169787.4831 - mse: 142070.8099 - mae: 316.2046\n",
      "Epoch 177/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 158002.9831 - mse: 131880.7865 - mae: 295.0099\n",
      "Epoch 178/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 155853.9186 - mse: 130724.1332 - mae: 302.5810\n",
      "Epoch 179/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 153782.5113 - mse: 129641.7270 - mae: 296.5999\n",
      "Epoch 180/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 161240.9488 - mse: 136682.6170 - mae: 309.6163\n",
      "Epoch 181/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 145912.6194 - mse: 122994.1851 - mae: 288.3999\n",
      "Epoch 182/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 155267.3105 - mse: 133317.8815 - mae: 303.8919\n",
      "Epoch 183/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 150436.8381 - mse: 129530.9954 - mae: 292.5828\n",
      "Epoch 184/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 152350.3320 - mse: 131541.7413 - mae: 299.7169\n",
      "Epoch 185/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 149979.5152 - mse: 130109.0438 - mae: 300.9880\n",
      "Epoch 186/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 148183.7140 - mse: 129328.8498 - mae: 298.1990\n",
      "Epoch 187/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 161893.7426 - mse: 142873.7099 - mae: 312.1861\n",
      "Epoch 188/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 149476.8112 - mse: 131788.6693 - mae: 302.9928\n",
      "Epoch 189/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 153935.1382 - mse: 136514.9102 - mae: 307.0172\n",
      "Epoch 190/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 149390.6706 - mse: 132949.6953 - mae: 304.8564\n",
      "Epoch 191/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 150574.5651 - mse: 134735.6181 - mae: 302.6042\n",
      "Epoch 192/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 145236.5143 - mse: 130162.1434 - mae: 297.8177\n",
      "Epoch 193/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 146067.2309 - mse: 132116.7689 - mae: 301.3101\n",
      "Epoch 194/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 153996.9288 - mse: 140381.2760 - mae: 310.8846\n",
      "Epoch 195/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 147481.9796 - mse: 134242.6365 - mae: 300.9114\n",
      "Epoch 196/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 143968.1239 - mse: 131473.1463 - mae: 297.8627\n",
      "Epoch 197/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 137080.8160 - mse: 124748.1341 - mae: 288.4784\n",
      "Epoch 198/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 154066.8329 - mse: 141379.1189 - mae: 310.8355\n",
      "Epoch 199/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 146634.4586 - mse: 133966.8472 - mae: 306.9839\n",
      "Epoch 200/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 151225.7487 - mse: 139232.1682 - mae: 308.9790\n",
      "CPU times: user 27.3 s, sys: 4.83 s, total: 32.1 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(model,\n",
    "                                   param_distributions=params, n_jobs=-1)\n",
    "\n",
    "random_search_results = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'epochs': 200, 'batch_size': 16, 'activation': 'relu'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_tuned = random_search_results.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (1.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.5.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error test: 144.0608607556509\n",
      "Mean Absolute Error train: 1.9467544881761938\n"
     ]
    }
   ],
   "source": [
    "xgbr = XGBRegressor()\n",
    "\n",
    "xgbr.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_metric=\"mae\",  \n",
    "    verbose=True)\n",
    "\n",
    "predictions = xgbr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error test: \" + str(mean_absolute_error(predictions, y_test)))\n",
    "print(\"Mean Absolute Error train: \" + str(mean_absolute_error(xgbr.predict(X_train), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  191.018436\n",
      "Mean MAE: 148.588 (11.174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8386416.326196362, tolerance: 2469.0218836503623\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "lasso_reg = Lasso(alpha=0.0)\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "pred = lasso_reg.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "scores = cross_val_score(lasso_reg, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "\n",
    "rmse = np.sqrt(MSE(y_test, pred))\n",
    "print(\"RMSE : % f\" %(rmse))\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "             estimator=Lasso(), n_jobs=-1,\n",
       "             param_grid={'alpha': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99])},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search hyperparameters for lasso regression\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# define model\n",
    "model = Lasso()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = arange(0, 1, 0.01)\n",
    "# define search\n",
    "lasso_tuned = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "lasso_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 152.367 (13.922)\n"
     ]
    }
   ],
   "source": [
    "# evaluate an ridge regression model on the dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define model\n",
    "ridge_reg = Ridge(alpha=0.00)\n",
    "# define model evaluation method\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(ridge_reg, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.12575e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.34071e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99]),\n",
       "        cv=RepeatedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "        scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import arange\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ridge_tune2 = RidgeCV(alphas=arange(0, 1, 0.01), cv=cv, scoring='neg_mean_absolute_error')\n",
    "# fit model\n",
    "ridge_tune2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = svm.SVR()\n",
    "sv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance with twitter BERT and news sentiments\n",
      "Epoch 1/200\n",
      "35/35 [==============================] - 1s 3ms/step - loss: 211454468.4444 - mse: 211040527.5556 - mae: 6832.4496\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 615391.5712 - mse: 174074.2530 - mae: 358.0845\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 603398.2934 - mse: 178959.7804 - mae: 364.1222\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 625669.4054 - mse: 245851.6510 - mae: 363.9838\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 555335.8368 - mse: 173194.5417 - mae: 357.5488\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 529892.0182 - mse: 173642.2661 - mae: 353.7227\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 584089.1215 - mse: 227551.3464 - mae: 370.8581\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 635421.4262 - mse: 275740.7231 - mae: 381.1418\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 582986.4236 - mse: 215356.8511 - mae: 364.0679\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 533294.6198 - mse: 171503.1220 - mae: 356.4167\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 779119.5035 - mse: 420753.0803 - mae: 393.9028\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 545134.5304 - mse: 196111.8598 - mae: 375.4625\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 542736.7049 - mse: 178503.2500 - mae: 362.3746\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 512631.1832 - mse: 168096.2565 - mae: 351.4697\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 547364.0139 - mse: 194887.6172 - mae: 361.7406\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 527101.5460 - mse: 176897.8220 - mae: 360.1395\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 604310.3733 - mse: 266985.2352 - mae: 386.7348\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 674424.7083 - mse: 336780.9401 - mae: 391.0140\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 518627.9887 - mse: 173448.8537 - mae: 355.8780\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 534539.6484 - mse: 203133.3694 - mae: 371.4077\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 519014.0122 - mse: 172707.5204 - mae: 352.3183\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 514411.4792 - mse: 188159.3438 - mae: 362.9925\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 509435.6849 - mse: 176778.2496 - mae: 357.7870\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 495191.4080 - mse: 177808.0747 - mae: 355.4258\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 510009.4965 - mse: 172390.3841 - mae: 356.5620\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 675334.3160 - mse: 353884.5747 - mae: 389.8903\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 479254.9227 - mse: 170054.0382 - mae: 352.1638\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 517696.1172 - mse: 187670.5052 - mae: 370.7496\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 528275.1701 - mse: 213157.8824 - mae: 375.3951\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 489046.6962 - mse: 165729.9497 - mae: 348.3084\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 485281.3542 - mse: 171982.5130 - mae: 355.9615\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 664704.2595 - mse: 354303.3958 - mae: 377.1413\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 648673.9366 - mse: 338321.6250 - mae: 377.8738\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 485004.5851 - mse: 167460.6901 - mae: 349.6385\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 497312.7839 - mse: 177357.7613 - mae: 366.8803\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 496158.7622 - mse: 174163.4653 - mae: 361.5130\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 504985.3993 - mse: 188103.1163 - mae: 364.5818\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 473116.0686 - mse: 169214.1198 - mae: 352.2634\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 488889.5981 - mse: 182605.7912 - mae: 374.1692\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 514077.8411 - mse: 197201.0911 - mae: 363.7331\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 474042.6345 - mse: 172016.1845 - mae: 354.3305\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 473590.7092 - mse: 176220.5343 - mae: 354.9216\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 472559.5217 - mse: 176453.3503 - mae: 359.3905\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 501119.0530 - mse: 209150.5109 - mae: 377.5233\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 462604.2743 - mse: 162987.3550 - mae: 340.7865\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 468684.3142 - mse: 173272.4583 - mae: 363.6272\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 470157.5799 - mse: 178568.5859 - mae: 367.4233\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 564874.4991 - mse: 286040.7760 - mae: 387.4075\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 434530.1380 - mse: 160733.2118 - mae: 342.6354\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 456103.0191 - mse: 174519.6033 - mae: 360.3825\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 459045.2118 - mse: 180853.0677 - mae: 362.7576\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 429724.3750 - mse: 174678.5178 - mae: 365.1658\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 439548.2396 - mse: 177206.8442 - mae: 366.3270\n",
      "Epoch 54/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 436147.6059 - mse: 184160.7135 - mae: 368.6730\n",
      "Epoch 55/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 434547.6675 - mse: 168746.0603 - mae: 352.0375\n",
      "Epoch 56/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 468386.1328 - mse: 198527.1806 - mae: 375.3400\n",
      "Epoch 57/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 429006.4740 - mse: 173786.4128 - mae: 363.5023\n",
      "Epoch 58/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 429209.9340 - mse: 171488.8364 - mae: 355.2737\n",
      "Epoch 59/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 412527.5911 - mse: 165792.0556 - mae: 349.2967\n",
      "Epoch 60/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 439699.5877 - mse: 182271.1076 - mae: 368.0229\n",
      "Epoch 61/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 431621.7760 - mse: 182863.0890 - mae: 366.7765\n",
      "Epoch 62/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 405764.3194 - mse: 168897.4861 - mae: 353.2440\n",
      "Epoch 63/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 422516.1120 - mse: 183719.5647 - mae: 361.4245\n",
      "Epoch 64/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 413750.4106 - mse: 176398.8372 - mae: 360.0101\n",
      "Epoch 65/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 414435.7925 - mse: 177938.5898 - mae: 360.6915\n",
      "Epoch 66/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 422961.8125 - mse: 181024.5707 - mae: 362.0928\n",
      "Epoch 67/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 416141.9887 - mse: 173449.4262 - mae: 357.4201\n",
      "Epoch 68/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 396166.5625 - mse: 166955.6732 - mae: 352.6866\n",
      "Epoch 69/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 402017.5712 - mse: 177641.3203 - mae: 362.3348\n",
      "Epoch 70/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 387449.8438 - mse: 167142.5065 - mae: 351.8073\n",
      "Epoch 71/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 377056.1753 - mse: 157768.3891 - mae: 334.6652\n",
      "Epoch 72/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 383531.5972 - mse: 168932.7895 - mae: 352.0918\n",
      "Epoch 73/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 386833.2205 - mse: 169134.5582 - mae: 351.9011\n",
      "Epoch 74/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 389966.6892 - mse: 176500.0651 - mae: 362.7427\n",
      "Epoch 75/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 371556.3542 - mse: 166619.9479 - mae: 349.8570\n",
      "Epoch 76/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 401678.7569 - mse: 182445.9214 - mae: 366.6581\n",
      "Epoch 77/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 381728.5477 - mse: 175527.6445 - mae: 359.8809\n",
      "Epoch 78/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 363725.0911 - mse: 173660.0135 - mae: 353.6667\n",
      "Epoch 79/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 370227.0885 - mse: 176440.8928 - mae: 357.6676\n",
      "Epoch 80/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 367790.0304 - mse: 172748.3444 - mae: 358.0759\n",
      "Epoch 81/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 361649.5538 - mse: 165594.0803 - mae: 349.7899\n",
      "Epoch 82/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 375055.7257 - mse: 182028.5035 - mae: 369.7108\n",
      "Epoch 83/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 363507.9019 - mse: 177001.4648 - mae: 359.4369\n",
      "Epoch 84/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 359588.4983 - mse: 175061.0343 - mae: 360.9792\n",
      "Epoch 85/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 352696.6866 - mse: 173158.7483 - mae: 358.9244\n",
      "Epoch 86/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 352054.4557 - mse: 178949.3924 - mae: 363.9348\n",
      "Epoch 87/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 348485.1927 - mse: 173431.9531 - mae: 357.0140\n",
      "Epoch 88/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 346394.9688 - mse: 174628.3746 - mae: 358.2105\n",
      "Epoch 89/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 361440.1198 - mse: 188292.8168 - mae: 359.3895\n",
      "Epoch 90/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 341027.7509 - mse: 168457.7535 - mae: 347.1149\n",
      "Epoch 91/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 350332.4800 - mse: 184258.7799 - mae: 370.3565\n",
      "Epoch 92/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 348379.9444 - mse: 179525.2118 - mae: 367.8138\n",
      "Epoch 93/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 325742.9219 - mse: 165125.6211 - mae: 350.8291\n",
      "Epoch 94/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 331321.7231 - mse: 174090.4006 - mae: 363.0526\n",
      "Epoch 95/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 340373.9931 - mse: 181093.2044 - mae: 368.5349\n",
      "Epoch 96/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 336451.5286 - mse: 182842.8598 - mae: 366.2840\n",
      "Epoch 97/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 324882.9983 - mse: 173391.8568 - mae: 359.0054\n",
      "Epoch 98/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 317590.0877 - mse: 167269.3260 - mae: 352.2420\n",
      "Epoch 99/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 314816.5443 - mse: 168963.2118 - mae: 349.3770\n",
      "Epoch 100/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 310528.7917 - mse: 168038.6176 - mae: 352.7309\n",
      "Epoch 101/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 324804.1285 - mse: 180692.2010 - mae: 366.0842\n",
      "Epoch 102/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 376401.6432 - mse: 233554.1806 - mae: 374.0773\n",
      "Epoch 103/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 349430.7769 - mse: 207441.0872 - mae: 369.5337\n",
      "Epoch 104/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 314916.4201 - mse: 175798.9527 - mae: 362.9301\n",
      "Epoch 105/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 311562.6814 - mse: 176671.7899 - mae: 358.8403\n",
      "Epoch 106/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 312162.7309 - mse: 178256.2912 - mae: 364.5757\n",
      "Epoch 107/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 308686.3316 - mse: 172975.7127 - mae: 360.4233\n",
      "Epoch 108/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 307555.6571 - mse: 177290.6823 - mae: 363.3654\n",
      "Epoch 109/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 313074.2405 - mse: 183999.6897 - mae: 363.4823\n",
      "Epoch 110/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 299235.3238 - mse: 174812.4792 - mae: 358.4223\n",
      "Epoch 111/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 301689.7587 - mse: 181957.9570 - mae: 365.1424\n",
      "Epoch 112/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 288059.7500 - mse: 167812.3247 - mae: 350.8687\n",
      "Epoch 113/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 309060.1944 - mse: 188400.4549 - mae: 377.0583\n",
      "Epoch 114/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 285909.2474 - mse: 170753.7387 - mae: 355.2099\n",
      "Epoch 115/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 290280.0616 - mse: 173901.4991 - mae: 357.9780\n",
      "Epoch 116/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 286863.8863 - mse: 176118.1068 - mae: 360.8611\n",
      "Epoch 117/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 278198.4436 - mse: 166298.7092 - mae: 350.3292\n",
      "Epoch 118/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 299065.6411 - mse: 188920.7704 - mae: 364.9730\n",
      "Epoch 119/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 274775.2804 - mse: 170652.4319 - mae: 358.2305\n",
      "Epoch 120/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 277765.2917 - mse: 173681.4666 - mae: 359.6704\n",
      "Epoch 121/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 277654.0278 - mse: 174427.9796 - mae: 359.5478\n",
      "Epoch 122/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 272480.7352 - mse: 169774.0230 - mae: 354.4787\n",
      "Epoch 123/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 275962.4844 - mse: 176146.7331 - mae: 363.3291\n",
      "Epoch 124/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 284701.4922 - mse: 182937.4657 - mae: 366.0585\n",
      "Epoch 125/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 255951.2201 - mse: 160146.1332 - mae: 344.3054\n",
      "Epoch 126/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 285473.5929 - mse: 189069.0720 - mae: 372.6949\n",
      "Epoch 127/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 262550.2860 - mse: 170227.4262 - mae: 357.6283\n",
      "Epoch 128/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 254501.8030 - mse: 165815.9449 - mae: 350.9886\n",
      "Epoch 129/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 263483.5278 - mse: 174845.3134 - mae: 363.0112\n",
      "Epoch 130/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 274229.2830 - mse: 185606.4049 - mae: 373.3403\n",
      "Epoch 131/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 250628.0143 - mse: 165522.3340 - mae: 348.0039\n",
      "Epoch 132/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 259937.6484 - mse: 175040.7891 - mae: 357.4486\n",
      "Epoch 133/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 265601.9592 - mse: 180822.5725 - mae: 363.8259\n",
      "Epoch 134/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 256745.0312 - mse: 176071.8602 - mae: 362.8392\n",
      "Epoch 135/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 253776.2817 - mse: 176134.2031 - mae: 361.0525\n",
      "Epoch 136/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 246338.2713 - mse: 169336.9240 - mae: 352.2722\n",
      "Epoch 137/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 279184.3924 - mse: 202786.9349 - mae: 365.6086\n",
      "Epoch 138/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 255483.9891 - mse: 181868.1150 - mae: 366.7554\n",
      "Epoch 139/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 254515.8646 - mse: 180728.2995 - mae: 360.4035\n",
      "Epoch 140/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 252021.3494 - mse: 181239.6068 - mae: 366.1941\n",
      "Epoch 141/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 252773.5430 - mse: 182690.3025 - mae: 367.0380\n",
      "Epoch 142/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 231222.0139 - mse: 164772.7986 - mae: 350.7413\n",
      "Epoch 143/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 248291.3459 - mse: 181266.1198 - mae: 368.1958\n",
      "Epoch 144/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 234261.5846 - mse: 168810.1311 - mae: 352.5736\n",
      "Epoch 145/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 236510.8971 - mse: 174133.8442 - mae: 359.8250\n",
      "Epoch 146/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 236485.4184 - mse: 175630.6775 - mae: 357.2837\n",
      "Epoch 147/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 242566.6367 - mse: 181012.3420 - mae: 367.4368\n",
      "Epoch 148/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 228811.3199 - mse: 170241.1905 - mae: 352.2548\n",
      "Epoch 149/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 242890.5929 - mse: 186413.4657 - mae: 370.5584\n",
      "Epoch 150/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 242973.4618 - mse: 185371.9931 - mae: 369.4058\n",
      "Epoch 151/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 232750.8889 - mse: 175861.3251 - mae: 360.5330\n",
      "Epoch 152/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 216336.8173 - mse: 163164.2192 - mae: 343.7544\n",
      "Epoch 153/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 228029.8168 - mse: 173608.1654 - mae: 360.9664\n",
      "Epoch 154/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 221285.9067 - mse: 169825.5534 - mae: 354.9845\n",
      "Epoch 155/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 225589.6085 - mse: 175429.2652 - mae: 359.8625\n",
      "Epoch 156/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 212683.7543 - mse: 164879.5200 - mae: 346.9471\n",
      "Epoch 157/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 246791.5547 - mse: 198240.0412 - mae: 391.4690\n",
      "Epoch 158/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 225387.8780 - mse: 176821.2934 - mae: 363.8022\n",
      "Epoch 159/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 223744.8698 - mse: 177774.8207 - mae: 365.8986\n",
      "Epoch 160/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 218898.0347 - mse: 174857.8689 - mae: 360.8432\n",
      "Epoch 161/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 220838.1580 - mse: 178385.5295 - mae: 362.5338\n",
      "Epoch 162/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 219145.8811 - mse: 177480.7348 - mae: 362.7225\n",
      "Epoch 163/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 211430.6437 - mse: 169676.3668 - mae: 352.0140\n",
      "Epoch 164/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 204970.2856 - mse: 165039.4935 - mae: 349.3934\n",
      "Epoch 165/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 214796.3741 - mse: 176584.8928 - mae: 362.5874\n",
      "Epoch 166/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 206329.3720 - mse: 169114.0000 - mae: 354.1608\n",
      "Epoch 167/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 216551.6714 - mse: 180808.7730 - mae: 371.8984\n",
      "Epoch 168/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 209835.0490 - mse: 174310.4601 - mae: 359.4761\n",
      "Epoch 169/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 214314.0560 - mse: 179002.8394 - mae: 364.2690\n",
      "Epoch 170/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 212840.2600 - mse: 180091.2669 - mae: 366.1224\n",
      "Epoch 171/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 211579.9306 - mse: 179610.6879 - mae: 364.4723\n",
      "Epoch 172/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 205063.0473 - mse: 173960.1276 - mae: 357.9400\n",
      "Epoch 173/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 195134.9761 - mse: 164553.5961 - mae: 347.0469\n",
      "Epoch 174/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 200795.8155 - mse: 172741.2938 - mae: 359.3838\n",
      "Epoch 175/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 201324.9718 - mse: 172429.4288 - mae: 359.2136\n",
      "Epoch 176/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 193971.0169 - mse: 166499.8451 - mae: 344.3979\n",
      "Epoch 177/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 207324.4648 - mse: 180129.4783 - mae: 364.5491\n",
      "Epoch 178/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 213791.8025 - mse: 187804.6875 - mae: 378.9160\n",
      "Epoch 179/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 202976.0838 - mse: 177249.7018 - mae: 360.8922\n",
      "Epoch 180/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 199887.6970 - mse: 175988.1806 - mae: 362.0552\n",
      "Epoch 181/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 185825.9918 - mse: 161896.1137 - mae: 345.4536\n",
      "Epoch 182/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 199929.5256 - mse: 177172.8012 - mae: 365.1472\n",
      "Epoch 183/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 198889.2595 - mse: 176268.9206 - mae: 363.9077\n",
      "Epoch 184/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 201735.1176 - mse: 181089.5742 - mae: 367.7004\n",
      "Epoch 185/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 206037.2986 - mse: 184736.4336 - mae: 375.2720\n",
      "Epoch 186/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 192743.9609 - mse: 172690.2765 - mae: 358.2792\n",
      "Epoch 187/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 191112.5781 - mse: 171055.3676 - mae: 354.8266\n",
      "Epoch 188/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 200325.4128 - mse: 181158.0343 - mae: 371.7016\n",
      "Epoch 189/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 187126.2648 - mse: 168880.1766 - mae: 351.8284\n",
      "Epoch 190/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 193640.8841 - mse: 175823.8320 - mae: 359.7201\n",
      "Epoch 191/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 170065.4692 - mse: 152096.8611 - mae: 330.4094\n",
      "Epoch 192/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 187339.3733 - mse: 168876.9006 - mae: 348.5702\n",
      "Epoch 193/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 190479.8181 - mse: 171958.6810 - mae: 351.9708\n",
      "Epoch 194/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 153745.7378 - mse: 136006.7739 - mae: 306.7831\n",
      "Epoch 195/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 164970.9466 - mse: 147777.7500 - mae: 316.6384\n",
      "Epoch 196/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 157154.1892 - mse: 140612.2320 - mae: 311.9704\n",
      "Epoch 197/200\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 148801.6467 - mse: 132030.8034 - mae: 295.8286\n",
      "Epoch 198/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 154262.3016 - mse: 138006.2724 - mae: 315.2720\n",
      "Epoch 199/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 162084.7509 - mse: 146355.5768 - mae: 322.2050\n",
      "Epoch 200/200\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 150676.1649 - mse: 136033.3867 - mae: 307.0405\n",
      "35/35 [==============================] - 0s 835us/step\n",
      "12/12 [==============================] - 0s 933us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8386416.326196362, tolerance: 2469.0218836503623\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.12575e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.34071e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>xgb</th>\n",
       "      <th>knn</th>\n",
       "      <th>lasso</th>\n",
       "      <th>lasso_tuned</th>\n",
       "      <th>ridge</th>\n",
       "      <th>ridge_tuned</th>\n",
       "      <th>rf</th>\n",
       "      <th>sv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train: Rsquare</th>\n",
       "      <td>0.999803</td>\n",
       "      <td>-1.029608</td>\n",
       "      <td>0.320669</td>\n",
       "      <td>0.320301</td>\n",
       "      <td>0.311212</td>\n",
       "      <td>0.320668</td>\n",
       "      <td>0.915082</td>\n",
       "      <td>0.031581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: Rsquare</th>\n",
       "      <td>0.235026</td>\n",
       "      <td>-1.488829</td>\n",
       "      <td>0.194721</td>\n",
       "      <td>0.196868</td>\n",
       "      <td>0.180295</td>\n",
       "      <td>0.194866</td>\n",
       "      <td>0.380534</td>\n",
       "      <td>-0.007643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAE</th>\n",
       "      <td>1.946754</td>\n",
       "      <td>249.481601</td>\n",
       "      <td>144.715306</td>\n",
       "      <td>144.806554</td>\n",
       "      <td>144.600874</td>\n",
       "      <td>144.722199</td>\n",
       "      <td>49.765362</td>\n",
       "      <td>179.702344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAE</th>\n",
       "      <td>144.060861</td>\n",
       "      <td>280.564016</td>\n",
       "      <td>159.568432</td>\n",
       "      <td>159.404311</td>\n",
       "      <td>160.703696</td>\n",
       "      <td>159.553698</td>\n",
       "      <td>137.719565</td>\n",
       "      <td>185.019818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: RMSE</th>\n",
       "      <td>2.967180</td>\n",
       "      <td>301.299890</td>\n",
       "      <td>174.314562</td>\n",
       "      <td>174.361823</td>\n",
       "      <td>175.523659</td>\n",
       "      <td>174.314633</td>\n",
       "      <td>61.630167</td>\n",
       "      <td>208.125108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: RMSE</th>\n",
       "      <td>186.176669</td>\n",
       "      <td>335.814603</td>\n",
       "      <td>191.018436</td>\n",
       "      <td>190.763555</td>\n",
       "      <td>192.721848</td>\n",
       "      <td>191.001163</td>\n",
       "      <td>167.537066</td>\n",
       "      <td>213.675802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train: MAPE</th>\n",
       "      <td>1.392140</td>\n",
       "      <td>168.167872</td>\n",
       "      <td>191.083032</td>\n",
       "      <td>189.280976</td>\n",
       "      <td>189.224972</td>\n",
       "      <td>190.953624</td>\n",
       "      <td>52.675247</td>\n",
       "      <td>316.076221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test: MAPE</th>\n",
       "      <td>82.039050</td>\n",
       "      <td>105.079740</td>\n",
       "      <td>128.377555</td>\n",
       "      <td>129.122331</td>\n",
       "      <td>120.866021</td>\n",
       "      <td>128.415484</td>\n",
       "      <td>81.265742</td>\n",
       "      <td>190.543998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                  xgb         knn       lasso  lasso_tuned       ridge  \\\n",
       "Train: Rsquare    0.999803   -1.029608    0.320669     0.320301    0.311212   \n",
       "Test: Rsquare     0.235026   -1.488829    0.194721     0.196868    0.180295   \n",
       "Train: MAE        1.946754  249.481601  144.715306   144.806554  144.600874   \n",
       "Test: MAE       144.060861  280.564016  159.568432   159.404311  160.703696   \n",
       "Train: RMSE       2.967180  301.299890  174.314562   174.361823  175.523659   \n",
       "Test: RMSE      186.176669  335.814603  191.018436   190.763555  192.721848   \n",
       "Train: MAPE       1.392140  168.167872  191.083032   189.280976  189.224972   \n",
       "Test: MAPE       82.039050  105.079740  128.377555   129.122331  120.866021   \n",
       "\n",
       "Model           ridge_tuned          rf          sv  \n",
       "Train: Rsquare     0.320668    0.915082    0.031581  \n",
       "Test: Rsquare      0.194866    0.380534   -0.007643  \n",
       "Train: MAE       144.722199   49.765362  179.702344  \n",
       "Test: MAE        159.553698  137.719565  185.019818  \n",
       "Train: RMSE      174.314633   61.630167  208.125108  \n",
       "Test: RMSE       191.001163  167.537066  213.675802  \n",
       "Train: MAPE      190.953624   52.675247  316.076221  \n",
       "Test: MAPE       128.415484   81.265742  190.543998  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model performance with twitter BERT and news sentiments\")\n",
    "model_performance([xgbr, nn_tuned, lasso_reg, lasso_tuned, ridge_reg, ridge_tune2, rf, sv], [\"xgb\", 'knn', 'lasso', 'lasso_tuned', 'ridge', 'ridge_tuned', 'rf', 'sv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_important_gain = xgbr.get_booster().get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_important_gain = xgbr.get_booster().get_score(importance_type='gain')\n",
    "keys = list(feature_important_gain.keys())\n",
    "values = list(feature_important_gain.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=['score']).sort_values(by='score', ascending=False)\n",
    "data.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
